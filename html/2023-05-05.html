<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Catch Missing Details: Image Reconstruction with Frequency Augmented Variational Autoencoder. (arXiv:2305.02541v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02541">http://arxiv.org/abs/2305.02541</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02541] Catch Missing Details: Image Reconstruction with Frequency Augmented Variational Autoencoder](http://arxiv.org/abs/2305.02541) #transformer</code></li>
<li>Summary: <p>The popular VQ-VAE models reconstruct images through learning a discrete
codebook but suffer from a significant issue in the rapid quality degradation
of image reconstruction as the compression rate rises. One major reason is that
a higher compression rate induces more loss of visual signals on the higher
frequency spectrum which reflect the details on pixel space. In this paper, a
Frequency Complement Module (FCM) architecture is proposed to capture the
missing frequency information for enhancing reconstruction quality. The FCM can
be easily incorporated into the VQ-VAE structure, and we refer to the new model
as Frequency Augmented VAE (FA-VAE). In addition, a Dynamic Spectrum Loss (DSL)
is introduced to guide the FCMs to balance between various frequencies
dynamically for optimal reconstruction. FA-VAE is further extended to the
text-to-image synthesis task, and a Cross-attention Autoregressive Transformer
(CAT) is proposed to obtain more precise semantic attributes in texts.
Extensive reconstruction experiments with different compression rates are
conducted on several benchmark datasets, and the results demonstrate that the
proposed FA-VAE is able to restore more faithfully the details compared to SOTA
methods. CAT also shows improved generation quality with better image-text
semantic alignment.
</p></li>
</ul>
<h3>Title: LayoutDM: Transformer-based Diffusion Model for Layout Generation. (arXiv:2305.02567v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02567">http://arxiv.org/abs/2305.02567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02567] LayoutDM: Transformer-based Diffusion Model for Layout Generation](http://arxiv.org/abs/2305.02567) #transformer</code></li>
<li>Summary: <p>Automatic layout generation that can synthesize high-quality layouts is an
important tool for graphic design in many applications. Though existing methods
based on generative models such as Generative Adversarial Networks (GANs) and
Variational Auto-Encoders (VAEs) have progressed, they still leave much room
for improving the quality and diversity of the results. Inspired by the recent
success of diffusion models in generating high-quality images, this paper
explores their potential for conditional layout generation and proposes
Transformer-based Layout Diffusion Model (LayoutDM) by instantiating the
conditional denoising diffusion probabilistic model (DDPM) with a purely
transformer-based architecture. Instead of using convolutional neural networks,
a transformer-based conditional Layout Denoiser is proposed to learn the
reverse diffusion process to generate samples from noised layout data.
Benefitting from both transformer and DDPM, our LayoutDM is of desired
properties such as high-quality generation, strong sample diversity, faithful
distribution coverage, and stationary training in comparison to GANs and VAEs.
Quantitative and qualitative experimental results show that our method
outperforms state-of-the-art generative models in terms of quality and
diversity.
</p></li>
</ul>
<h3>Title: Towards End-to-End Semi-Supervised Table Detection with Deformable Transformer. (arXiv:2305.02769v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02769">http://arxiv.org/abs/2305.02769</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02769] Towards End-to-End Semi-Supervised Table Detection with Deformable Transformer](http://arxiv.org/abs/2305.02769) #transformer</code></li>
<li>Summary: <p>Table detection is the task of classifying and localizing table objects
within document images. With the recent development in deep learning methods,
we observe remarkable success in table detection. However, a significant amount
of labeled data is required to train these models effectively. Many
semi-supervised approaches are introduced to mitigate the need for a
substantial amount of label data. These approaches use CNN-based detectors that
rely on anchor proposals and post-processing stages such as NMS. To tackle
these limitations, this paper presents a novel end-to-end semi-supervised table
detection method that employs the deformable transformer for detecting table
objects. We evaluate our semi-supervised method on PubLayNet, DocBank, ICADR-19
and TableBank datasets, and it achieves superior performance compared to
previous methods. It outperforms the fully supervised method (Deformable
transformer) by +3.4 points on 10\% labels of TableBank-both dataset and the
previous CNN-based semi-supervised approach (Soft Teacher) by +1.8 points on
10\% labels of PubLayNet dataset. We hope this work opens new possibilities
towards semi-supervised and unsupervised table detection methods.
</p></li>
</ul>
<h3>Title: MTLSegFormer: Multi-task Learning with Transformers for Semantic Segmentation in Precision Agriculture. (arXiv:2305.02813v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02813">http://arxiv.org/abs/2305.02813</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02813] MTLSegFormer: Multi-task Learning with Transformers for Semantic Segmentation in Precision Agriculture](http://arxiv.org/abs/2305.02813) #transformer</code></li>
<li>Summary: <p>Multi-task learning has proven to be effective in improving the performance
of correlated tasks. Most of the existing methods use a backbone to extract
initial features with independent branches for each task, and the exchange of
information between the branches usually occurs through the concatenation or
sum of the feature maps of the branches. However, this type of information
exchange does not directly consider the local characteristics of the image nor
the level of importance or correlation between the tasks. In this paper, we
propose a semantic segmentation method, MTLSegFormer, which combines multi-task
learning and attention mechanisms. After the backbone feature extraction, two
feature maps are learned for each task. The first map is proposed to learn
features related to its task, while the second map is obtained by applying
learned visual attention to locally re-weigh the feature maps of the other
tasks. In this way, weights are assigned to local regions of the image of other
tasks that have greater importance for the specific task. Finally, the two maps
are combined and used to solve a task. We tested the performance in two
challenging problems with correlated tasks and observed a significant
improvement in accuracy, mainly in tasks with high dependence on the others.
</p></li>
</ul>
<h3>Title: UPDExplainer: an Interpretable Transformer-based Framework for Urban Physical Disorder Detection Using Street View Imagery. (arXiv:2305.02911v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02911">http://arxiv.org/abs/2305.02911</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02911] UPDExplainer: an Interpretable Transformer-based Framework for Urban Physical Disorder Detection Using Street View Imagery](http://arxiv.org/abs/2305.02911) #transformer</code></li>
<li>Summary: <p>Urban Physical Disorder (UPD), such as old or abandoned buildings, broken
sidewalks, litter, and graffiti, has a negative impact on residents' quality of
life. They can also increase crime rates, cause social disorder, and pose a
public health risk. Currently, there is a lack of efficient and reliable
methods for detecting and understanding UPD. To bridge this gap, we propose
UPDExplainer, an interpretable transformer-based framework for UPD detection.
We first develop a UPD detection model based on the Swin Transformer
architecture, which leverages readily accessible street view images to learn
discriminative representations. In order to provide clear and comprehensible
evidence and analysis, we subsequently introduce a UPD factor identification
and ranking module that combines visual explanation maps with semantic
segmentation maps. This novel integrated approach enables us to identify the
exact objects within street view images that are responsible for physical
disorders and gain insights into the underlying causes. Experimental results on
the re-annotated Place Pulse 2.0 dataset demonstrate promising detection
performance of the proposed method, with an accuracy of 79.9%. For a
comprehensive evaluation of the method's ranking performance, we report the
mean Average Precision (mAP), R-Precision (RPrec), and Normalized Discounted
Cumulative Gain (NDCG), with success rates of 75.51%, 80.61%, and 82.58%,
respectively. We also present a case study of detecting and ranking physical
disorders in the southern region of downtown Los Angeles, California, to
demonstrate the practicality and effectiveness of our framework.
</p></li>
</ul>
<h3>Title: Adversarially-Guided Portrait Matting. (arXiv:2305.02981v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02981">http://arxiv.org/abs/2305.02981</a></li>
<li>Code URL: <a href="https://github.com/chroneus/stylematte">https://github.com/chroneus/stylematte</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02981] Adversarially-Guided Portrait Matting](http://arxiv.org/abs/2305.02981) #transformer</code></li>
<li>Summary: <p>We present a method for generating alpha mattes using a limited data source.
We pretrain a novel transformerbased model (StyleMatte) on portrait datasets.
We utilize this model to provide image-mask pairs for the StyleGAN3- based
network (StyleMatteGAN). This network is trained unsupervisedly and generates
previously unseen imagemask training pairs that are fed back to StyleMatte. We
demonstrate that the performance of the matte pulling network improves during
this cycle and obtains top results on the used datasets. Furthermore,
StyleMatteGAN provides high-resolution, privacy-preserving portraits with alpha
mattes, making it suitable for various image composition tasks. Our code is
available at https://github.com/chroneus/stylematte
</p></li>
</ul>
<h3>Title: OctFormer: Octree-based Transformers for 3D Point Clouds. (arXiv:2305.03045v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03045">http://arxiv.org/abs/2305.03045</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03045] OctFormer: Octree-based Transformers for 3D Point Clouds](http://arxiv.org/abs/2305.03045) #transformer</code></li>
<li>Summary: <p>OctFormer can not only serve as a general and effective backbone for 3D point
cloud segmentation and object detection but also have linear complexity and is
scalable for large-scale point clouds. The key challenge in applying
transformers to point clouds is reducing the quadratic, thus overwhelming,
computation complexity of attentions. To combat this issue, several works
divide point clouds into non-overlapping windows and constrain attentions in
each local window. However, the point number in each window varies greatly,
impeding the efficient execution on GPU. Observing that attentions are robust
to the shapes of local windows, we propose a novel octree attention, which
leverages sorted shuffled keys of octrees to partition point clouds into local
windows containing a fixed number of points while permitting shapes of windows
to change freely. And we also introduce dilated octree attention to expand the
receptive field further. Our octree attention can be implemented in 10 lines of
code with open-sourced libraries and runs 17 times faster than other point
cloud attentions when the point number exceeds 200k. Built upon the octree
attention, OctFormer can be easily scaled up and achieves state-of-the-art
performances on a series of 3D segmentation and detection benchmarks,
surpassing previous sparse-voxel-based CNNs and point cloud transformers in
terms of both efficiency and effectiveness. Notably, on the challenging
ScanNet200 dataset, OctFormer outperforms sparse-voxel-based CNNs by 7.3 in
mIoU. Our code and trained models are available at
https://wang-ps.github.io/octformer.
</p></li>
</ul>
<h3>Title: Tracking through Containers and Occluders in the Wild. (arXiv:2305.03052v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03052">http://arxiv.org/abs/2305.03052</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03052] Tracking through Containers and Occluders in the Wild](http://arxiv.org/abs/2305.03052) #transformer</code></li>
<li>Summary: <p>Tracking objects with persistence in cluttered and dynamic environments
remains a difficult challenge for computer vision systems. In this paper, we
introduce $\textbf{TCOW}$, a new benchmark and model for visual tracking
through heavy occlusion and containment. We set up a task where the goal is to,
given a video sequence, segment both the projected extent of the target object,
as well as the surrounding container or occluder whenever one exists. To study
this task, we create a mixture of synthetic and annotated real datasets to
support both supervised learning and structured evaluation of model performance
under various forms of task variation, such as moving or nested containment. We
evaluate two recent transformer-based video models and find that while they can
be surprisingly capable of tracking targets under certain settings of task
variation, there remains a considerable performance gap before we can claim a
tracking model to have acquired a true notion of object permanence.
</p></li>
</ul>
<h3>Title: A Novel Plagiarism Detection Approach Combining BERT-based Word Embedding, Attention-based LSTMs and an Improved Differential Evolution Algorithm. (arXiv:2305.02374v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02374">http://arxiv.org/abs/2305.02374</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02374] A Novel Plagiarism Detection Approach Combining BERT-based Word Embedding, Attention-based LSTMs and an Improved Differential Evolution Algorithm](http://arxiv.org/abs/2305.02374) #transformer</code></li>
<li>Summary: <p>Detecting plagiarism involves finding similar items in two different sources.
In this article, we propose a novel method for detecting plagiarism that is
based on attention mechanism-based long short-term memory (LSTM) and
bidirectional encoder representations from transformers (BERT) word embedding,
enhanced with optimized differential evolution (DE) method for pre-training and
a focal loss function for training. BERT could be included in a downstream task
and fine-tuned as a task-specific BERT can be included in a downstream task and
fine-tuned as a task-specific structure, while the trained BERT model is
capable of detecting various linguistic characteristics. Unbalanced
classification is one of the primary issues with plagiarism detection. We
suggest a focal loss-based training technique that carefully learns minority
class instances to solve this. Another issue that we tackle is the training
phase itself, which typically employs gradient-based methods like
back-propagation for the learning process and thus suffers from some drawbacks,
including sensitivity to initialization. To initiate the BP process, we suggest
a novel DE algorithm that makes use of a clustering-based mutation operator.
Here, a winning cluster is identified for the current DE population, and a
fresh updating method is used to produce potential answers. We evaluate our
proposed approach on three benchmark datasets ( MSRP, SNLI, and SemEval2014)
and demonstrate that it performs well when compared to both conventional and
population-based methods.
</p></li>
</ul>
<h3>Title: Approximating CKY with Transformers. (arXiv:2305.02386v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02386">http://arxiv.org/abs/2305.02386</a></li>
<li>Code URL: <a href="https://github.com/ghazalkhalighinejad/approximating-cky">https://github.com/ghazalkhalighinejad/approximating-cky</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02386] Approximating CKY with Transformers](http://arxiv.org/abs/2305.02386) #transformer</code></li>
<li>Summary: <p>We investigate the ability of transformer models to approximate the CKY
algorithm, using them to directly predict a parse and thus avoid the CKY
algorithm's cubic dependence on sentence length. We find that on standard
constituency parsing benchmarks this approach achieves competitive or better
performance than comparable parsers that make use of CKY, while being faster.
We also evaluate the viability of this approach for parsing under random PCFGs.
Here we find that performance declines as the grammar becomes more ambiguous,
suggesting that the transformer is not fully capturing the CKY computation.
However, we also find that incorporating additional inductive bias is helpful,
and we propose a novel approach that makes use of gradients with respect to
chart representations in predicting the parse, in analogy with the CKY
algorithm being the subgradient of a partition function variant with respect to
the chart.
</p></li>
</ul>
<h3>Title: Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents. (arXiv:2305.02412v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02412">http://arxiv.org/abs/2305.02412</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02412] Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents](http://arxiv.org/abs/2305.02412) #transformer</code></li>
<li>Summary: <p>Pre-trained large language models (LLMs) capture procedural knowledge about
the world. Recent work has leveraged LLM's ability to generate abstract plans
to simplify challenging control tasks, either by action scoring, or action
modeling (fine-tuning). However, the transformer architecture inherits several
constraints that make it difficult for the LLM to directly serve as the agent:
e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training,
and incompatibility with non-text environments. To maintain compatibility with
a low-level trainable actor, we propose to instead use the knowledge in LLMs to
simplify the control problem, rather than solving it. We propose the Plan,
Eliminate, and Track (PET) framework. The Plan module translates a task
description into a list of high-level sub-tasks. The Eliminate module masks out
irrelevant objects and receptacles from the observation for the current
sub-task. Finally, the Track module determines whether the agent has
accomplished each sub-task. On the AlfWorld instruction following benchmark,
the PET framework leads to a significant 15% improvement over SOTA for
generalization to human goal specifications.
</p></li>
</ul>
<h3>Title: Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge. (arXiv:2305.02459v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02459">http://arxiv.org/abs/2305.02459</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02459] Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge](http://arxiv.org/abs/2305.02459) #transformer</code></li>
<li>Summary: <p>While transformer-based systems have enabled greater accuracies with fewer
training examples, data acquisition obstacles still persist for rare-class
tasks -- when the class label is very infrequent (e.g. < 5% of samples). Active
learning has in general been proposed to alleviate such challenges, but choice
of selection strategy, the criteria by which rare-class examples are chosen,
has not been systematically evaluated. Further, transformers enable iterative
transfer-learning approaches. We propose and investigate transfer- and active
learning solutions to the rare class problem of dissonance detection through
utilizing models trained on closely related tasks and the evaluation of
acquisition strategies, including a proposed probability-of-rare-class (PRC)
approach. We perform these experiments for a specific rare class problem:
collecting language samples of cognitive dissonance from social media. We find
that PRC is a simple and effective strategy to guide annotations and ultimately
improve model accuracy while transfer-learning in a specific order can improve
the cold-start performance of the learner but does not benefit iterations of
active learning.
</p></li>
</ul>
<h3>Title: Learning Language-Specific Layers for Multilingual Machine Translation. (arXiv:2305.02665v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02665">http://arxiv.org/abs/2305.02665</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02665] Learning Language-Specific Layers for Multilingual Machine Translation](http://arxiv.org/abs/2305.02665) #transformer</code></li>
<li>Summary: <p>Multilingual Machine Translation promises to improve translation quality
between non-English languages. This is advantageous for several reasons, namely
lower latency (no need to translate twice), and reduced error cascades (e.g.,
avoiding losing gender and formality information when translating through
English). On the downside, adding more languages reduces model capacity per
language, which is usually countered by increasing the overall model size,
making training harder and inference slower. In this work, we introduce
Language-Specific Transformer Layers (LSLs), which allow us to increase model
capacity, while keeping the amount of computation and the number of parameters
used in the forward pass constant. The key idea is to have some layers of the
encoder be source or target language-specific, while keeping the remaining
layers shared. We study the best way to place these layers using a neural
architecture search inspired approach, and achieve an improvement of 1.3 chrF
(1.5 spBLEU) points over not using LSLs on a separate decoder architecture, and
1.9 chrF (2.2 spBLEU) on a shared decoder one.
</p></li>
</ul>
<h3>Title: BranchNorm: Robustly Scaling Extremely Deep Transformers. (arXiv:2305.02790v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02790">http://arxiv.org/abs/2305.02790</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02790] BranchNorm: Robustly Scaling Extremely Deep Transformers](http://arxiv.org/abs/2305.02790) #transformer</code></li>
<li>Summary: <p>Recently, DeepNorm scales Transformers into extremely deep (i.e., 1000
layers) and reveals the promising potential of deep scaling. To stabilize the
training of deep models, DeepNorm (Wang et al., 2022) attempts to constrain the
model update to a constant value. Although applying such a constraint can
benefit the early stage of model training, it may lead to undertrained models
during the whole training procedure. In this paper, we propose BranchNorm,
which dynamically rescales the non-residual branch of Transformer in accordance
with the training period. BranchNorm not only theoretically stabilizes the
training with smooth gradient norms at the early stage, but also encourages
better convergence in the subsequent training stage. Experiment results on
multiple translation tasks demonstrate that BranchNorm achieves a better
trade-off between training stability and converge performance.
</p></li>
</ul>
<h3>Title: Interpretable Sentence Representation with Variational Autoencoders and Attention. (arXiv:2305.02810v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02810">http://arxiv.org/abs/2305.02810</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02810] Interpretable Sentence Representation with Variational Autoencoders and Attention](http://arxiv.org/abs/2305.02810) #transformer</code></li>
<li>Summary: <p>In this thesis, we develop methods to enhance the interpretability of recent
representation learning techniques in natural language processing (NLP) while
accounting for the unavailability of annotated data. We choose to leverage
Variational Autoencoders (VAEs) due to their efficiency in relating
observations to latent generative factors and their effectiveness in
data-efficient learning and interpretable representation learning. As a first
contribution, we identify and remove unnecessary components in the functioning
scheme of semi-supervised VAEs making them faster, smaller and easier to
design. Our second and main contribution is to use VAEs and Transformers to
build two models with inductive bias to separate information in latent
representations into understandable concepts without annotated data. The first
model, Attention-Driven VAE (ADVAE), is able to separately represent and
control information about syntactic roles in sentences. The second model,
QKVAE, uses separate latent variables to form keys and values for its
Transformer decoder and is able to separate syntactic and semantic information
in its neural representations. In transfer experiments, QKVAE has competitive
performance compared to supervised models and equivalent performance to a
supervised model using 50K annotated samples. Additionally, QKVAE displays
improved syntactic role disentanglement capabilities compared to ADVAE.
Overall, we demonstrate that it is possible to enhance the interpretability of
state-of-the-art deep learning architectures for language modeling with
unannotated data in situations where text data is abundant but annotations are
scarce.
</p></li>
</ul>
<h3>Title: 2x Faster Language Model Pre-training via Masked Structural Growth. (arXiv:2305.02869v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02869">http://arxiv.org/abs/2305.02869</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02869] 2x Faster Language Model Pre-training via Masked Structural Growth](http://arxiv.org/abs/2305.02869) #transformer</code></li>
<li>Summary: <p>Acceleration of large language model pre-training is a critical issue in
present NLP research. In this paper, we focus on speeding up pre-training by
progressively growing from a small Transformer structure to a large one. There
are two main research problems related to progressive growth: growth schedule
and growth operator. For growth schedule, existing work has explored
multi-stage expansion of depth and feedforward layers. However, the impact of
each dimension on the schedule's efficiency is still an open question. For
growth operator, existing work relies on the initialization of new weights to
inherit knowledge, and achieve only non-strict function preservation, limiting
further optimization of training dynamics. To address these issues, we propose
Masked Structural Growth (MSG), including growth schedules involving all
possible dimensions and strictly function-preserving growth operators that is
independent of the initialization of new weights. Experiments show that MSG is
significantly faster than related work: we achieve a speed-up of 80% for
Bert-base and 120% for Bert-large pre-training. Moreover, MSG is able to
improve fine-tuning performances at the same time.
</p></li>
</ul>
<h3>Title: Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs. (arXiv:2305.02440v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02440">http://arxiv.org/abs/2305.02440</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02440] Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs](http://arxiv.org/abs/2305.02440) #transformer</code></li>
<li>Summary: <p>Large language models (LLMs) power many state-of-the-art systems in natural
language processing. However, these models are extremely computationally
expensive, even at inference time, raising the natural question: when is the
extra cost of deploying a larger model worth the anticipated boost in
capabilities? Better understanding this tradeoff fundamentally could benefit
from an inference efficiency metric that is both (i) easily comparable across
models from different providers, and (ii) representative of the true cost of
running queries in an isolated performance environment. Unfortunately, access
to LLMs today is largely restricted to black-box text generation APIs and raw
runtimes measured through this interface do not satisfy these desiderata: model
providers can apply various software and hardware optimizations orthogonal to
the model, and models served on shared infrastructure are susceptible to
performance contention. To circumvent these problems, we propose a new metric
for comparing inference efficiency across models. This metric puts models on
equal footing as though they were served (i) on uniform hardware and software,
and (ii) without performance contention. We call this metric the
\emph{idealized runtime}, and we propose a methodology to efficiently estimate
this metric for autoregressive Transformer models. We also propose cost-aware
variants that incorporate the number of accelerators needed to serve the model.
Using these metrics, we compare ten state-of-the-art LLMs to provide the first
analysis of inference efficiency-capability tradeoffs; we make several
observations from this analysis, including the fact that the superior inference
runtime performance of certain APIs is often a byproduct of optimizations
within the API rather than the underlying model. Our methodology also
facilitates the efficient comparison of different software and hardware stacks.
</p></li>
</ul>
<h3>Title: On the Expressivity Role of LayerNorm in Transformers' Attention. (arXiv:2305.02582v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02582">http://arxiv.org/abs/2305.02582</a></li>
<li>Code URL: <a href="https://github.com/tech-srl/layer_norm_expressivity_role">https://github.com/tech-srl/layer_norm_expressivity_role</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02582] On the Expressivity Role of LayerNorm in Transformers' Attention](http://arxiv.org/abs/2305.02582) #transformer</code></li>
<li>Summary: <p>Layer Normalization (LayerNorm) is an inherent component in all
Transformer-based models. In this paper, we show that LayerNorm is crucial to
the expressivity of the multi-head attention layer that follows it. This is in
contrast to the common belief that LayerNorm's only role is to normalize the
activations during the forward pass, and their gradients during the backward
pass. We consider a geometric interpretation of LayerNorm and show that it
consists of two components: (a) projection of the input vectors to a $d-1$
space that is orthogonal to the $\left[1,1,...,1\right]$ vector, and (b)
scaling of all vectors to the same norm of $\sqrt{d}$. We show that each of
these components is important for the attention layer that follows it in
Transformers: (a) projection allows the attention mechanism to create an
attention query that attends to all keys equally, offloading the need to learn
this operation by the attention; and (b) scaling allows each key to potentially
receive the highest attention, and prevents keys from being "un-select-able".
We show empirically that Transformers do indeed benefit from these properties
of LayeNorm in general language modeling and even in computing simple functions
such as "majority". Our code is available at
https://github.com/tech-srl/layer_norm_expressivity_role .
</p></li>
</ul>
<h3>Title: Hierarchical Transformer for Scalable Graph Learning. (arXiv:2305.02866v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02866">http://arxiv.org/abs/2305.02866</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02866] Hierarchical Transformer for Scalable Graph Learning](http://arxiv.org/abs/2305.02866) #transformer</code></li>
<li>Summary: <p>Graph Transformer is gaining increasing attention in the field of machine
learning and has demonstrated state-of-the-art performance on benchmarks for
graph representation learning. However, as current implementations of Graph
Transformer primarily focus on learning representations of small-scale graphs,
the quadratic complexity of the global self-attention mechanism presents a
challenge for full-batch training when applied to larger graphs. Additionally,
conventional sampling-based methods fail to capture necessary high-level
contextual information, resulting in a significant loss of performance. In this
paper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as a
solution to these challenges. HSGT successfully scales the Transformer
architecture to node representation learning tasks on large-scale graphs, while
maintaining high performance. By utilizing graph hierarchies constructed
through coarsening techniques, HSGT efficiently updates and stores multi-scale
information in node embeddings at different levels. Together with
sampling-based training methods, HSGT effectively captures and aggregates
multi-level information on the hierarchical graph using only Transformer
blocks. Empirical evaluations demonstrate that HSGT achieves state-of-the-art
performance on large-scale benchmarks with graphs containing millions of nodes
with high efficiency.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: Shap-E: Generating Conditional 3D Implicit Functions. (arXiv:2305.02463v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02463">http://arxiv.org/abs/2305.02463</a></li>
<li>Code URL: <a href="https://github.com/openai/shap-e">https://github.com/openai/shap-e</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02463] Shap-E: Generating Conditional 3D Implicit Functions](http://arxiv.org/abs/2305.02463) #generative</code></li>
<li>Summary: <p>We present Shap-E, a conditional generative model for 3D assets. Unlike
recent work on 3D generative models which produce a single output
representation, Shap-E directly generates the parameters of implicit functions
that can be rendered as both textured meshes and neural radiance fields. We
train Shap-E in two stages: first, we train an encoder that deterministically
maps 3D assets into the parameters of an implicit function; second, we train a
conditional diffusion model on outputs of the encoder. When trained on a large
dataset of paired 3D and text data, our resulting models are capable of
generating complex and diverse 3D assets in a matter of seconds. When compared
to Point-E, an explicit generative model over point clouds, Shap-E converges
faster and reaches comparable or better sample quality despite modeling a
higher-dimensional, multi-representation output space. We release model
weights, inference code, and samples at https://github.com/openai/shap-e.
</p></li>
</ul>
<h3>Title: Additive Class Distinction Maps using Branched-GANs. (arXiv:2305.02899v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02899">http://arxiv.org/abs/2305.02899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02899] Additive Class Distinction Maps using Branched-GANs](http://arxiv.org/abs/2305.02899) #generative</code></li>
<li>Summary: <p>We present a new model, training procedure and architecture to create precise
maps of distinction between two classes of images. The objective is to
comprehend, in pixel-wise resolution, the unique characteristics of a class.
These maps can facilitate self-supervised segmentation and objectdetection in
addition to new capabilities in explainable AI (XAI). Our proposed architecture
is based on image decomposition, where the output is the sum of multiple
generative networks (branched-GANs). The distinction between classes is
isolated in a dedicated branch. This approach allows clear, precise and
interpretable visualization of the unique characteristics of each class. We
show how our generic method can be used in several modalities for various
tasks, such as MRI brain tumor extraction, isolating cars in aerial photography
and obtaining feminine and masculine face features. This is a preliminary
report of our initial findings and results.
</p></li>
</ul>
<h3>Title: Controllable Visual-Tactile Synthesis. (arXiv:2305.03051v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03051">http://arxiv.org/abs/2305.03051</a></li>
<li>Code URL: <a href="https://github.com/ruihangao/visual-tactile-synthesis">https://github.com/ruihangao/visual-tactile-synthesis</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03051] Controllable Visual-Tactile Synthesis](http://arxiv.org/abs/2305.03051) #generative</code></li>
<li>Summary: <p>Deep generative models have various content creation applications such as
graphic design, e-commerce, and virtual Try-on. However, current works mainly
focus on synthesizing realistic visual outputs, often ignoring other sensory
modalities, such as touch, which limits physical interaction with users. In
this work, we leverage deep generative models to create a multi-sensory
experience where users can touch and see the synthesized object when sliding
their fingers on a haptic surface. The main challenges lie in the significant
scale discrepancy between vision and touch sensing and the lack of explicit
mapping from touch sensing data to a haptic rendering device. To bridge this
gap, we collect high-resolution tactile data with a GelSight sensor and create
a new visuotactile clothing dataset. We then develop a conditional generative
model that synthesizes both visual and tactile outputs from a single sketch. We
evaluate our method regarding image quality and tactile rendering accuracy.
Finally, we introduce a pipeline to render high-quality visual and tactile
outputs on an electroadhesion-based haptic device for an immersive experience,
allowing for challenging materials and editable sketch inputs.
</p></li>
</ul>
<h3>Title: Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence. (arXiv:2305.03010v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03010">http://arxiv.org/abs/2305.03010</a></li>
<li>Code URL: <a href="https://github.com/hkust-knowcomp/geia">https://github.com/hkust-knowcomp/geia</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03010] Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence](http://arxiv.org/abs/2305.03010) #generative</code></li>
<li>Summary: <p>Sentence-level representations are beneficial for various natural language
processing tasks. It is commonly believed that vector representations can
capture rich linguistic properties. Currently, large language models (LMs)
achieve state-of-the-art performance on sentence embedding. However, some
recent works suggest that vector representations from LMs can cause information
leakage. In this work, we further investigate the information leakage issue and
propose a generative embedding inversion attack (GEIA) that aims to reconstruct
input sequences based only on their sentence embeddings. Given the black-box
access to a language model, we treat sentence embeddings as initial tokens'
representations and train or fine-tune a powerful decoder model to decode the
whole sequences directly. We conduct extensive experiments to demonstrate that
our generative inversion attack outperforms previous embedding inversion
attacks in classification metrics and generates coherent and contextually
similar sentences as the original inputs.
</p></li>
</ul>
<h3>Title: Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03047">http://arxiv.org/abs/2305.03047</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03047] Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](http://arxiv.org/abs/2305.03047) #generative</code></li>
<li>Summary: <p>Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised
fine-tuning (SFT) with human annotations and reinforcement learning from human
feedback (RLHF) to align the output of large language models (LLMs) with human
intentions, ensuring they are helpful, ethical, and reliable. However, this
dependence can significantly constrain the true potential of AI-assistant
agents due to the high cost of obtaining human supervision and the related
issues on quality, reliability, diversity, self-consistency, and undesirable
biases. To address these challenges, we propose a novel approach called
SELF-ALIGN, which combines principle-driven reasoning and the generative power
of LLMs for the self-alignment of AI agents with minimal human supervision. Our
approach encompasses four stages: first, we use an LLM to generate synthetic
prompts, and a topic-guided method to augment the prompt diversity; second, we
use a small set of human-written principles for AI models to follow, and guide
the LLM through in-context learning from demonstrations (of principles
application) to produce helpful, ethical, and reliable responses to user's
queries; third, we fine-tune the original LLM with the high-quality
self-aligned responses so that the resulting model can generate desirable
responses for each query directly without the principle set and the
demonstrations anymore; and finally, we offer a refinement step to address the
issues of overly-brief or indirect responses. Applying SELF-ALIGN to the
LLaMA-65b base language model, we develop an AI assistant named Dromedary. With
fewer than 300 lines of human annotations (including < 200 seed prompts, 16
generic principles, and 5 exemplars for in-context learning). Dromedary
significantly surpasses the performance of several state-of-the-art AI systems,
including Text-Davinci-003 and Alpaca, on benchmark datasets with various
settings.
</p></li>
</ul>
<h3>Title: Tensorizing flows: a tool for variational inference. (arXiv:2305.02460v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02460">http://arxiv.org/abs/2305.02460</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02460] Tensorizing flows: a tool for variational inference](http://arxiv.org/abs/2305.02460) #generative</code></li>
<li>Summary: <p>Fueled by the expressive power of deep neural networks, normalizing flows
have achieved spectacular success in generative modeling, or learning to draw
new samples from a distribution given a finite dataset of training samples.
Normalizing flows have also been applied successfully to variational inference,
wherein one attempts to learn a sampler based on an expression for the
log-likelihood or energy function of the distribution, rather than on data. In
variational inference, the unimodality of the reference Gaussian distribution
used within the normalizing flow can cause difficulties in learning multimodal
distributions. We introduce an extension of normalizing flows in which the
Gaussian reference is replaced with a reference distribution that is
constructed via a tensor network, specifically a matrix product state or tensor
train. We show that by combining flows with tensor networks on difficult
variational inference tasks, we can improve on the results obtained by using
either tool without the other.
</p></li>
</ul>
<h3>Title: Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era. (arXiv:2305.02555v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02555">http://arxiv.org/abs/2305.02555</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02555] Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era](http://arxiv.org/abs/2305.02555) #generative</code></li>
<li>Summary: <p>With various AI tools such as ChatGPT becoming increasingly popular, we are
entering a true AI era. We can foresee that exceptional AI tools will soon reap
considerable profits. A crucial question arise: should AI tools share revenue
with their training data providers in additional to traditional stakeholders
and shareholders? The answer is Yes. Large AI tools, such as large language
models, always require more and better quality data to continuously improve,
but current copyright laws limit their access to various types of data. Sharing
revenue between AI tools and their data providers could transform the current
hostile zero-sum game relationship between AI tools and a majority of
copyrighted data owners into a collaborative and mutually beneficial one, which
is necessary to facilitate the development of a virtuous cycle among AI tools,
their users and data providers that drives forward AI technology and builds a
healthy AI ecosystem. However, current revenue-sharing business models do not
work for AI tools in the forthcoming AI era, since the most widely used metrics
for website-based traffic and action, such as clicks, will be replaced by new
metrics such as prompts and cost per prompt for generative AI tools. A
completely new revenue-sharing business model, which must be almost independent
of AI tools and be easily explained to data providers, needs to establish a
prompt-based scoring system to measure data engagement of each data provider.
This paper systematically discusses how to build such a scoring system for all
data providers for AI tools based on classification and content similarity
models, and outlines the requirements for AI tools or third parties to build
it. Sharing revenue with data providers using such a scoring system would
encourage more data owners to participate in the revenue-sharing program. This
will be a utilitarian AI era where all parties benefit.
</p></li>
</ul>
<h3>Title: Are VAEs Bad at Reconstructing Molecular Graphs?. (arXiv:2305.03041v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03041">http://arxiv.org/abs/2305.03041</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03041] Are VAEs Bad at Reconstructing Molecular Graphs?](http://arxiv.org/abs/2305.03041) #generative</code></li>
<li>Summary: <p>Many contemporary generative models of molecules are variational
auto-encoders of molecular graphs. One term in their training loss pertains to
reconstructing the input, yet reconstruction capabilities of state-of-the-art
models have not yet been thoroughly compared on a large and chemically diverse
dataset. In this work, we show that when several state-of-the-art generative
models are evaluated under the same conditions, their reconstruction accuracy
is surprisingly low, worse than what was previously reported on seemingly
harder datasets. However, we show that improving reconstruction does not
directly lead to better sampling or optimization performance. Failed
reconstructions from the MoLeR model are usually similar to the inputs,
assembling the same motifs in a different way, and possess similar chemical
properties such as solubility. Finally, we show that the input molecule and its
failed reconstruction are usually mapped by the different encoders to
statistically distinguishable posterior distributions, hinting that posterior
collapse may not fully explain why VAEs are bad at reconstructing molecular
graphs.
</p></li>
</ul>
<h2>label correction</h2>
<h2>noise</h2>
<h3>Title: Contrastive Mean Teacher for Domain Adaptive Object Detectors. (arXiv:2305.03034v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03034">http://arxiv.org/abs/2305.03034</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03034] Contrastive Mean Teacher for Domain Adaptive Object Detectors](http://arxiv.org/abs/2305.03034) #noise</code></li>
<li>Summary: <p>Object detectors often suffer from the domain gap between training (source
domain) and real-world applications (target domain). Mean-teacher self-training
is a powerful paradigm in unsupervised domain adaptation for object detection,
but it struggles with low-quality pseudo-labels. In this work, we identify the
intriguing alignment and synergy between mean-teacher self-training and
contrastive learning. Motivated by this, we propose Contrastive Mean Teacher
(CMT) -- a unified, general-purpose framework with the two paradigms naturally
integrated to maximize beneficial learning signals. Instead of using
pseudo-labels solely for final predictions, our strategy extracts object-level
features using pseudo-labels and optimizes them via contrastive learning,
without requiring labels in the target domain. When combined with recent
mean-teacher self-training methods, CMT leads to new state-of-the-art
target-domain performance: 51.9% mAP on Foggy Cityscapes, outperforming the
previously best by 2.1% mAP. Notably, CMT can stabilize performance and provide
more significant gains as pseudo-label noise increases.
</p></li>
</ul>
<h3>Title: PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer. (arXiv:2305.02423v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02423">http://arxiv.org/abs/2305.02423</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02423] PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer](http://arxiv.org/abs/2305.02423) #noise</code></li>
<li>Summary: <p>Recent studies show that prompt tuning can better leverage the power of large
language models than fine-tuning on downstream natural language understanding
tasks. However, the existing prompt tuning methods have training instability
issues, as the variance of scores under different random seeds is quite large.
To address this critical problem, we first investigate and find that the loss
landscape of vanilla prompt tuning is precipitous when it is visualized, where
a slight change of input data can cause a big fluctuation in the loss
landscape. This is an essential factor that leads to the instability of prompt
tuning. Based on this observation, we introduce perturbation-based
regularizers, which can smooth the loss landscape, into prompt tuning. We
propose a new algorithm, called Prompt Tuning with Perturbation-based
regularizer~(PTP), which can not only alleviate training instability
dramatically but also boost the performance of prompt tuning. We design two
kinds of perturbation-based regularizers, including random-noise-based and
adversarial-based. In particular, our proposed perturbations are flexible on
both text space and embedding space. Extensive experiments show the
effectiveness of our proposed methods in stabilizing the training. Our new
algorithms improve the state-of-the-art prompt tuning methods by 1.94\% and
2.34\% on SuperGLUE and FewGLUE benchmarks, respectively.
</p></li>
</ul>
<h3>Title: Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02640">http://arxiv.org/abs/2305.02640</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02640] Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders](http://arxiv.org/abs/2305.02640) #noise</code></li>
<li>Summary: <p>In Causal Discovery with latent variables, We define two data paradigms:
definite data: a single-skeleton structure with observed nodes single-value,
and indefinite data: a set of multi-skeleton structures with observed nodes
multi-value. Multi,skeletons induce low sample utilization and multi values
induce incapability of the distribution assumption, both leading that
recovering causal relations from indefinite data is, as of yet, largely
unexplored. We design the causal strength variational model to settle down
these two problems. Specifically, we leverage the causal strength instead of
independent noise as latent variable to mediate evidence lower bound. By this
design ethos, The causal strength of different skeletons is regarded as a
distribution and can be expressed as a single-valued causal graph matrix.
Moreover, considering the latent confounders, we disentangle the causal graph G
into two relatisubgraphs O and C. O contains pure relations between observed
nodes, while C represents the relations from latent variables to observed
nodes. We summarize the above designs as Confounding Disentanglement Causal
Discovery (biCD), which is tailored to learn causal representation from
indefinite data under the latent confounding. Finally, we conduct comprehensive
experiments on synthetic and real-world data to demonstrate the effectiveness
of our method.
</p></li>
</ul>
<h3>Title: Simple Noisy Environment Augmentation for Reinforcement Learning. (arXiv:2305.02882v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02882">http://arxiv.org/abs/2305.02882</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02882] Simple Noisy Environment Augmentation for Reinforcement Learning](http://arxiv.org/abs/2305.02882) #noise</code></li>
<li>Summary: <p>Data augmentation is a widely used technique for improving model performance
in machine learning, particularly in computer vision and natural language
processing. Recently, there has been increasing interest in applying
augmentation techniques to reinforcement learning (RL) problems, with a focus
on image-based augmentation. In this paper, we explore a set of generic
wrappers designed to augment RL environments with noise and encourage agent
exploration and improve training data diversity which are applicable to a broad
spectrum of RL algorithms and environments. Specifically, we concentrate on
augmentations concerning states, rewards, and transition dynamics and introduce
two novel augmentation techniques. In addition, we introduce a noise rate
hyperparameter for control over the frequency of noise injection. We present
experimental results on the impact of these wrappers on return using three
popular RL algorithms, Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), and
Proximal Policy Optimization (PPO), across five MuJoCo environments. To support
the choice of augmentation technique in practice, we also present analysis that
explores the performance these techniques across environments. Lastly, we
publish the wrappers in our noisyenv repository for use with gym environments.
</p></li>
</ul>
<h2>diffusion</h2>
<h3>Title: Multimodal-driven Talking Face Generation, Face Swapping, Diffusion Model. (arXiv:2305.02594v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02594">http://arxiv.org/abs/2305.02594</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02594] Multimodal-driven Talking Face Generation, Face Swapping, Diffusion Model](http://arxiv.org/abs/2305.02594) #diffusion</code></li>
<li>Summary: <p>Multimodal-driven talking face generation refers to animating a portrait with
the given pose, expression, and gaze transferred from the driving image and
video, or estimated from the text and audio. However, existing methods ignore
the potential of text modal, and their generators mainly follow the
source-oriented feature rearrange paradigm coupled with unstable GAN
frameworks. In this work, we first represent the emotion in the text prompt,
which could inherit rich semantics from the CLIP, allowing flexible and
generalized emotion control. We further reorganize these tasks as the
target-oriented texture transfer and adopt the Diffusion Models. More
specifically, given a textured face as the source and the rendered face
projected from the desired 3DMM coefficients as the target, our proposed
Texture-Geometry-aware Diffusion Model decomposes the complex transfer problem
into multi-conditional denoising process, where a Texture Attention-based
module accurately models the correspondences between appearance and geometry
cues contained in source and target conditions, and incorporate extra implicit
information for high-fidelity talking face generation. Additionally, TGDM can
be gracefully tailored for face swapping. We derive a novel paradigm free of
unstable seesaw-style optimization, resulting in simple, stable, and effective
training and inference schemes. Extensive experiments demonstrate the
superiority of our method.
</p></li>
</ul>
<h3>Title: Personalize Segment Anything Model with One Shot. (arXiv:2305.03048v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03048">http://arxiv.org/abs/2305.03048</a></li>
<li>Code URL: <a href="https://github.com/zrrskywalker/personalize-sam">https://github.com/zrrskywalker/personalize-sam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03048] Personalize Segment Anything Model with One Shot](http://arxiv.org/abs/2305.03048) #diffusion</code></li>
<li>Summary: <p>Driven by large-data pre-training, Segment Anything Model (SAM) has been
demonstrated as a powerful and promptable framework, revolutionizing the
segmentation models. Despite the generality, customizing SAM for specific
visual concepts without man-powered prompting is under explored, e.g.,
automatically segmenting your pet dog in different images. In this paper, we
propose a training-free Personalization approach for SAM, termed as PerSAM.
Given only a single image with a reference mask, PerSAM first localizes the
target concept by a location prior, and segments it within other images or
videos via three techniques: target-guided attention, target-semantic
prompting, and cascaded post-refinement. In this way, we effectively adapt SAM
for private use without any training. To further alleviate the mask ambiguity,
we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the
entire SAM, we introduce two learnable weights for multi-scale masks, only
training 2 parameters within 10 seconds for improved performance. To
demonstrate our efficacy, we construct a new segmentation dataset, PerSeg, for
personalized evaluation, and test our methods on video object segmentation with
competitive performance. Besides, our approach can also enhance DreamBooth to
personalize Stable Diffusion for text-to-image generation, which discards the
background disturbance for better target appearance learning. Code is released
at https://github.com/ZrrSkywalker/Personalize-SAM
</p></li>
</ul>
<h2>LLM</h2>
<h2>segmentation</h2>
<h3>Title: Unsupervised Domain Adaptation for Neuron Membrane Segmentation based on Structural Features. (arXiv:2305.02569v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02569">http://arxiv.org/abs/2305.02569</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02569] Unsupervised Domain Adaptation for Neuron Membrane Segmentation based on Structural Features](http://arxiv.org/abs/2305.02569) #segmentation</code></li>
<li>Summary: <p>AI-enhanced segmentation of neuronal boundaries in electron microscopy (EM)
images is crucial for automatic and accurate neuroinformatics studies. To
enhance the limited generalization ability of typical deep learning frameworks
for medical image analysis, unsupervised domain adaptation (UDA) methods have
been applied. In this work, we propose to improve the performance of UDA
methods on cross-domain neuron membrane segmentation in EM images. First, we
designed a feature weight module considering the structural features during
adaptation. Second, we introduced a structural feature-based super-resolution
approach to alleviating the domain gap by adjusting the cross-domain image
resolutions. Third, we proposed an orthogonal decomposition module to
facilitate the extraction of domain-invariant features. Extensive experiments
on two domain adaptive membrane segmentation applications have indicated the
effectiveness of our method.
</p></li>
</ul>
<h3>Title: Text Reading Order in Uncontrolled Conditions by Sparse Graph Segmentation. (arXiv:2305.02577v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02577">http://arxiv.org/abs/2305.02577</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02577] Text Reading Order in Uncontrolled Conditions by Sparse Graph Segmentation](http://arxiv.org/abs/2305.02577) #segmentation</code></li>
<li>Summary: <p>Text reading order is a crucial aspect in the output of an OCR engine, with a
large impact on downstream tasks. Its difficulty lies in the large variation of
domain specific layout structures, and is further exacerbated by real-world
image degradations such as perspective distortions. We propose a lightweight,
scalable and generalizable approach to identify text reading order with a
multi-modal, multi-task graph convolutional network (GCN) running on a sparse
layout based graph. Predictions from the model provide hints of bidimensional
relations among text lines and layout region structures, upon which a
post-processing cluster-and-sort algorithm generates an ordered sequence of all
the text lines. The model is language-agnostic and runs effectively across
multi-language datasets that contain various types of images taken in
uncontrolled conditions, and it is small enough to be deployed on virtually any
platform including mobile devices.
</p></li>
</ul>
<h3>Title: Point2Tree(P2T) -- framework for parameter tuning of semantic and instance segmentation used with mobile laser scanning data in coniferous forest. (arXiv:2305.02651v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02651">http://arxiv.org/abs/2305.02651</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02651] Point2Tree(P2T) -- framework for parameter tuning of semantic and instance segmentation used with mobile laser scanning data in coniferous forest](http://arxiv.org/abs/2305.02651) #segmentation</code></li>
<li>Summary: <p>This article introduces Point2Tree, a novel framework that incorporates a
three-stage process involving semantic segmentation, instance segmentation,
optimization analysis of hyperparemeters importance. It introduces a
comprehensive and modular approach to processing laser points clouds in
Forestry. We tested it on two independent datasets. The first area was located
in an actively managed boreal coniferous dominated forest in V{\aa}ler, Norway,
16 circular plots of 400 square meters were selected to cover a range of forest
conditions in terms of species composition and stand density. We trained a
model based on Pointnet++ architecture which achieves 0.92 F1-score in semantic
segmentation. As a second step in our pipeline we used graph-based approach for
instance segmentation which reached F1-score approx. 0.6. The optimization
allowed to further boost the performance of the pipeline by approx. 4 \%
points.
</p></li>
</ul>
<h3>Title: Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with Uncertainty. (arXiv:2305.02722v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02722">http://arxiv.org/abs/2305.02722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02722] Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with Uncertainty](http://arxiv.org/abs/2305.02722) #segmentation</code></li>
<li>Summary: <p>Knowledge distillation is an effective paradigm for boosting the performance
of pocket-size model, especially when multiple teacher models are available,
the student would break the upper limit again. However, it is not economical to
train diverse teacher models for the disposable distillation. In this paper, we
introduce a new concept dubbed Avatars for distillation, which are the
inference ensemble models derived from the teacher. Concretely, (1) For each
iteration of distillation training, various Avatars are generated by a
perturbation transformation. We validate that Avatars own higher upper limit of
working capacity and teaching ability, aiding the student model in learning
diverse and receptive knowledge perspectives from the teacher model. (2) During
the distillation, we propose an uncertainty-aware factor from the variance of
statistical differences between the vanilla teacher and Avatars, to adjust
Avatars' contribution on knowledge transfer adaptively. Avatar Knowledge
Distillation AKD is fundamentally different from existing methods and refines
with the innovative view of unequal training. Comprehensive experiments
demonstrate the effectiveness of our Avatars mechanism, which polishes up the
state-of-the-art distillation methods for dense prediction without more extra
computational cost. The AKD brings at most 0.7 AP gains on COCO 2017 for Object
Detection and 1.83 mIoU gains on Cityscapes for Semantic Segmentation,
respectively.
</p></li>
</ul>
<h3>Title: FUSegNet: A Deep Convolutional Neural Network for Foot Ulcer Segmentation. (arXiv:2305.02961v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02961">http://arxiv.org/abs/2305.02961</a></li>
<li>Code URL: <a href="https://github.com/mrinal054/fusegnet">https://github.com/mrinal054/fusegnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02961] FUSegNet: A Deep Convolutional Neural Network for Foot Ulcer Segmentation](http://arxiv.org/abs/2305.02961) #segmentation</code></li>
<li>Summary: <p>This paper presents FUSegNet, a new model for foot ulcer segmentation in
diabetes patients, which uses the pre-trained EfficientNet-b7 as a backbone to
address the issue of limited training samples. A modified spatial and channel
squeeze-and-excitation (scSE) module called parallel scSE or P-scSE is proposed
that combines additive and max-out scSE. A new arrangement is introduced for
the module by fusing it in the middle of each decoder stage. As the top decoder
stage carries a limited number of feature maps, max-out scSE is bypassed there
to form a shorted P-scSE. A set of augmentations, comprising geometric,
morphological, and intensity-based augmentations, is applied before feeding the
data into the network. The proposed model is first evaluated on a publicly
available chronic wound dataset where it achieves a data-based dice score of
92.70%, which is the highest score among the reported approaches. The model
outperforms other scSE-based UNet models in terms of Pratt's figure of merits
(PFOM) scores in most categories, which evaluates the accuracy of edge
localization. The model is then tested in the MICCAI 2021 FUSeg challenge,
where a variation of FUSegNet called x-FUSegNet is submitted. The x-FUSegNet
model, which takes the average of outputs obtained by FUSegNet using 5-fold
cross-validation, achieves a dice score of 89.23%, placing it at the top of the
FUSeg Challenge leaderboard. The source code for the model is available on
https://github.com/mrinal054/FUSegNet.
</p></li>
</ul>
<h3>Title: Unsupervised Dialogue Topic Segmentation with Topic-aware Utterance Representation. (arXiv:2305.02747v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02747">http://arxiv.org/abs/2305.02747</a></li>
<li>Code URL: <a href="https://github.com/alibabaresearch/damo-convai">https://github.com/alibabaresearch/damo-convai</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02747] Unsupervised Dialogue Topic Segmentation with Topic-aware Utterance Representation](http://arxiv.org/abs/2305.02747) #segmentation</code></li>
<li>Summary: <p>Dialogue Topic Segmentation (DTS) plays an essential role in a variety of
dialogue modeling tasks. Previous DTS methods either focus on semantic
similarity or dialogue coherence to assess topic similarity for unsupervised
dialogue segmentation. However, the topic similarity cannot be fully identified
via semantic similarity or dialogue coherence. In addition, the unlabeled
dialogue data, which contains useful clues of utterance relationships, remains
underexploited. In this paper, we propose a novel unsupervised DTS framework,
which learns topic-aware utterance representations from unlabeled dialogue data
through neighboring utterance matching and pseudo-segmentation. Extensive
experiments on two benchmark datasets (i.e., DialSeg711 and Doc2Dial)
demonstrate that our method significantly outperforms the strong baseline
methods. For reproducibility, we provide our code and data
at:https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial-start.
</p></li>
</ul>
<h2>object detection</h2>
<h3>Title: Learning-based Relational Object Matching Across Views. (arXiv:2305.02398v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02398">http://arxiv.org/abs/2305.02398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02398] Learning-based Relational Object Matching Across Views](http://arxiv.org/abs/2305.02398) #object detection</code></li>
<li>Summary: <p>Intelligent robots require object-level scene understanding to reason about
possible tasks and interactions with the environment. Moreover, many perception
tasks such as scene reconstruction, image retrieval, or place recognition can
benefit from reasoning on the level of objects. While keypoint-based matching
can yield strong results for finding correspondences for images with small to
medium view point changes, for large view point changes, matching semantically
on the object-level becomes advantageous. In this paper, we propose a
learning-based approach which combines local keypoints with novel object-level
features for matching object detections between RGB images. We train our
object-level matching features based on appearance and inter-frame and
cross-frame spatial relations between objects in an associative graph neural
network. We demonstrate our approach in a large variety of views on
realistically rendered synthetic images. Our approach compares favorably to
previous state-of-the-art object-level matching approaches and achieves
improved performance over a pure keypoint-based approach for large view-point
changes.
</p></li>
</ul>
<h3>Title: Aligning Bird-Eye View Representation of Point Cloud Sequences using Scene Flow. (arXiv:2305.02909v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02909">http://arxiv.org/abs/2305.02909</a></li>
<li>Code URL: <a href="https://github.com/quan-dao/pc-corrector">https://github.com/quan-dao/pc-corrector</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02909] Aligning Bird-Eye View Representation of Point Cloud Sequences using Scene Flow](http://arxiv.org/abs/2305.02909) #object detection</code></li>
<li>Summary: <p>Low-resolution point clouds are challenging for object detection methods due
to their sparsity. Densifying the present point cloud by concatenating it with
its predecessors is a popular solution to this challenge. Such concatenation is
possible thanks to the removal of ego vehicle motion using its odometry. This
method is called Ego Motion Compensation (EMC). Thanks to the added points, EMC
significantly improves the performance of single-frame detectors. However, it
suffers from the shadow effect that manifests in dynamic objects' points
scattering along their trajectories. This effect results in a misalignment
between feature maps and objects' locations, thus limiting performance
improvement to stationary and slow-moving objects only. Scene flow allows
aligning point clouds in 3D space, thus naturally resolving the misalignment in
feature spaces. By observing that scene flow computation shares several
components with 3D object detection pipelines, we develop a plug-in module that
enables single-frame detectors to compute scene flow to rectify their Bird-Eye
View representation. Experiments on the NuScenes dataset show that our module
leads to a significant increase (up to 16%) in the Average Precision of large
vehicles, which interestingly demonstrates the most severe shadow effect. The
code is published at https://github.com/quan-dao/pc-corrector.
</p></li>
</ul>
<h3>Title: OSDaR23: Open Sensor Data for Rail 2023. (arXiv:2305.03001v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03001">http://arxiv.org/abs/2305.03001</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03001] OSDaR23: Open Sensor Data for Rail 2023](http://arxiv.org/abs/2305.03001) #object detection</code></li>
<li>Summary: <p>For driverless train operation on mainline railways, several tasks need to be
implemented by technical systems. One of the most challenging tasks is to
monitor the train's driveway and its surroundings for potential obstacles due
to long braking distances. Machine learning algorithms can be used to analyze
data from vision sensors such as infrared (IR) and visual (RGB) cameras,
lidars, and radars to detect objects. Such algorithms require large amounts of
annotated data from objects in the rail environment that may pose potential
obstacles, as well as rail-specific objects such as tracks or catenary poles,
as training data. However, only very few datasets are publicly available and
these available datasets typically involve only a limited number of sensors.
Datasets and trained models from other domains, such as automotive, are useful
but insufficient for object detection in the railway context. Therefore, this
publication presents OSDaR23, a multi-sensor dataset of 21 sequences captured
in Hamburg, Germany, in September 2021. The sensor setup consisted of multiple
calibrated and synchronized IR/RGB cameras, lidars, a radar, and position and
acceleration sensors front-mounted on a railway vehicle. In addition to raw
data, the dataset contains 204091 polyline, polygonal, rectangle and cuboid
annotations for 20 different object classes. This dataset can also be used for
tasks going beyond collision prediction, which are listed in this paper.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-05-05]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
