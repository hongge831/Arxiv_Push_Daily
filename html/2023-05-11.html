<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Change Detection Methods for Remote Sensing in the Last Decade: A Comprehensive Review. (arXiv:2305.05813v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05813">http://arxiv.org/abs/2305.05813</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05813] Change Detection Methods for Remote Sensing in the Last Decade: A Comprehensive Review](http://arxiv.org/abs/2305.05813) #transformer</code></li>
<li>Summary: <p>Change detection is an essential and widely utilized task in remote sensing
that aims to detect and analyze changes occurring in the same geographical area
over time, which has broad applications in urban development, agricultural
surveys, and land cover monitoring. Detecting changes in remote sensing images
is a complex challenge due to various factors, including variations in image
quality, noise, registration errors, illumination changes, complex landscapes,
and spatial heterogeneity. In recent years, deep learning has emerged as a
powerful tool for feature extraction and addressing these challenges. Its
versatility has resulted in its widespread adoption for numerous
image-processing tasks. This paper presents a comprehensive survey of
significant advancements in change detection for remote sensing images over the
past decade. We first introduce some preliminary knowledge for the change
detection task, such as problem definition, datasets, evaluation metrics, and
transformer basics, as well as provide a detailed taxonomy of existing
algorithms from three different perspectives: algorithm granularity,
supervision modes, and learning frameworks in the methodology section. This
survey enables readers to gain systematic knowledge of change detection tasks
from various angles. We then summarize the state-of-the-art performance on
several dominant change detection datasets, providing insights into the
strengths and limitations of existing algorithms. Based on our survey, some
future research directions for change detection in remote sensing are well
identified. This survey paper will shed some light on the community and inspire
further research efforts in the change detection task.
</p></li>
</ul>
<h3>Title: MMoT: Mixture-of-Modality-Tokens Transformer for Composed Multimodal Conditional Image Synthesis. (arXiv:2305.05992v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05992">http://arxiv.org/abs/2305.05992</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05992] MMoT: Mixture-of-Modality-Tokens Transformer for Composed Multimodal Conditional Image Synthesis](http://arxiv.org/abs/2305.05992) #transformer</code></li>
<li>Summary: <p>Existing multimodal conditional image synthesis (MCIS) methods generate
images conditioned on any combinations of various modalities that require all
of them must be exactly conformed, hindering the synthesis controllability and
leaving the potential of cross-modality under-exploited. To this end, we
propose to generate images conditioned on the compositions of multimodal
control signals, where modalities are imperfectly complementary, i.e., composed
multimodal conditional image synthesis (CMCIS). Specifically, we observe two
challenging issues of the proposed CMCIS task, i.e., the modality coordination
problem and the modality imbalance problem. To tackle these issues, we
introduce a Mixture-of-Modality-Tokens Transformer (MMoT) that adaptively fuses
fine-grained multimodal control signals, a multimodal balanced training loss to
stabilize the optimization of each modality, and a multimodal sampling guidance
to balance the strength of each modality control signal. Comprehensive
experimental results demonstrate that MMoT achieves superior performance on
both unimodal conditional image synthesis (UCIS) and MCIS tasks with
high-quality and faithful image synthesis on complex multimodal conditions. The
project website is available at https://jabir-zheng.github.io/MMoT.
</p></li>
</ul>
<h3>Title: Brain Tumor Detection using Swin Transformers. (arXiv:2305.06025v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06025">http://arxiv.org/abs/2305.06025</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06025] Brain Tumor Detection using Swin Transformers](http://arxiv.org/abs/2305.06025) #transformer</code></li>
<li>Summary: <p>The first MRI scan was done in the year 1978 by researchers at EML
Laboratories. As per an estimate, approximately 251,329 people died due to
primary cancerous brain and CNS (Central Nervous System) Tumors in the year</li>
<li>It has been recommended by various medical professionals that brain tumor
detection at an early stage would help in saving many lives. Whenever
radiologists deal with a brain MRI they try to diagnose it with the
histological subtype which is quite subjective and here comes the major issue.
Upon that, in developing countries like India, where there is 1 doctor for
every 1151 people, the need for efficient diagnosis to help radiologists and
doctors come into picture. In our approach, we aim to solve the problem using
swin transformers and deep learning to detect, classify, locate and provide the
size of the tumor in the particular MRI scan which would assist the doctors and
radiologists in increasing their efficiency. At the end, the medics would be
able to download the predictions and measures in a PDF (Portable Document
Format). Keywords: brain tumor, transformers, classification, medical, deep
learning, detection
</p></li>
</ul>
<h3>Title: VTPNet for 3D deep learning on point cloud. (arXiv:2305.06115v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06115">http://arxiv.org/abs/2305.06115</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06115] VTPNet for 3D deep learning on point cloud](http://arxiv.org/abs/2305.06115) #transformer</code></li>
<li>Summary: <p>Recently, Transformer-based methods for point cloud learning have achieved
good results on various point cloud learning benchmarks. However, since the
attention mechanism needs to generate three feature vectors of query, key, and
value to calculate attention features, most of the existing Transformer-based
point cloud learning methods usually consume a large amount of computational
time and memory resources when calculating global attention. To address this
problem, we propose a Voxel-Transformer-Point (VTP) Block for extracting local
and global features of point clouds. VTP combines the advantages of
voxel-based, point-based and Transformer-based methods, which consists of
Voxel-Based Branch (V branch), Point-Based Transformer Branch (PT branch) and
Point-Based Branch (P branch). The V branch extracts the coarse-grained
features of the point cloud through low voxel resolution; the PT branch obtains
the fine-grained features of the point cloud by calculating the self-attention
in the local neighborhood and the inter-neighborhood cross-attention; the P
branch uses a simplified MLP network to generate the global location
information of the point cloud. In addition, to enrich the local features of
point clouds at different scales, we set the voxel scale in the V branch and
the neighborhood sphere scale in the PT branch to one large and one small
(large voxel scale \&amp; small neighborhood sphere scale or small voxel scale \&amp;
large neighborhood sphere scale). Finally, we use VTP as the feature extraction
network to construct a VTPNet for point cloud learning, and performs shape
classification, part segmentation, and semantic segmentation tasks on the
ModelNet40, ShapeNet Part, and S3DIS datasets. The experimental results
indicate that VTPNet has good performance in 3D point cloud learning.
</p></li>
</ul>
<h3>Title: Transformer-based model for monocular visual odometry: a video understanding approach. (arXiv:2305.06121v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06121">http://arxiv.org/abs/2305.06121</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06121] Transformer-based model for monocular visual odometry: a video understanding approach](http://arxiv.org/abs/2305.06121) #transformer</code></li>
<li>Summary: <p>Estimating the camera pose given images of a single camera is a traditional
task in mobile robots and autonomous vehicles. This problem is called monocular
visual odometry and it often relies on geometric approaches that require
engineering effort for a specific scenario. Deep learning methods have shown to
be generalizable after proper training and a considerable amount of available
data. Transformer-based architectures have dominated the state-of-the-art in
natural language processing and computer vision tasks, such as image and video
understanding. In this work, we deal with the monocular visual odometry as a
video understanding task to estimate the 6-DoF camera's pose. We contribute by
presenting the TSformer-VO model based on spatio-temporal self-attention
mechanisms to extract features from clips and estimate the motions in an
end-to-end manner. Our approach achieved competitive state-of-the-art
performance compared with geometry-based and deep learning-based methods on the
KITTI visual odometry dataset, outperforming the DeepVO implementation highly
accepted in the visual odometry community.
</p></li>
</ul>
<h3>Title: Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving. (arXiv:2305.06242v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06242">http://arxiv.org/abs/2305.06242</a></li>
<li>Code URL: <a href="https://github.com/opendrivelab/thinktwice">https://github.com/opendrivelab/thinktwice</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06242] Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving](http://arxiv.org/abs/2305.06242) #transformer</code></li>
<li>Summary: <p>End-to-end autonomous driving has made impressive progress in recent years.
Existing methods usually adopt the decoupled encoder-decoder paradigm, where
the encoder extracts hidden features from raw sensor data, and the decoder
outputs the ego-vehicle's future trajectories or actions. Under such a
paradigm, the encoder does not have access to the intended behavior of the ego
agent, leaving the burden of finding out safety-critical regions from the
massive receptive field and inferring about future situations to the decoder.
Even worse, the decoder is usually composed of several simple multi-layer
perceptrons (MLP) or GRUs while the encoder is delicately designed (e.g., a
combination of heavy ResNets or Transformer). Such an imbalanced resource-task
division hampers the learning process.
</p></li>
</ul>
<p>In this work, we aim to alleviate the aforementioned problem by two
principles: (1) fully utilizing the capacity of the encoder; (2) increasing the
capacity of the decoder. Concretely, we first predict a coarse-grained future
position and action based on the encoder features. Then, conditioned on the
position and action, the future scene is imagined to check the ramification if
we drive accordingly. We also retrieve the encoder features around the
predicted coordinate to obtain fine-grained information about the
safety-critical region. Finally, based on the predicted future and the
retrieved salient feature, we refine the coarse-grained position and action by
predicting its offset from ground-truth. The above refinement module could be
stacked in a cascaded fashion, which extends the capacity of the decoder with
spatial-temporal prior knowledge about the conditioned future. We conduct
experiments on the CARLA simulator and achieve state-of-the-art performance in
closed-loop benchmarks. Extensive ablation studies demonstrate the
effectiveness of each proposed module.
</p>

<h3>Title: SoGAR: Self-supervised Spatiotemporal Attention-based Social Group Activity Recognition. (arXiv:2305.06310v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06310">http://arxiv.org/abs/2305.06310</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06310] SoGAR: Self-supervised Spatiotemporal Attention-based Social Group Activity Recognition](http://arxiv.org/abs/2305.06310) #transformer</code></li>
<li>Summary: <p>This paper introduces a novel approach to Social Group Activity Recognition
(SoGAR) using Self-supervised Transformers network that can effectively utilize
unlabeled video data. To extract spatio-temporal information, we create local
and global views with varying frame rates. Our self-supervised objective
ensures that features extracted from contrasting views of the same video are
consistent across spatio-temporal domains. Our proposed approach is efficient
in using transformer-based encoders for alleviating the weakly supervised
setting of group activity recognition. By leveraging the benefits of
transformer models, our approach can model long-term relationships along
spatio-temporal dimensions. Our proposed SoGAR method achieves state-of-the-art
results on three group activity recognition benchmarks, namely JRDB-PAR, NBA,
and Volleyball datasets, surpassing the current state-of-the-art in terms of
F1-score, MCA, and MPCA metrics.
</p></li>
</ul>
<h3>Title: Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception. (arXiv:2305.06324v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06324">http://arxiv.org/abs/2305.06324</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06324] Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception](http://arxiv.org/abs/2305.06324) #transformer</code></li>
<li>Summary: <p>We present Integrated Multimodal Perception (IMP), a simple and scalable
multimodal multi-task training and modeling approach. IMP integrates multimodal
inputs including image, video, text, and audio into a single Transformer
encoder with minimal modality-specific components. IMP makes use of a novel
design that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts
(MoE) for efficient model \&amp; task scaling. We conduct extensive empirical
studies about IMP and reveal the following key insights: 1) performing gradient
descent updates by alternating on diverse heterogeneous modalities, loss
functions, and tasks, while also varying input resolutions, efficiently
improves multimodal understanding. 2) model sparsification with MoE on a single
modality-agnostic encoder substantially improves the performance, outperforming
dense models that use modality-specific encoders or additional fusion layers
and greatly mitigating the conflicts between modalities. IMP achieves
competitive performance on a wide range of downstream tasks including image
classification, video classification, image-text, and video-text retrieval.
Most notably, we train a sparse IMP-MoE-L focusing on video tasks that achieves
new state-of-the-art in zero-shot video classification. Our model achieves
77.0% on Kinetics-400, 76.8% on Kinetics-600, and 76.8% on Kinetics-700
zero-shot classification accuracy, improving the previous state-of-the-art by
+5%, +6.7%, and +5.8%, respectively, while using only 15% of their total
training computational cost.
</p></li>
</ul>
<h3>Title: Multi-Path Transformer is Better: A Case Study on Neural Machine Translation. (arXiv:2305.05948v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05948">http://arxiv.org/abs/2305.05948</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05948] Multi-Path Transformer is Better: A Case Study on Neural Machine Translation](http://arxiv.org/abs/2305.05948) #transformer</code></li>
<li>Summary: <p>For years the model performance in machine learning obeyed a power-law
relationship with the model size. For the consideration of parameter
efficiency, recent studies focus on increasing model depth rather than width to
achieve better performance. In this paper, we study how model width affects the
Transformer model through a parameter-efficient multi-path structure. To better
fuse features extracted from different paths, we add three additional
operations to each sublayer: a normalization at the end of each path, a cheap
operation to produce more features, and a learnable weighted mechanism to fuse
all features flexibly. Extensive experiments on 12 WMT machine translation
tasks show that, with the same number of parameters, the shallower multi-path
model can achieve similar or even better performance than the deeper model. It
reveals that we should pay more attention to the multi-path structure, and
there should be a balance between the model depth and width to train a better
large-scale Transformer.
</p></li>
</ul>
<h3>Title: PAI at SemEval-2023 Task 2: A Universal System for Named Entity Recognition with External Entity Information. (arXiv:2305.06099v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06099">http://arxiv.org/abs/2305.06099</a></li>
<li>Code URL: <a href="https://github.com/diqiuzhuanzhuan/semeval-2023">https://github.com/diqiuzhuanzhuan/semeval-2023</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06099] PAI at SemEval-2023 Task 2: A Universal System for Named Entity Recognition with External Entity Information](http://arxiv.org/abs/2305.06099) #transformer</code></li>
<li>Summary: <p>The MultiCoNER II task aims to detect complex, ambiguous, and fine-grained
named entities in low-context situations and noisy scenarios like the presence
of spelling mistakes and typos for multiple languages. The task poses
significant challenges due to the scarcity of contextual information, the high
granularity of the entities(up to 33 classes), and the interference of noisy
data. To address these issues, our team {\bf PAI} proposes a universal Named
Entity Recognition (NER) system that integrates external entity information to
improve performance. Specifically, our system retrieves entities with
properties from the knowledge base (i.e. Wikipedia) for a given text, then
concatenates entity information with the input sentence and feeds it into
Transformer-based models. Finally, our system wins 2 first places, 4 second
places, and 1 third place out of 13 tracks. The code is publicly available at
\url{https://github.com/diqiuzhuanzhuan/semeval-2023}.
</p></li>
</ul>
<h3>Title: Multi-Task End-to-End Training Improves Conversational Recommendation. (arXiv:2305.06218v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06218">http://arxiv.org/abs/2305.06218</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06218] Multi-Task End-to-End Training Improves Conversational Recommendation](http://arxiv.org/abs/2305.06218) #transformer</code></li>
<li>Summary: <p>In this paper, we analyze the performance of a multitask end-to-end
transformer model on the task of conversational recommendations, which aim to
provide recommendations based on a user's explicit preferences expressed in
dialogue. While previous works in this area adopt complex multi-component
approaches where the dialogue management and entity recommendation tasks are
handled by separate components, we show that a unified transformer model, based
on the T5 text-to-text transformer model, can perform competitively in both
recommending relevant items and generating conversation dialogue. We fine-tune
our model on the ReDIAL conversational movie recommendation dataset, and create
additional training tasks derived from MovieLens (such as the prediction of
movie attributes and related movies based on an input movie), in a multitask
learning setting. Using a series of probe studies, we demonstrate that the
learned knowledge in the additional tasks is transferred to the conversational
setting, where each task leads to a 9%-52% increase in its related probe score.
</p></li>
</ul>
<h3>Title: RECKONING: Reasoning through Dynamic Knowledge Encoding. (arXiv:2305.06349v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06349">http://arxiv.org/abs/2305.06349</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06349] RECKONING: Reasoning through Dynamic Knowledge Encoding](http://arxiv.org/abs/2305.06349) #transformer</code></li>
<li>Summary: <p>Recent studies on transformer-based language models show that they can answer
questions by reasoning over knowledge provided as part of the context (i.e.,
in-context reasoning). However, since the available knowledge is often not
filtered for a particular question, in-context reasoning can be sensitive to
distractor facts, additional content that is irrelevant to a question but that
may be relevant for a different question (i.e., not necessarily random noise).
In these situations, the model fails to distinguish the knowledge that is
necessary to answer the question, leading to spurious reasoning and degraded
performance. This reasoning failure contrasts with the model's apparent ability
to distinguish its contextual knowledge from all the knowledge it has memorized
during pre-training. Following this observation, we propose teaching the model
to reason more robustly by folding the provided contextual knowledge into the
model's parameters before presenting it with a question. Our method, RECKONING,
is a bi-level learning algorithm that teaches language models to reason by
updating their parametric knowledge through back-propagation, allowing them to
then answer questions using the updated parameters. During training, the inner
loop rapidly adapts a copy of the model weights to encode contextual knowledge
into its parameters. In the outer loop, the model learns to uses the updated
weights to reproduce and answer reasoning questions about the memorized
knowledge. Our experiments on two multi-hop reasoning datasets show that
RECKONING's performance improves over the in-context reasoning baseline (by up
to 4.5%). We also find that compared to in-context reasoning, RECKONING
generalizes better to longer reasoning chains unseen during training, is more
robust to distractors in the context, and is more computationally efficient
when multiple questions are asked about the same knowledge.
</p></li>
</ul>
<h3>Title: Inclusive FinTech Lending via Contrastive Learning and Domain Adaptation. (arXiv:2305.05827v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05827">http://arxiv.org/abs/2305.05827</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05827] Inclusive FinTech Lending via Contrastive Learning and Domain Adaptation](http://arxiv.org/abs/2305.05827) #transformer</code></li>
<li>Summary: <p>FinTech lending (e.g., micro-lending) has played a significant role in
facilitating financial inclusion. It has reduced processing times and costs,
enhanced the user experience, and made it possible for people to obtain loans
who may not have qualified for credit from traditional lenders. However, there
are concerns about the potentially biased algorithmic decision-making during
loan screening. Machine learning algorithms used to evaluate credit quality can
be influenced by representation bias in the training data, as we only have
access to the default outcome labels of approved loan applications, for which
the borrowers' socioeconomic characteristics are better than those of rejected
ones. In this case, the model trained on the labeled data performs well on the
historically approved population, but does not generalize well to borrowers of
low socioeconomic background. In this paper, we investigate the problem of
representation bias in loan screening for a real-world FinTech lending
platform. We propose a new Transformer-based sequential loan screening model
with self-supervised contrastive learning and domain adaptation to tackle this
challenging issue. We use contrastive learning to train our feature extractor
on unapproved (unlabeled) loan applications and use domain adaptation to
generalize the performance of our label predictor. We demonstrate the
effectiveness of our model through extensive experimentation in the real-world
micro-lending setting. Our results show that our model significantly promotes
the inclusiveness of funding decisions, while also improving loan screening
accuracy and profit by 7.10% and 8.95%, respectively. We also show that
incorporating the test data into contrastive learning and domain adaptation and
labeling a small ratio of test data can further boost model performance.
</p></li>
</ul>
<h3>Title: Fast Distributed Inference Serving for Large Language Models. (arXiv:2305.05920v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05920">http://arxiv.org/abs/2305.05920</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05920] Fast Distributed Inference Serving for Large Language Models](http://arxiv.org/abs/2305.05920) #transformer</code></li>
<li>Summary: <p>Large language models (LLMs) power a new generation of interactive AI
applications exemplified by ChatGPT. The interactive nature of these
applications demand low job completion time (JCT) for model inference. Existing
LLM serving systems use run-to-completion processing for inference jobs, which
suffers from head-of-line blocking and long JCT. We present FastServe, a
distributed inference serving system for LLMs. FastServe exploits the
autoregressive pattern of LLM inference to enable preemption at the granularity
of each output token. FastServe uses preemptive scheduling to minimize JCT with
a novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi
information-agnostic setting of LLM inference, the scheduler leverages the
input length information to assign an appropriate initial queue for each
arrival job to join. The higher priority queues than the joined queue are
skipped to reduce demotions. We design an efficient GPU memory management
mechanism that proactively offloads and uploads intermediate states between GPU
memory and host memory for LLM inference. We build a system prototype of
FastServe based on NVIDIA FasterTransformer. Experimental results show that
compared to the state-of-the-art solution Orca, FastServe improves the average
and tail JCT by up to 5.1$\times$ and 6.4$\times$, respectively.
</p></li>
</ul>
<h3>Title: XTab: Cross-table Pretraining for Tabular Transformers. (arXiv:2305.06090v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06090">http://arxiv.org/abs/2305.06090</a></li>
<li>Code URL: <a href="https://github.com/bingzhaozhu/xtab">https://github.com/bingzhaozhu/xtab</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06090] XTab: Cross-table Pretraining for Tabular Transformers](http://arxiv.org/abs/2305.06090) #transformer</code></li>
<li>Summary: <p>The success of self-supervised learning in computer vision and natural
language processing has motivated pretraining methods on tabular data. However,
most existing tabular self-supervised learning models fail to leverage
information across multiple data tables and cannot generalize to new tables. In
this work, we introduce XTab, a framework for cross-table pretraining of
tabular transformers on datasets from various domains. We address the challenge
of inconsistent column types and quantities among tables by utilizing
independent featurizers and using federated learning to pretrain the shared
component. Tested on 84 tabular prediction tasks from the OpenML-AutoML
Benchmark (AMLB), we show that (1) XTab consistently boosts the
generalizability, learning speed, and performance of multiple tabular
transformers, (2) by pretraining FT-Transformer via XTab, we achieve superior
performance than other state-of-the-art tabular deep learning models on various
tasks such as regression, binary, and multiclass classification.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: Generative Steganographic Flow. (arXiv:2305.05838v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05838">http://arxiv.org/abs/2305.05838</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05838] Generative Steganographic Flow](http://arxiv.org/abs/2305.05838) #generative</code></li>
<li>Summary: <p>Generative steganography (GS) is a new data hiding manner, featuring direct
generation of stego media from secret data. Existing GS methods are generally
criticized for their poor performances. In this paper, we propose a novel flow
based GS approach -- Generative Steganographic Flow (GSF), which provides
direct generation of stego images without cover image. We take the stego image
generation and secret data recovery process as an invertible transformation,
and build a reversible bijective mapping between input secret data and
generated stego images. In the forward mapping, secret data is hidden in the
input latent of Glow model to generate stego images. By reversing the mapping,
hidden data can be extracted exactly from generated stego images. Furthermore,
we propose a novel latent optimization strategy to improve the fidelity of
stego images. Experimental results show our proposed GSF has far better
performances than SOTA works.
</p></li>
</ul>
<h3>Title: Low-Light Image Enhancement via Structure Modeling and Guidance. (arXiv:2305.05839v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05839">http://arxiv.org/abs/2305.05839</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05839] Low-Light Image Enhancement via Structure Modeling and Guidance](http://arxiv.org/abs/2305.05839) #generative</code></li>
<li>Summary: <p>This paper proposes a new framework for low-light image enhancement by
simultaneously conducting the appearance as well as structure modeling. It
employs the structural feature to guide the appearance enhancement, leading to
sharp and realistic results. The structure modeling in our framework is
implemented as the edge detection in low-light images. It is achieved with a
modified generative model via designing a structure-aware feature extractor and
generator. The detected edge maps can accurately emphasize the essential
structural information, and the edge prediction is robust towards the noises in
dark areas. Moreover, to improve the appearance modeling, which is implemented
with a simple U-Net, a novel structure-guided enhancement module is proposed
with structure-guided feature synthesis layers. The appearance modeling, edge
detector, and enhancement module can be trained end-to-end. The experiments are
conducted on representative datasets (sRGB and RAW domains), showing that our
model consistently achieves SOTA performance on all datasets with the same
architecture.
</p></li>
</ul>
<h3>Title: A Hybrid of Generative and Discriminative Models Based on the Gaussian-coupled Softmax Layer. (arXiv:2305.05912v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05912">http://arxiv.org/abs/2305.05912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05912] A Hybrid of Generative and Discriminative Models Based on the Gaussian-coupled Softmax Layer](http://arxiv.org/abs/2305.05912) #generative</code></li>
<li>Summary: <p>Generative models have advantageous characteristics for classification tasks
such as the availability of unsupervised data and calibrated confidence,
whereas discriminative models have advantages in terms of the simplicity of
their model structures and learning algorithms and their ability to outperform
their generative counterparts. In this paper, we propose a method to train a
hybrid of discriminative and generative models in a single neural network (NN),
which exhibits the characteristics of both models. The key idea is the
Gaussian-coupled softmax layer, which is a fully connected layer with a softmax
activation function coupled with Gaussian distributions. This layer can be
embedded into an NN-based classifier and allows the classifier to estimate both
the class posterior distribution and the class-conditional data distribution.
We demonstrate that the proposed hybrid model can be applied to semi-supervised
learning and confidence calibration.
</p></li>
</ul>
<h3>Title: Post-training Model Quantization Using GANs for Synthetic Data Generation. (arXiv:2305.06052v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06052">http://arxiv.org/abs/2305.06052</a></li>
<li>Code URL: <a href="https://github.com/thanosm97/gsoc2022-openvino">https://github.com/thanosm97/gsoc2022-openvino</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06052] Post-training Model Quantization Using GANs for Synthetic Data Generation](http://arxiv.org/abs/2305.06052) #generative</code></li>
<li>Summary: <p>Quantization is a widely adopted technique for deep neural networks to reduce
the memory and computational resources required. However, when quantized, most
models would need a suitable calibration process to keep their performance
intact, which requires data from the target domain, such as a fraction of the
dataset used in model training and model validation (i.e. calibration dataset).
</p></li>
</ul>
<p>In this study, we investigate the use of synthetic data as a substitute for
the calibration with real data for the quantization method. We propose a data
generation method based on Generative Adversarial Networks that are trained
prior to the model quantization step. We compare the performance of models
quantized using data generated by StyleGAN2-ADA and our pre-trained DiStyleGAN,
with quantization using real data and an alternative data generation method
based on fractal images. Overall, the results of our experiments demonstrate
the potential of leveraging synthetic data for calibration during the
quantization process. In our experiments, the percentage of accuracy
degradation of the selected models was less than 0.6%, with our best
performance achieved on MobileNetV2 (0.05%). The code is available at:
https://github.com/ThanosM97/gsoc2022-openvino
</p>

<h3>Title: Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era. (arXiv:2305.06131v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06131">http://arxiv.org/abs/2305.06131</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06131] Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era](http://arxiv.org/abs/2305.06131) #generative</code></li>
<li>Summary: <p>Generative AI (AIGC, a.k.a. AI generated content) has made remarkable
progress in the past few years, among which text-guided content generation is
the most practical one since it enables the interaction between human
instruction and AIGC. Due to the development in text-to-image as well 3D
modeling technologies (like NeRF), text-to-3D has become a newly emerging yet
highly active research field. Our work conducts the first yet comprehensive
survey on text-to-3D to help readers interested in this direction quickly catch
up with its fast development. First, we introduce 3D data representations,
including both Euclidean data and non-Euclidean data. On top of that, we
introduce various foundation technologies as well as summarize how recent works
combine those foundation technologies to realize satisfactory text-to-3D.
Moreover, we summarize how text-to-3D technology is used in various
applications, including avatar generation, texture generation, shape
transformation, and scene generation.
</p></li>
</ul>
<h3>Title: DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation. (arXiv:2305.06225v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06225">http://arxiv.org/abs/2305.06225</a></li>
<li>Code URL: <a href="https://github.com/harlanhong/cvpr2022-dagan">https://github.com/harlanhong/cvpr2022-dagan</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06225] DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation](http://arxiv.org/abs/2305.06225) #generative</code></li>
<li>Summary: <p>Predominant techniques on talking head generation largely depend on 2D
information, including facial appearances and motions from input face images.
Nevertheless, dense 3D facial geometry, such as pixel-wise depth, plays a
critical role in constructing accurate 3D facial structures and suppressing
complex background noises for generation. However, dense 3D annotations for
facial videos is prohibitively costly to obtain. In this work, firstly, we
present a novel self-supervised method for learning dense 3D facial geometry
(ie, depth) from face videos, without requiring camera parameters and 3D
geometry annotations in training. We further propose a strategy to learn
pixel-level uncertainties to perceive more reliable rigid-motion pixels for
geometry learning. Secondly, we design an effective geometry-guided facial
keypoint estimation module, providing accurate keypoints for generating motion
fields. Lastly, we develop a 3D-aware cross-modal (ie, appearance and depth)
attention mechanism, which can be applied to each generation layer, to capture
facial geometries in a coarse-to-fine manner. Extensive experiments are
conducted on three challenging benchmarks (ie, VoxCeleb1, VoxCeleb2, and HDTF).
The results demonstrate that our proposed framework can generate highly
realistic-looking reenacted talking videos, with new state-of-the-art
performances established on these benchmarks. The codes and trained models are
publicly available on the GitHub project page at
https://github.com/harlanhong/CVPR2022-DaGAN
</p></li>
</ul>
<h3>Title: CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors. (arXiv:2305.05711v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05711">http://arxiv.org/abs/2305.05711</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05711] CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors](http://arxiv.org/abs/2305.05711) #generative</code></li>
<li>Summary: <p>Large language models (LLMs) pre-trained on massive corpora have demonstrated
impressive few-shot learning ability on many NLP tasks. A common practice is to
recast the task into a text-to-text format such that generative LLMs of natural
language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is
nontrivial to perform information extraction (IE) tasks with NL-LLMs since the
output of the IE task is usually structured and therefore is hard to be
converted into plain text. In this paper, we propose to recast the structured
output in the form of code instead of natural language and utilize generative
LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular,
named entity recognition and relation extraction. In contrast to NL-LLMs, we
show that Code-LLMs can be well-aligned with these IE tasks by designing
code-style prompts and formulating these IE tasks as code generation tasks.
Experiment results on seven benchmarks show that our method consistently
outperforms fine-tuning moderate-size pre-trained models specially designed for
IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further
conduct a series of in-depth analyses to demonstrate the merits of leveraging
Code-LLMs for IE tasks.
</p></li>
</ul>
<h3>Title: Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks. (arXiv:2305.05862v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05862">http://arxiv.org/abs/2305.05862</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05862] Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks](http://arxiv.org/abs/2305.05862) #generative</code></li>
<li>Summary: <p>The most recent large language models such as ChatGPT and GPT-4 have garnered
significant attention, as they are capable of generating high-quality responses
to human input. Despite the extensive testing of ChatGPT and GPT-4 on generic
text corpora, showcasing their impressive capabilities, a study focusing on
financial corpora has not been conducted. In this study, we aim to bridge this
gap by examining the potential of ChatGPT and GPT-4 as a solver for typical
financial text analytic problems in the zero-shot or few-shot setting.
Specifically, we assess their capabilities on four representative tasks over
five distinct financial textual datasets. The preliminary study shows that
ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition
(NER) and sentiment analysis, where domain-specific knowledge is required,
while they excel in numerical reasoning tasks. We report both the strengths and
limitations of the current versions of ChatGPT and GPT-4, comparing them to the
state-of-the-art finetuned models as well as pretrained domain-specific
generative models. Our experiments provide qualitative studies, through which
we hope to help understand the capability of the existing models and facilitate
further improvements.
</p></li>
</ul>
<h3>Title: CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia. (arXiv:2305.06147v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06147">http://arxiv.org/abs/2305.06147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06147] CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia](http://arxiv.org/abs/2305.06147) #generative</code></li>
<li>Summary: <p>Debatepedia is a publicly available dataset consisting of arguments and
counter-arguments on controversial topics that has been widely used for the
single-document query-focused abstractive summarization task in recent years.
However, it has been recently found that this dataset is limited by noise and
even most queries in this dataset do not have any relevance to the respective
document. In this paper, we present a methodology for cleaning the Debatepedia
dataset by leveraging the generative power of large language models to make it
suitable for query-focused abstractive summarization. More specifically, we
harness the language generation capabilities of ChatGPT to regenerate its
queries. We evaluate the effectiveness of the proposed ChatGPT annotated
version of the Debatepedia dataset using several benchmark summarization models
and demonstrate that the newly annotated version of Debatepedia outperforms the
original dataset in terms of both query relevance as well as summary generation
quality. We will make this annotated and cleaned version of the dataset
publicly available.
</p></li>
</ul>
<h3>Title: Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06176">http://arxiv.org/abs/2305.06176</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06176] Fine-tuning Language Models with Generative Adversarial Feedback](http://arxiv.org/abs/2305.06176) #generative</code></li>
<li>Summary: <p>Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to
significantly enhance the performance of large language models (LLMs) by
aligning their outputs with desired human values. However, RLHF is constrained
by the expertise and productivity limitations of human evaluators. In this
study, we investigate an alternative approach: Reinforcement Learning with
Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings
indicate that RLGAF can help align LLMs outputs while not suffering from the
inherent restrictions of RLHF, suggesting promising avenues for further
research on automating AI alignment.
</p></li>
</ul>
<h3>Title: Automatic Evaluation of Attribution by Large Language Models. (arXiv:2305.06311v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06311">http://arxiv.org/abs/2305.06311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06311] Automatic Evaluation of Attribution by Large Language Models](http://arxiv.org/abs/2305.06311) #generative</code></li>
<li>Summary: <p>A recent focus of large language model (LLM) development, as exemplified by
generative search engines, is to incorporate external references to generate
and support their claims. However, evaluating the attribution, i.e., verifying
whether the generated statement is indeed fully supported by the cited
reference, remains an open problem. Although human evaluation is common
practice, it is costly and time-consuming. In this paper, we investigate the
automatic evaluation of attribution by LLMs. We begin by providing a definition
of attribution and then explore two approaches for automatic evaluation:
prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed
from related tasks, such as question answering, fact-checking, natural language
inference, and summarization. To facilitate the evaluation, we manually curate
a set of test examples covering 12 domains from a generative search engine, New
Bing. Our results on the curated test set and simulated test examples from
existing benchmark questions highlight both promising signals as well as
remaining challenges for the automatic evaluation of attribution. We hope our
testbed, modeling methodology, and insights will help lay the foundation for
future studies on this important problem.
</p></li>
</ul>
<h3>Title: Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files. (arXiv:2305.05708v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05708">http://arxiv.org/abs/2305.05708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05708] Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files](http://arxiv.org/abs/2305.05708) #generative</code></li>
<li>Summary: <p>Language models are powerful tools for molecular design. Currently, the
dominant paradigm is to parse molecular graphs into linear string
representations that can easily be trained on. This approach has been very
successful, however, it is limited to chemical structures that can be
completely represented by a graph -- like organic molecules -- while materials
and biomolecular structures like protein binding sites require a more complete
representation that includes the relative positioning of their atoms in space.
In this work, we show how language models, without any architecture
modifications, trained using next-token prediction -- can generate novel and
valid structures in three dimensions from various substantially different
distributions of chemical structures. In particular, we demonstrate that
language models trained directly on sequences derived directly from chemical
file formats like XYZ files, Crystallographic Information files (CIFs), or
Protein Data Bank files (PDBs) can directly generate molecules, crystals, and
protein binding sites in three dimensions. Furthermore, despite being trained
on chemical file sequences -- language models still achieve performance
comparable to state-of-the-art models that use graph and graph-derived string
representations, as well as other domain-specific 3D generative models. In
doing so, we demonstrate that it is not necessary to use simplified molecular
representations to train chemical language models -- that they are powerful
generative models capable of directly exploring chemical space in three
dimensions for very different structures.
</p></li>
</ul>
<h2>label correction</h2>
<h2>noise</h2>
<h3>Title: SHS-Net: Learning Signed Hyper Surfaces for Oriented Normal Estimation of Point Clouds. (arXiv:2305.05873v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05873">http://arxiv.org/abs/2305.05873</a></li>
<li>Code URL: <a href="https://github.com/leoqli/shs-net">https://github.com/leoqli/shs-net</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05873] SHS-Net: Learning Signed Hyper Surfaces for Oriented Normal Estimation of Point Clouds](http://arxiv.org/abs/2305.05873) #noise</code></li>
<li>Summary: <p>We propose a novel method called SHS-Net for oriented normal estimation of
point clouds by learning signed hyper surfaces, which can accurately predict
normals with global consistent orientation from various point clouds. Almost
all existing methods estimate oriented normals through a two-stage pipeline,
i.e., unoriented normal estimation and normal orientation, and each step is
implemented by a separate algorithm. However, previous methods are sensitive to
parameter settings, resulting in poor results from point clouds with noise,
density variations and complex geometries. In this work, we introduce signed
hyper surfaces (SHS), which are parameterized by multi-layer perceptron (MLP)
layers, to learn to estimate oriented normals from point clouds in an
end-to-end manner. The signed hyper surfaces are implicitly learned in a
high-dimensional feature space where the local and global information is
aggregated. Specifically, we introduce a patch encoding module and a shape
encoding module to encode a 3D point cloud into a local latent code and a
global latent code, respectively. Then, an attention-weighted normal prediction
module is proposed as a decoder, which takes the local and global latent codes
as input to predict oriented normals. Experimental results show that our
SHS-Net outperforms the state-of-the-art methods in both unoriented and
oriented normal estimation on the widely used benchmarks. The code, data and
pretrained models are publicly available.
</p></li>
</ul>
<h3>Title: Text-guided High-definition Consistency Texture Model. (arXiv:2305.05901v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05901">http://arxiv.org/abs/2305.05901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05901] Text-guided High-definition Consistency Texture Model](http://arxiv.org/abs/2305.05901) #noise</code></li>
<li>Summary: <p>With the advent of depth-to-image diffusion models, text-guided generation,
editing, and transfer of realistic textures are no longer difficult. However,
due to the limitations of pre-trained diffusion models, they can only create
low-resolution, inconsistent textures. To address this issue, we present the
High-definition Consistency Texture Model (HCTM), a novel method that can
generate high-definition and consistent textures for 3D meshes according to the
text prompts. We achieve this by leveraging a pre-trained depth-to-image
diffusion model to generate single viewpoint results based on the text prompt
and a depth map. We fine-tune the diffusion model with Parameter-Efficient
Fine-Tuning to quickly learn the style of the generated result, and apply the
multi-diffusion strategy to produce high-resolution and consistent results from
different viewpoints. Furthermore, we propose a strategy that prevents the
appearance of noise on the textures caused by backpropagation. Our proposed
approach has demonstrated promising results in generating high-definition and
consistent textures for 3D meshes, as demonstrated through a series of
experiments.
</p></li>
</ul>
<h3>Title: iEdit: Localised Text-guided Image Editing with Weak Supervision. (arXiv:2305.05947v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05947">http://arxiv.org/abs/2305.05947</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05947] iEdit: Localised Text-guided Image Editing with Weak Supervision](http://arxiv.org/abs/2305.05947) #noise</code></li>
<li>Summary: <p>Diffusion models (DMs) can generate realistic images with text guidance using
large-scale datasets. However, they demonstrate limited controllability in the
output space of the generated images. We propose a novel learning method for
text-guided image editing, namely \texttt{iEdit}, that generates images
conditioned on a source image and a textual edit prompt. As a fully-annotated
dataset with target images does not exist, previous approaches perform
subject-specific fine-tuning at test time or adopt contrastive learning without
a target image, leading to issues on preserving the fidelity of the source
image. We propose to automatically construct a dataset derived from LAION-5B,
containing pseudo-target images with their descriptive edit prompts given input
image-caption pairs. This dataset gives us the flexibility of introducing a
weakly-supervised loss function to generate the pseudo-target image from the
latent noise of the source image conditioned on the edit prompt. To encourage
localised editing and preserve or modify spatial structures in the image, we
propose a loss function that uses segmentation masks to guide the editing
during training and optionally at inference. Our model is trained on the
constructed dataset with 200K samples and constrained GPU resources. It shows
favourable results against its counterparts in terms of image fidelity, CLIP
alignment score and qualitatively for editing both generated and real images.
</p></li>
</ul>
<h3>Title: DMNR: Unsupervised De-noising of Point Clouds Corrupted by Airborne Particles. (arXiv:2305.05991v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05991">http://arxiv.org/abs/2305.05991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05991] DMNR: Unsupervised De-noising of Point Clouds Corrupted by Airborne Particles](http://arxiv.org/abs/2305.05991) #noise</code></li>
<li>Summary: <p>LiDAR sensors are critical for autonomous driving and robotics applications
due to their ability to provide accurate range measurements and their
robustness to lighting conditions. However, airborne particles, such as fog,
rain, snow, and dust, will degrade its performance and it is inevitable to
encounter these inclement environmental conditions outdoors. It would be a
straightforward approach to remove them by supervised semantic segmentation.
But annotating these particles point wisely is too laborious. To address this
problem and enhance the perception under inclement conditions, we develop two
dynamic filtering methods called Dynamic Multi-threshold Noise Removal (DMNR)
and DMNR-H by accurate analysis of the position distribution and intensity
characteristics of noisy points and clean points on publicly available WADS and
DENSE datasets. Both DMNR and DMNR-H outperform state-of-the-art unsupervised
methods by a significant margin on the two datasets and are slightly better
than supervised deep learning-based methods. Furthermore, our methods are more
robust to different LiDAR sensors and airborne particles, such as snow and fog.
</p></li>
</ul>
<h3>Title: The Robustness of Computer Vision Models against Common Corruptions: a Survey. (arXiv:2305.06024v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06024">http://arxiv.org/abs/2305.06024</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06024] The Robustness of Computer Vision Models against Common Corruptions: a Survey](http://arxiv.org/abs/2305.06024) #noise</code></li>
<li>Summary: <p>The performance of computer vision models is susceptible to unexpected
changes in input images when deployed in real scenarios. These changes are
referred to as common corruptions. While they can hinder the applicability of
computer vision models in real-world scenarios, they are not always considered
as a testbed for model generalization and robustness. In this survey, we
present a comprehensive and systematic overview of methods that improve
corruption robustness of computer vision models. Unlike existing surveys that
focus on adversarial attacks and label noise, we cover extensively the study of
robustness to common corruptions that can occur when deploying computer vision
models to work in practical applications. We describe different types of image
corruption and provide the definition of corruption robustness. We then
introduce relevant evaluation metrics and benchmark datasets. We categorize
methods into four groups. We also cover indirect methods that show improvements
in generalization and may improve corruption robustness as a byproduct. We
report benchmark results collected from the literature and find that they are
not evaluated in a unified manner, making it difficult to compare and analyze.
We thus built a unified benchmark framework to obtain directly comparable
results on benchmark datasets. Furthermore, we evaluate relevant backbone
networks pre-trained on ImageNet using our framework, providing an overview of
the base corruption robustness of existing models to help choose appropriate
backbones for computer vision tasks. We identify that developing methods to
handle a wide range of corruptions and efficiently learn with limited data and
computational resources is crucial for future development. Additionally, we
highlight the need for further investigation into the relationship among
corruption robustness, OOD generalization, and shortcut learning.
</p></li>
</ul>
<h3>Title: Autonomous Stabilization of Retinal Videos for Streamlining Assessment of Spontaneous Venous Pulsations. (arXiv:2305.06043v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06043">http://arxiv.org/abs/2305.06043</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06043] Autonomous Stabilization of Retinal Videos for Streamlining Assessment of Spontaneous Venous Pulsations](http://arxiv.org/abs/2305.06043) #noise</code></li>
<li>Summary: <p>Spontaneous retinal Venous Pulsations (SVP) are rhythmic changes in the
caliber of the central retinal vein and are observed in the optic disc region
(ODR) of the retina. Its absence is a critical indicator of various ocular or
neurological abnormalities. Recent advances in imaging technology have enabled
the development of portable smartphone-based devices for observing the retina
and assessment of SVPs. However, the quality of smartphone-based retinal videos
is often poor due to noise and image jitting, which in return, can severely
obstruct the observation of SVPs. In this work, we developed a fully automated
retinal video stabilization method that enables the examination of SVPs
captured by various mobile devices. Specifically, we first propose an ODR
Spatio-Temporal Localization (ODR-STL) module to localize visible ODR and
remove noisy and jittering frames. Then, we introduce a Noise-Aware Template
Matching (NATM) module to stabilize high-quality video segments at a fixed
position in the field of view. After the processing, the SVPs can be easily
observed in the stabilized videos, significantly facilitating user
observations. Furthermore, our method is cost-effective and has been tested in
both subjective and objective evaluations. Both of the evaluations support its
effectiveness in facilitating the observation of SVPs. This can improve the
timely diagnosis and treatment of associated diseases, making it a valuable
tool for eye health professionals.
</p></li>
</ul>
<h3>Title: Towards Effective Visual Representations for Partial-Label Learning. (arXiv:2305.06080v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06080">http://arxiv.org/abs/2305.06080</a></li>
<li>Code URL: <a href="https://github.com/alphaxia/papi">https://github.com/alphaxia/papi</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06080] Towards Effective Visual Representations for Partial-Label Learning](http://arxiv.org/abs/2305.06080) #noise</code></li>
<li>Summary: <p>Under partial-label learning (PLL) where, for each training instance, only a
set of ambiguous candidate labels containing the unknown true label is
accessible, contrastive learning has recently boosted the performance of PLL on
vision tasks, attributed to representations learned by contrasting the
same/different classes of entities. Without access to true labels, positive
points are predicted using pseudo-labels that are inherently noisy, and
negative points often require large batches or momentum encoders, resulting in
unreliable similarity information and a high computational overhead. In this
paper, we rethink a state-of-the-art contrastive PLL method PiCO[24], inspiring
the design of a simple framework termed PaPi (Partial-label learning with a
guided Prototypical classifier), which demonstrates significant scope for
improvement in representation learning, thus contributing to label
disambiguation. PaPi guides the optimization of a prototypical classifier by a
linear classifier with which they share the same feature encoder, thus
explicitly encouraging the representation to reflect visual similarity between
categories. It is also technically appealing, as PaPi requires only a few
components in PiCO with the opposite direction of guidance, and directly
eliminates the contrastive learning module that would introduce noise and
consume computational resources. We empirically demonstrate that PaPi
significantly outperforms other PLL methods on various image classification
tasks.
</p></li>
</ul>
<h3>Title: Spectrum Breathing: Protecting Over-the-Air Federated Learning Against Interference. (arXiv:2305.05933v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05933">http://arxiv.org/abs/2305.05933</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05933] Spectrum Breathing: Protecting Over-the-Air Federated Learning Against Interference](http://arxiv.org/abs/2305.05933) #noise</code></li>
<li>Summary: <p>Federated Learning (FL) is a widely embraced paradigm for distilling
artificial intelligence from distributed mobile data. However, the deployment
of FL in mobile networks can be compromised by exposure to interference from
neighboring cells or jammers. Existing interference mitigation techniques
require multi-cell cooperation or at least interference channel state
information, which is expensive in practice. On the other hand, power control
that treats interference as noise may not be effective due to limited power
budgets, and also that this mechanism can trigger countermeasures by
interference sources. As a practical approach for protecting FL against
interference, we propose Spectrum Breathing, which cascades stochastic-gradient
pruning and spread spectrum to suppress interference without bandwidth
expansion. The cost is higher learning latency by exploiting the graceful
degradation of learning speed due to pruning. We synchronize the two operations
such that their levels are controlled by the same parameter, Breathing Depth.
To optimally control the parameter, we develop a martingale-based approach to
convergence analysis of Over-the-Air FL with spectrum breathing, termed
AirBreathing FL. We show a performance tradeoff between gradient-pruning and
interference-induced error as regulated by the breathing depth. Given receive
SIR and model size, the optimization of the tradeoff yields two schemes for
controlling the breathing depth that can be either fixed or adaptive to
channels and the learning process. As shown by experiments, in scenarios where
traditional Over-the-Air FL fails to converge in the presence of strong
interference, AirBreahing FL with either fixed or adaptive breathing depth can
ensure convergence where the adaptive scheme achieves close-to-ideal
performance.
</p></li>
</ul>
<h3>Title: Multi-Object Self-Supervised Depth Denoising. (arXiv:2305.05778v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05778">http://arxiv.org/abs/2305.05778</a></li>
<li>Code URL: <a href="https://github.com/alr-internship/self-supervised-depth-denoising">https://github.com/alr-internship/self-supervised-depth-denoising</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05778] Multi-Object Self-Supervised Depth Denoising](http://arxiv.org/abs/2305.05778) #noise</code></li>
<li>Summary: <p>Depth cameras are frequently used in robotic manipulation, e.g. for visual
servoing. The quality of small and compact depth cameras is though often not
sufficient for depth reconstruction, which is required for precise tracking in
and perception of the robot's working space. Based on the work of Shabanov et
al. (2021), in this work, we present a self-supervised multi-object depth
denoising pipeline, that uses depth maps of higher-quality sensors as
close-to-ground-truth supervisory signals to denoise depth maps coming from a
lower-quality sensor. We display a computationally efficient way to align sets
of two frame pairs in space and retrieve a frame-based multi-object mask, in
order to receive a clean labeled dataset to train a denoising neural network
on. The implementation of our presented work can be found at
https://github.com/alr-internship/self-supervised-depth-denoising.
</p></li>
</ul>
<h3>Title: Rethinking the Value of Labels for Instance-Dependent Label Noise Learning. (arXiv:2305.06247v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06247">http://arxiv.org/abs/2305.06247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06247] Rethinking the Value of Labels for Instance-Dependent Label Noise Learning](http://arxiv.org/abs/2305.06247) #noise</code></li>
<li>Summary: <p>Label noise widely exists in large-scale datasets and significantly
degenerates the performances of deep learning algorithms. Due to the
non-identifiability of the instance-dependent noise transition matrix, most
existing algorithms address the problem by assuming the noisy label generation
process to be independent of the instance features. Unfortunately, noisy labels
in real-world applications often depend on both the true label and the
features. In this work, we tackle instance-dependent label noise with a novel
deep generative model that avoids explicitly modeling the noise transition
matrix. Our algorithm leverages casual representation learning and
simultaneously identifies the high-level content and style latent factors from
the data. By exploiting the supervision information of noisy labels with
structural causal models, our empirical evaluations on a wide range of
synthetic and real-world instance-dependent label noise datasets demonstrate
that the proposed algorithm significantly outperforms the state-of-the-art
counterparts.
</p></li>
</ul>
<h3>Title: Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning. (arXiv:2305.06295v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06295">http://arxiv.org/abs/2305.06295</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06295] Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning](http://arxiv.org/abs/2305.06295) #noise</code></li>
<li>Summary: <p>Clinical diagnosis guidelines aim at specifying the steps that may lead to a
diagnosis. Guidelines enable rationalizing and normalizing clinical decisions
but suffer drawbacks as they are built to cover the majority of the population
and may fail in guiding to the right diagnosis for patients with uncommon
conditions or multiple pathologies. Moreover, their updates are long and
expensive, making them unsuitable to emerging practices. Inspired by
guidelines, we formulate the task of diagnosis as a sequential decision-making
problem and study the use of Deep Reinforcement Learning (DRL) algorithms
trained on Electronic Health Records (EHRs) to learn the optimal sequence of
observations to perform in order to obtain a correct diagnosis. Because of the
variety of DRL algorithms and of their sensitivity to the context, we
considered several approaches and settings that we compared to each other, and
to classical classifiers. We experimented on a synthetic but realistic dataset
to differentially diagnose anemia and its subtypes and particularly evaluated
the robustness of various approaches to noise and missing data as those are
frequent in EHRs. Within the DRL algorithms, Dueling DQN with Prioritized
Experience Replay, and Dueling Double DQN with Prioritized Experience Replay
show the best and most stable performances. In the presence of imperfect data,
the DRL algorithms show competitive, but less stable performances when compared
to the classifiers (Random Forest and XGBoost); although they enable the
progressive generation of a pathway to the suggested diagnosis, which can both
guide or explain the decision process.
</p></li>
</ul>
<h2>diffusion</h2>
<h3>Title: DifFIQA: Face Image Quality Assessment Using Denoising Diffusion Probabilistic Models. (arXiv:2305.05768v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05768">http://arxiv.org/abs/2305.05768</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05768] DifFIQA: Face Image Quality Assessment Using Denoising Diffusion Probabilistic Models](http://arxiv.org/abs/2305.05768) #diffusion</code></li>
<li>Summary: <p>Modern face recognition (FR) models excel in constrained scenarios, but often
suffer from decreased performance when deployed in unconstrained (real-world)
environments due to uncertainties surrounding the quality of the captured
facial data. Face image quality assessment (FIQA) techniques aim to mitigate
these performance degradations by providing FR models with sample-quality
predictions that can be used to reject low-quality samples and reduce false
match errors. However, despite steady improvements, ensuring reliable quality
estimates across facial images with diverse characteristics remains
challenging. In this paper, we present a powerful new FIQA approach, named
DifFIQA, which relies on denoising diffusion probabilistic models (DDPM) and
ensures highly competitive results. The main idea behind the approach is to
utilize the forward and backward processes of DDPMs to perturb facial images
and quantify the impact of these perturbations on the corresponding image
embeddings for quality prediction. Because the diffusion-based perturbations
are computationally expensive, we also distill the knowledge encoded in DifFIQA
into a regression-based quality predictor, called DifFIQA(R), that balances
performance and execution time. We evaluate both models in comprehensive
experiments on 7 datasets, with 4 target FR models and against 10
state-of-the-art FIQA techniques with highly encouraging results. The source
code will be made publicly available.
</p></li>
</ul>
<h3>Title: Comprehensive Dataset of Synthetic and Manipulated Overhead Imagery for Development and Evaluation of Forensic Tools. (arXiv:2305.05784v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05784">http://arxiv.org/abs/2305.05784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05784] Comprehensive Dataset of Synthetic and Manipulated Overhead Imagery for Development and Evaluation of Forensic Tools](http://arxiv.org/abs/2305.05784) #diffusion</code></li>
<li>Summary: <p>We present a first of its kind dataset of overhead imagery for development
and evaluation of forensic tools. Our dataset consists of real, fully synthetic
and partially manipulated overhead imagery generated from a custom diffusion
model trained on two sets of different zoom levels and on two sources of
pristine data. We developed our model to support controllable generation of
multiple manipulation categories including fully synthetic imagery conditioned
on real and generated base maps, and location. We also support partial
in-painted imagery with same conditioning options and with several types of
manipulated content. The data consist of raw images and ground truth
annotations describing the manipulation parameters. We also report benchmark
performance on several tasks supported by our dataset including detection of
fully and partially manipulated imagery, manipulation localization and
classification.
</p></li>
</ul>
<h3>Title: Relightify: Relightable 3D Faces from a Single Image via Diffusion Models. (arXiv:2305.06077v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06077">http://arxiv.org/abs/2305.06077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06077] Relightify: Relightable 3D Faces from a Single Image via Diffusion Models](http://arxiv.org/abs/2305.06077) #diffusion</code></li>
<li>Summary: <p>Following the remarkable success of diffusion models on image generation,
recent works have also demonstrated their impressive ability to address a
number of inverse problems in an unsupervised way, by properly constraining the
sampling process based on a conditioning input. Motivated by this, in this
paper, we present the first approach to use diffusion models as a prior for
highly accurate 3D facial BRDF reconstruction from a single image. We start by
leveraging a high-quality UV dataset of facial reflectance (diffuse and
specular albedo and normals), which we render under varying illumination
settings to simulate natural RGB textures and, then, train an unconditional
diffusion model on concatenated pairs of rendered textures and reflectance
components. At test time, we fit a 3D morphable model to the given image and
unwrap the face in a partial UV texture. By sampling from the diffusion model,
while retaining the observed texture part intact, the model inpaints not only
the self-occluded areas but also the unknown reflectance components, in a
single sequence of denoising steps. In contrast to existing methods, we
directly acquire the observed texture from the input image, thus, resulting in
more faithful and consistent reflectance estimation. Through a series of
qualitative and quantitative comparisons, we demonstrate superior performance
in both texture completion as well as reflectance reconstruction tasks.
</p></li>
</ul>
<h2>LLM</h2>
<h2>segmentation</h2>
<h3>Title: Vision-Language Models in Remote Sensing: Current Progress and Future Trends. (arXiv:2305.05726v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05726">http://arxiv.org/abs/2305.05726</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05726] Vision-Language Models in Remote Sensing: Current Progress and Future Trends](http://arxiv.org/abs/2305.05726) #segmentation</code></li>
<li>Summary: <p>The remarkable achievements of ChatGPT and GPT-4 have sparked a wave of
interest and research in the field of large language models for Artificial
General Intelligence (AGI). These models provide us with intelligent solutions
that are more similar to human thinking, enabling us to use general artificial
intelligence to solve problems in various applications. However, in the field
of remote sensing, the scientific literature on the implementation of AGI
remains relatively scant. Existing AI-related research primarily focuses on
visual understanding tasks while neglecting the semantic understanding of the
objects and their relationships. This is where vision-language models excel, as
they enable reasoning about images and their associated textual descriptions,
allowing for a deeper understanding of the underlying semantics.
Vision-language models can go beyond recognizing the objects in an image and
can infer the relationships between them, as well as generate natural language
descriptions of the image. This makes them better suited for tasks that require
both visual and textual understanding, such as image captioning, text-based
image retrieval, and visual question answering. This paper provides a
comprehensive review of the research on vision-language models in remote
sensing, summarizing the latest progress, highlighting the current challenges,
and identifying potential research opportunities. Specifically, we review the
application of vision-language models in several mainstream remote sensing
tasks, including image captioning, text-based image generation, text-based
image retrieval, visual question answering, scene classification, semantic
segmentation, and object detection. For each task, we briefly describe the task
background and review some representative works. Finally, we summarize the
limitations of existing work and provide some possible directions for future
development.
</p></li>
</ul>
<h3>Title: Unsupervised Domain Adaptation for Semantic Segmentation via Feature-space Density Matching. (arXiv:2305.05789v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05789">http://arxiv.org/abs/2305.05789</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05789] Unsupervised Domain Adaptation for Semantic Segmentation via Feature-space Density Matching](http://arxiv.org/abs/2305.05789) #segmentation</code></li>
<li>Summary: <p>Semantic segmentation is a critical step in automated image interpretation
and analysis where pixels are classified into one or more predefined
semantically meaningful classes. Deep learning approaches for semantic
segmentation rely on harnessing the power of annotated images to learn features
indicative of these semantic classes. Nonetheless, they often fail to
generalize when there is a significant domain (i.e., distributional) shift
between the training (i.e., source) data and the dataset(s) encountered when
deployed (i.e., target), necessitating manual annotations for the target data
to achieve acceptable performance. This is especially important in medical
imaging because different image modalities have significant intra- and
inter-site variations due to protocol and vendor variability. Current
techniques are sensitive to hyperparameter tuning and target dataset size. This
paper presents an unsupervised domain adaptation approach for semantic
segmentation that alleviates the need for annotating target data. Using kernel
density estimation, we match the target data distribution to the source data in
the feature space. We demonstrate that our results are comparable or superior
on multiple-site prostate MRI and histopathology images, which mitigates the
need for annotating target data.
</p></li>
</ul>
<h3>Title: Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05803">http://arxiv.org/abs/2305.05803</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05803] Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation](http://arxiv.org/abs/2305.05803) #segmentation</code></li>
<li>Summary: <p>Weakly Supervised Semantic Segmentation (WSSS) with only image-level
supervision has garnered increasing attention due to its low annotation cost
compared to pixel-level annotation. Most existing methods rely on Class
Activation Maps (CAM) to generate pixel-level pseudo labels for supervised
training. However, it is well known that CAM often suffers from partial
activation -- activating the most discriminative part instead of the entire
object area, and false activation -- unnecessarily activating the background
around the object. In this study, we introduce a simple yet effective approach
to address these limitations by harnessing the recently released Segment
Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is
a segmentation foundation model that demonstrates strong zero-shot ability in
partitioning images into segments but lacks semantic labels for these regions.
To circumvent this, we employ pseudo labels for a specific class as the signal
to select the most relevant masks and label them to generate the refined pseudo
labels for this class. The segments generated by SAM are highly precise,
leading to substantial improvements in partial and false activation. Moreover,
existing post-processing modules for producing pseudo labels, such as
AffinityNet, are often computationally heavy, with a significantly long
training time. Surprisingly, we discovered that using the initial CAM with SAM
can achieve on-par performance as the post-processed pseudo label generated
from these modules with much less computational cost. Our approach is highly
versatile and capable of seamless integration into existing WSSS models without
modification to base networks or pipelines. Despite its simplicity, our
approach improves the mean Intersection over Union (mIoU) of pseudo labels from
five state-of-the-art WSSS methods by 6.2\% on average on the PASCAL VOC 2012
dataset.
</p></li>
</ul>
<h3>Title: A Self-Training Framework Based on Multi-Scale Attention Fusion for Weakly Supervised Semantic Segmentation. (arXiv:2305.05841v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05841">http://arxiv.org/abs/2305.05841</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05841] A Self-Training Framework Based on Multi-Scale Attention Fusion for Weakly Supervised Semantic Segmentation](http://arxiv.org/abs/2305.05841) #segmentation</code></li>
<li>Summary: <p>Weakly supervised semantic segmentation (WSSS) based on image-level labels is
challenging since it is hard to obtain complete semantic regions. To address
this issue, we propose a self-training method that utilizes fused multi-scale
class-aware attention maps. Our observation is that attention maps of different
scales contain rich complementary information, especially for large and small
objects. Therefore, we collect information from attention maps of different
scales and obtain multi-scale attention maps. We then apply denoising and
reactivation strategies to enhance the potential regions and reduce noisy
areas. Finally, we use the refined attention maps to retrain the network.
Experiments showthat our method enables the model to extract rich semantic
information from multi-scale images and achieves 72.4% mIou scores on both the
PASCAL VOC 2012 validation and test sets. The code is available at
https://bupt-ai-cz.github.io/SMAF.
</p></li>
</ul>
<h3>Title: Medical supervised masked autoencoders: Crafting a better masking strategy and efficient fine-tuning schedule for medical image classification. (arXiv:2305.05871v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05871">http://arxiv.org/abs/2305.05871</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05871] Medical supervised masked autoencoders: Crafting a better masking strategy and efficient fine-tuning schedule for medical image classification](http://arxiv.org/abs/2305.05871) #segmentation</code></li>
<li>Summary: <p>Masked autoencoders (MAEs) have displayed significant potential in the
classification and semantic segmentation of medical images in the last year.
Due to the high similarity of human tissues, even slight changes in medical
images may represent diseased tissues, necessitating fine-grained inspection to
pinpoint diseased tissues. The random masking strategy of MAEs is likely to
result in areas of lesions being overlooked by the model. At the same time,
inconsistencies between the pre-training and fine-tuning phases impede the
performance and efficiency of MAE in medical image classification. To address
these issues, we propose a medical supervised masked autoencoder (MSMAE) in
this paper. In the pre-training phase, MSMAE precisely masks medical images via
the attention maps obtained from supervised training, contributing to the
representation learning of human tissue in the lesion area. During the
fine-tuning phase, MSMAE is also driven by attention to the accurate masking of
medical images. This improves the computational efficiency of the MSMAE while
increasing the difficulty of fine-tuning, which indirectly improves the quality
of MSMAE medical diagnosis. Extensive experiments demonstrate that MSMAE
achieves state-of-the-art performance in case with three official medical
datasets for various diseases. Meanwhile, transfer learning for MSMAE also
demonstrates the great potential of our approach for medical semantic
segmentation tasks. Moreover, the MSMAE accelerates the inference time in the
fine-tuning phase by 11.2% and reduces the number of floating-point operations
(FLOPs) by 74.08% compared to a traditional MAE.
</p></li>
</ul>
<h3>Title: Radious: Unveiling the Enigma of Dental Radiology with BEIT Adaptor and Mask2Former in Semantic Segmentation. (arXiv:2305.06236v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06236">http://arxiv.org/abs/2305.06236</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06236] Radious: Unveiling the Enigma of Dental Radiology with BEIT Adaptor and Mask2Former in Semantic Segmentation](http://arxiv.org/abs/2305.06236) #segmentation</code></li>
<li>Summary: <p>X-ray images are the first steps for diagnosing and further treating dental
problems. So, early diagnosis prevents the development and increase of oral and
dental diseases. In this paper, we developed a semantic segmentation algorithm
based on BEIT adaptor and Mask2Former to detect and identify teeth, roots, and
multiple dental diseases and abnormalities such as pulp chamber, restoration,
endodontics, crown, decay, pin, composite, bridge, pulpitis, orthodontics,
radicular cyst, periapical cyst, cyst, implant, and bone graft material in
panoramic, periapical, and bitewing X-ray images. We compared the result of our
algorithm to two state-of-the-art algorithms in image segmentation named:
Deeplabv3 and Segformer on our own data set. We discovered that Radious
outperformed those algorithms by increasing the mIoU scores by 9% and 33% in
Deeplabv3+ and Segformer, respectively.
</p></li>
</ul>
<h3>Title: Self-Supervised Instance Segmentation by Grasping. (arXiv:2305.06305v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06305">http://arxiv.org/abs/2305.06305</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06305] Self-Supervised Instance Segmentation by Grasping](http://arxiv.org/abs/2305.06305) #segmentation</code></li>
<li>Summary: <p>Instance segmentation is a fundamental skill for many robotic applications.
We propose a self-supervised method that uses grasp interactions to collect
segmentation supervision for an instance segmentation model. When a robot
grasps an item, the mask of that grasped item can be inferred from the images
of the scene before and after the grasp. Leveraging this insight, we learn a
grasp segmentation model to segment the grasped object from before and after
grasp images. Such a model can segment grasped objects from thousands of grasp
interactions without costly human annotation. Using the segmented grasped
objects, we can "cut" objects from their original scenes and "paste" them into
new scenes to generate instance supervision. We show that our grasp
segmentation model provides a 5x error reduction when segmenting grasped
objects compared with traditional image subtraction approaches. Combined with
our "cut-and-paste" generation method, instance segmentation models trained
with our method achieve better performance than a model trained with 10x the
amount of labeled data. On a real robotic grasping system, our instance
segmentation model reduces the rate of grasp errors by over 3x compared to an
image subtraction baseline.
</p></li>
</ul>
<h3>Title: Scan2LoD3: Reconstructing semantic 3D building models at LoD3 using ray casting and Bayesian networks. (arXiv:2305.06314v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06314">http://arxiv.org/abs/2305.06314</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06314] Scan2LoD3: Reconstructing semantic 3D building models at LoD3 using ray casting and Bayesian networks](http://arxiv.org/abs/2305.06314) #segmentation</code></li>
<li>Summary: <p>Reconstructing semantic 3D building models at the level of detail (LoD) 3 is
a long-standing challenge. Unlike mesh-based models, they require watertight
geometry and object-wise semantics at the fa\c{c}ade level. The principal
challenge of such demanding semantic 3D reconstruction is reliable
fa\c{c}ade-level semantic segmentation of 3D input data. We present a novel
method, called Scan2LoD3, that accurately reconstructs semantic LoD3 building
models by improving fa\c{c}ade-level semantic 3D segmentation. To this end, we
leverage laser physics and 3D building model priors to probabilistically
identify model conflicts. These probabilistic physical conflicts propose
locations of model openings: Their final semantics and shapes are inferred in a
Bayesian network fusing multimodal probabilistic maps of conflicts, 3D point
clouds, and 2D images. To fulfill demanding LoD3 requirements, we use the
estimated shapes to cut openings in 3D building priors and fit semantic 3D
objects from a library of fa\c{c}ade objects. Extensive experiments on the TUM
city campus datasets demonstrate the superior performance of the proposed
Scan2LoD3 over the state-of-the-art methods in fa\c{c}ade-level detection,
semantic segmentation, and LoD3 building model reconstruction. We believe our
method can foster the development of probability-driven semantic 3D
reconstruction at LoD3 since not only the high-definition reconstruction but
also reconstruction confidence becomes pivotal for various applications such as
autonomous driving and urban simulations.
</p></li>
</ul>
<h3>Title: Korean Named Entity Recognition Based on Language-Specific Features. (arXiv:2305.06330v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06330">http://arxiv.org/abs/2305.06330</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06330] Korean Named Entity Recognition Based on Language-Specific Features](http://arxiv.org/abs/2305.06330) #segmentation</code></li>
<li>Summary: <p>In the paper, we propose a novel way of improving named entity recognition in
the Korean language using its language-specific features. While the field of
named entity recognition has been studied extensively in recent years, the
mechanism of efficiently recognizing named entities in Korean has hardly been
explored. This is because the Korean language has distinct linguistic
properties that prevent models from achieving their best performances.
Therefore, an annotation scheme for {Korean corpora} by adopting the CoNLL-U
format, which decomposes Korean words into morphemes and reduces the ambiguity
of named entities in the original segmentation that may contain functional
morphemes such as postpositions and particles, is proposed herein. We
investigate how the named entity tags are best represented in this
morpheme-based scheme and implement an algorithm to convert word-based {and
syllable-based Korean corpora} with named entities into the proposed
morpheme-based format. Analyses of the results of {statistical and neural}
models reveal that the proposed morpheme-based format is feasible, and the
{varied} performances of the models under the influence of various additional
language-specific features are demonstrated. Extrinsic conditions were also
considered to observe the variance of the performances of the proposed models,
given different types of data, including the original segmentation and
different types of tagging formats.
</p></li>
</ul>
<h2>object detection</h2>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-05-11]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
