<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: DBAT: Dynamic Backward Attention Transformer for Material Segmentation with Cross-Resolution Patches. (arXiv:2305.03919v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03919">http://arxiv.org/abs/2305.03919</a></li>
<li>Code URL: <a href="https://github.com/heng-yuwen/dynamic-backward-attention-transformer">https://github.com/heng-yuwen/dynamic-backward-attention-transformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03919] DBAT: Dynamic Backward Attention Transformer for Material Segmentation with Cross-Resolution Patches](http://arxiv.org/abs/2305.03919) #transformer</code></li>
<li>Summary: <p>The objective of dense material segmentation is to identify the material
categories for every image pixel. Recent studies adopt image patches to extract
material features. Although the trained networks can improve the segmentation
performance, their methods choose a fixed patch resolution which fails to take
into account the variation in pixel area covered by each material. In this
paper, we propose the Dynamic Backward Attention Transformer (DBAT) to
aggregate cross-resolution features. The DBAT takes cropped image patches as
input and gradually increases the patch resolution by merging adjacent patches
at each transformer stage, instead of fixing the patch resolution during
training. We explicitly gather the intermediate features extracted from
cross-resolution patches and merge them dynamically with predicted attention
masks. Experiments show that our DBAT achieves an accuracy of 86.85%, which is
the best performance among state-of-the-art real-time models. Like other
successful deep learning solutions with complex architectures, the DBAT also
suffers from lack of interpretability. To address this problem, this paper
examines the properties that the DBAT makes use of. By analysing the
cross-resolution features and the attention weights, this paper interprets how
the DBAT learns from image patches. We further align features to semantic
labels, performing network dissection, to infer that the proposed model can
extract material-related features better than other methods. We show that the
DBAT model is more robust to network initialisation, and yields fewer variable
predictions compared to other models. The project code is available at
https://github.com/heng-yuwen/Dynamic-Backward-Attention-Transformer.
</p></li>
</ul>
<h3>Title: Transformer-Based Hierarchical Clustering for Brain Network Analysis. (arXiv:2305.04142v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04142">http://arxiv.org/abs/2305.04142</a></li>
<li>Code URL: <a href="https://github.com/ddvd233/thc">https://github.com/ddvd233/thc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04142] Transformer-Based Hierarchical Clustering for Brain Network Analysis](http://arxiv.org/abs/2305.04142) #transformer</code></li>
<li>Summary: <p>Brain networks, graphical models such as those constructed from MRI, have
been widely used in pathological prediction and analysis of brain functions.
Within the complex brain system, differences in neuronal connection strengths
parcellate the brain into various functional modules (network communities),
which are critical for brain analysis. However, identifying such communities
within the brain has been a nontrivial issue due to the complexity of neuronal
interactions. In this work, we propose a novel interpretable transformer-based
model for joint hierarchical cluster identification and brain network
classification. Extensive experimental results on real-world brain network
datasets show that with the help of hierarchical clustering, the model achieves
increased accuracy and reduced runtime complexity while providing plausible
insight into the functional organization of brain regions. The implementation
is available at https://github.com/DDVD233/THC.
</p></li>
</ul>
<h3>Title: UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in Vietnamese. (arXiv:2305.04166v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04166">http://arxiv.org/abs/2305.04166</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04166] UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in Vietnamese](http://arxiv.org/abs/2305.04166) #transformer</code></li>
<li>Summary: <p>Image Captioning is one of the vision-language tasks that still interest the
research community worldwide in the 2020s. MS-COCO Caption benchmark is
commonly used to evaluate the performance of advanced captioning models,
although it was published in 2015. Recent captioning models trained on the
MS-COCO Caption dataset only have good performance in language patterns of
English; they do not have such good performance in contexts captured in Vietnam
or fluently caption images using Vietnamese. To contribute to the low-resources
research community as in Vietnam, we introduce a novel image captioning dataset
in Vietnamese, the Open-domain Vietnamese Image Captioning dataset
(UIT-OpenViIC). The introduced dataset includes complex scenes captured in
Vietnam and manually annotated by Vietnamese under strict rules and
supervision. In this paper, we present in more detail the dataset creation
process. From preliminary analysis, we show that our dataset is challenging to
recent state-of-the-art (SOTA) Transformer-based baselines, which performed
well on the MS COCO dataset. Then, the modest results prove that UIT-OpenViIC
has room to grow, which can be one of the standard benchmarks in Vietnamese for
the research community to evaluate their captioning models. Furthermore, we
present a CAMO approach that effectively enhances the image representation
ability by a multi-level encoder output fusion mechanism, which helps improve
the quality of generated captions compared to previous captioning models.
</p></li>
</ul>
<h3>Title: Cross-Modal Retrieval for Motion and Text via MildTriple Loss. (arXiv:2305.04195v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04195">http://arxiv.org/abs/2305.04195</a></li>
<li>Code URL: <a href="https://github.com/eanson023/rehamot">https://github.com/eanson023/rehamot</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04195] Cross-Modal Retrieval for Motion and Text via MildTriple Loss](http://arxiv.org/abs/2305.04195) #transformer</code></li>
<li>Summary: <p>Cross-modal retrieval has become a prominent research topic in computer
vision and natural language processing with advances made in image-text and
video-text retrieval technologies. However, cross-modal retrieval between human
motion sequences and text has not garnered sufficient attention despite the
extensive application value it holds, such as aiding virtual reality
applications in better understanding users' actions and language. This task
presents several challenges, including joint modeling of the two modalities,
demanding the understanding of person-centered information from text, and
learning behavior features from 3D human motion sequences. Previous work on
motion data modeling mainly relied on autoregressive feature extractors that
may forget previous information, while we propose an innovative model that
includes simple yet powerful transformer-based motion and text encoders, which
can learn representations from the two different modalities and capture
long-term dependencies. Furthermore, the overlap of the same atomic actions of
different human motions can cause semantic conflicts, leading us to explore a
new triplet loss function, MildTriple Loss. it leverages the similarity between
samples in intra-modal space to guide soft-hard negative sample mining in the
joint embedding space to train the triplet loss and reduce the violation caused
by false negative samples. We evaluated our model and method on the latest
HumanML3D and KIT Motion-Language datasets, achieving a 62.9\% recall for
motion retrieval and a 71.5\% recall for text retrieval (based on R@10) on the
HumanML3D dataset. Our code is available at
https://github.com/eanson023/rehamot.
</p></li>
</ul>
<h3>Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation. (arXiv:2305.03796v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03796">http://arxiv.org/abs/2305.03796</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03796] Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation](http://arxiv.org/abs/2305.03796) #transformer</code></li>
<li>Summary: <p>Unlike recurrent models, conventional wisdom has it that Transformers cannot
perfectly model regular languages. Inspired by the notion of working memory, we
propose a new Transformer variant named RegularGPT. With its novel combination
of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT
constructs working memory along the depth dimension, thereby enabling efficient
and successful modeling of regular languages such as PARITY. We further test
RegularGPT on the task of natural language length extrapolation and
surprisingly find that it rediscovers the local windowed attention effect
deemed necessary in prior work for length extrapolation.
</p></li>
</ul>
<h3>Title: Adapting Transformer Language Models for Predictive Typing in Brain-Computer Interfaces. (arXiv:2305.03819v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03819">http://arxiv.org/abs/2305.03819</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03819] Adapting Transformer Language Models for Predictive Typing in Brain-Computer Interfaces](http://arxiv.org/abs/2305.03819) #transformer</code></li>
<li>Summary: <p>Brain-computer interfaces (BCI) are an important mode of alternative and
augmentative communication for many people. Unlike keyboards, many BCI systems
do not display even the 26 letters of English at one time, let alone all the
symbols in more complex systems. Using language models to make character-level
predictions, therefore, can greatly speed up BCI typing (Ghosh and Kristensson,
2017). While most existing BCI systems employ character n-gram models or no LM
at all, this paper adapts several wordpiece-level Transformer LMs to make
character predictions and evaluates them on typing tasks. GPT-2 fares best on
clean text, but different LMs react differently to noisy histories. We further
analyze the effect of character positions in a word and context lengths.
</p></li>
</ul>
<h3>Title: Large Language Models in Sport Science &amp; Medicine: Opportunities, Risks and Considerations. (arXiv:2305.03851v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03851">http://arxiv.org/abs/2305.03851</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03851] Large Language Models in Sport Science &amp; Medicine: Opportunities, Risks and Considerations](http://arxiv.org/abs/2305.03851) #transformer</code></li>
<li>Summary: <p>This paper explores the potential opportunities, risks, and challenges
associated with the use of large language models (LLMs) in sports science and
medicine. LLMs are large neural networks with transformer style architectures
trained on vast amounts of textual data, and typically refined with human
feedback. LLMs can perform a large range of natural language processing tasks.
In sports science and medicine, LLMs have the potential to support and augment
the knowledge of sports medicine practitioners, make recommendations for
personalised training programs, and potentially distribute high-quality
information to practitioners in developing countries. However, there are also
potential risks associated with the use and development of LLMs, including
biases in the dataset used to create the model, the risk of exposing
confidential data, the risk of generating harmful output, and the need to align
these models with human preferences through feedback. Further research is
needed to fully understand the potential applications of LLMs in sports science
and medicine and to ensure that their use is ethical and beneficial to
athletes, clients, patients, practitioners, and the general public.
</p></li>
</ul>
<h3>Title: An Adversarial Non-Autoregressive Model for Text Generation with Incomplete Information. (arXiv:2305.03977v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03977">http://arxiv.org/abs/2305.03977</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03977] An Adversarial Non-Autoregressive Model for Text Generation with Incomplete Information](http://arxiv.org/abs/2305.03977) #transformer</code></li>
<li>Summary: <p>Non-autoregressive models have been widely studied in the Complete
Information Scenario (CIS), in which the models have complete input information
to obtain corresponding output. However, their explorations in the Incomplete
Information Scenario (IIS) are extremely limited. Our analyses reveal that the
IIS's incomplete input information will augment the inherent limitations of
existing non-autoregressive models trained under Maximum Likelihood Estimation.
In this paper, we propose for the IIS an Adversarial Non-autoregressive
Transformer (ANT) which has two novel features: 1) Position Aware
Self-Modulation to provide more reasonable hidden representations, and 2)
Dependency Feed Forward Network to strengthen its capacity in dependency
modeling. We compare ANT with other mainstream models in the IIS and
demonstrate that ANT can achieve comparable performance with much fewer
decoding iterations. Furthermore, we show its great potential in various
applications like latent interpolation and semi-supervised learning.
</p></li>
</ul>
<h3>Title: Rhetorical Role Labeling of Legal Documents using Transformers and Graph Neural Networks. (arXiv:2305.04100v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04100">http://arxiv.org/abs/2305.04100</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04100] Rhetorical Role Labeling of Legal Documents using Transformers and Graph Neural Networks](http://arxiv.org/abs/2305.04100) #transformer</code></li>
<li>Summary: <p>A legal document is usually long and dense requiring human effort to parse
it. It also contains significant amounts of jargon which make deriving insights
from it using existing models a poor approach. This paper presents the
approaches undertaken to perform the task of rhetorical role labelling on
Indian Court Judgements as part of SemEval Task 6: understanding legal texts,
shared subtask A. We experiment with graph based approaches like Graph
Convolutional Networks and Label Propagation Algorithm, and transformer-based
approaches including variants of BERT to improve accuracy scores on text
classification of complex legal documents.
</p></li>
</ul>
<h3>Title: MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents. (arXiv:2305.04177v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04177">http://arxiv.org/abs/2305.04177</a></li>
<li>Code URL: <a href="https://github.com/arazd/miread">https://github.com/arazd/miread</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04177] MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents](http://arxiv.org/abs/2305.04177) #transformer</code></li>
<li>Summary: <p>Learning semantically meaningful representations from scientific documents
can facilitate academic literature search and improve performance of
recommendation systems. Pre-trained language models have been shown to learn
rich textual representations, yet they cannot provide powerful document-level
representations for scientific articles. We propose MIReAD, a simple method
that learns high-quality representations of scientific papers by fine-tuning
transformer model to predict the target journal class based on the abstract. We
train MIReAD on more than 500,000 PubMed and arXiv abstracts across over 2,000
journal classes. We show that MIReAD produces representations that can be used
for similar papers retrieval, topic categorization and literature search. Our
proposed approach outperforms six existing models for representation learning
on scientific documents across four evaluation standards.
</p></li>
</ul>
<h3>Title: OpenViVQA: Task, Dataset, and Multimodal Fusion Models for Visual Question Answering in Vietnamese. (arXiv:2305.04183v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04183">http://arxiv.org/abs/2305.04183</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04183] OpenViVQA: Task, Dataset, and Multimodal Fusion Models for Visual Question Answering in Vietnamese](http://arxiv.org/abs/2305.04183) #transformer</code></li>
<li>Summary: <p>In recent years, visual question answering (VQA) has attracted attention from
the research community because of its highly potential applications (such as
virtual assistance on intelligent cars, assistant devices for blind people, or
information retrieval from document images using natural language as queries)
and challenge. The VQA task requires methods that have the ability to fuse the
information from questions and images to produce appropriate answers. Neural
visual question answering models have achieved tremendous growth on large-scale
datasets which are mostly for resource-rich languages such as English. However,
available datasets narrow the VQA task as the answers selection task or answer
classification task. We argue that this form of VQA is far from human ability
and eliminates the challenge of the answering aspect in the VQA task by just
selecting answers rather than generating them. In this paper, we introduce the
OpenViVQA (Open-domain Vietnamese Visual Question Answering) dataset, the first
large-scale dataset for VQA with open-ended answers in Vietnamese, consists of
11,000+ images associated with 37,000+ question-answer pairs (QAs). Moreover,
we proposed FST, QuMLAG, and MLPAG which fuse information from images and
answers, then use these fused features to construct answers as humans
iteratively. Our proposed methods achieve results that are competitive with
SOTA models such as SAAA, MCAN, LORA, and M4C. The dataset is available to
encourage the research community to develop more generalized algorithms
including transformers for low-resource languages such as Vietnamese.
</p></li>
</ul>
<h3>Title: Spatiotemporal Transformer for Stock Movement Prediction. (arXiv:2305.03835v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03835">http://arxiv.org/abs/2305.03835</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03835] Spatiotemporal Transformer for Stock Movement Prediction](http://arxiv.org/abs/2305.03835) #transformer</code></li>
<li>Summary: <p>Financial markets are an intriguing place that offer investors the potential
to gain large profits if timed correctly. Unfortunately, the dynamic,
non-linear nature of financial markets makes it extremely hard to predict
future price movements. Within the US stock exchange, there are a countless
number of factors that play a role in the price of a company's stock, including
but not limited to financial statements, social and news sentiment, overall
market sentiment, political happenings and trading psychology. Correlating
these factors is virtually impossible for a human. Therefore, we propose STST,
a novel approach using a Spatiotemporal Transformer-LSTM model for stock
movement prediction. Our model obtains accuracies of 63.707 and 56.879 percent
against the ACL18 and KDD17 datasets, respectively. In addition, our model was
used in simulation to determine its real-life applicability. It obtained a
minimum of 10.41% higher profit than the S&amp;P500 stock index, with a minimum
annualized return of 31.24%.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: Evading Watermark based Detection of AI-Generated Content. (arXiv:2305.03807v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03807">http://arxiv.org/abs/2305.03807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03807] Evading Watermark based Detection of AI-Generated Content](http://arxiv.org/abs/2305.03807) #generative</code></li>
<li>Summary: <p>A generative AI model -- such as DALL-E, Stable Diffusion, and ChatGPT -- can
generate extremely realistic-looking content, posing growing challenges to the
authenticity of information. To address the challenges, watermark has been
leveraged to detect AI-generated content. Specifically, a watermark is embedded
into an AI-generated content before it is released. A content is detected as
AI-generated if a similar watermark can be decoded from it. In this work, we
perform a systematic study on the robustness of such watermark-based
AI-generated content detection. We focus on AI-generated images. Our work shows
that an attacker can post-process an AI-generated watermarked image via adding
a small, human-imperceptible perturbation to it, such that the post-processed
AI-generated image evades detection while maintaining its visual quality. We
demonstrate the effectiveness of our attack both theoretically and empirically.
Moreover, to evade detection, our adversarial post-processing method adds much
smaller perturbations to the AI-generated images and thus better maintain their
visual quality than existing popular image post-processing methods such as JPEG
compression, Gaussian blur, and Brightness/Contrast. Our work demonstrates the
insufficiency of existing watermark-based detection of AI-generated content,
highlighting the urgent needs of new detection methods.
</p></li>
</ul>
<h3>Title: Multi-object Video Generation from Single Frame Layouts. (arXiv:2305.03983v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03983">http://arxiv.org/abs/2305.03983</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03983] Multi-object Video Generation from Single Frame Layouts](http://arxiv.org/abs/2305.03983) #generative</code></li>
<li>Summary: <p>In this paper, we study video synthesis with emphasis on simplifying the
generation conditions. Most existing video synthesis models or datasets are
designed to address complex motions of a single object, lacking the ability of
comprehensively understanding the spatio-temporal relationships among multiple
objects. Besides, current methods are usually conditioned on intricate
annotations (e.g. video segmentations) to generate new videos, being
fundamentally less practical. These motivate us to generate multi-object videos
conditioning exclusively on object layouts from a single frame. To solve above
challenges and inspired by recent research on image generation from layouts, we
have proposed a novel video generative framework capable of synthesizing global
scenes with local objects, via implicit neural representations and layout
motion self-inference. Our framework is a non-trivial adaptation from image
generation methods, and is new to this field. In addition, our model has been
evaluated on two widely-used video recognition benchmarks, demonstrating
effectiveness compared to the baseline model.
</p></li>
</ul>
<h3>Title: LEO: Generative Latent Image Animator for Human Video Synthesis. (arXiv:2305.03989v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03989">http://arxiv.org/abs/2305.03989</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03989] LEO: Generative Latent Image Animator for Human Video Synthesis](http://arxiv.org/abs/2305.03989) #generative</code></li>
<li>Summary: <p>Spatio-temporal coherency is a major challenge in synthesizing high quality
videos, particularly in synthesizing human videos that contain rich global and
local deformations. To resolve this challenge, previous approaches have
resorted to different features in the generation process aimed at representing
appearance and motion. However, in the absence of strict mechanisms to
guarantee such disentanglement, a separation of motion from appearance has
remained challenging, resulting in spatial distortions and temporal jittering
that break the spatio-temporal coherency. Motivated by this, we here propose
LEO, a novel framework for human video synthesis, placing emphasis on
spatio-temporal coherency. Our key idea is to represent motion as a sequence of
flow maps in the generation process, which inherently isolate motion from
appearance. We implement this idea via a flow-based image animator and a Latent
Motion Diffusion Model (LMDM). The former bridges a space of motion codes with
the space of flow maps, and synthesizes video frames in a warp-and-inpaint
manner. LMDM learns to capture motion prior in the training data by
synthesizing sequences of motion codes. Extensive quantitative and qualitative
analysis suggests that LEO significantly improves coherent synthesis of human
videos over previous methods on the datasets TaichiHD, FaceForensics and
CelebV-HQ. In addition, the effective disentanglement of appearance and motion
in LEO allows for two additional tasks, namely infinite-length human video
synthesis, as well as content-preserving video editing.
</p></li>
</ul>
<h3>Title: A Sea-Land Clutter Classification Framework for Over-the-Horizon-Radar Based on Weighted Loss Semi-supervised GAN. (arXiv:2305.04021v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04021">http://arxiv.org/abs/2305.04021</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04021] A Sea-Land Clutter Classification Framework for Over-the-Horizon-Radar Based on Weighted Loss Semi-supervised GAN](http://arxiv.org/abs/2305.04021) #generative</code></li>
<li>Summary: <p>Deep convolutional neural network has made great achievements in sea-land
clutter classification for over-the-horizon-radar (OTHR). The premise is that a
large number of labeled training samples must be provided for a sea-land
clutter classifier. In practical engineering applications, it is relatively
easy to obtain label-free sea-land clutter samples. However, the labeling
process is extremely cumbersome and requires expertise in the field of OTHR. To
solve this problem, we propose an improved generative adversarial network,
namely weighted loss semi-supervised generative adversarial network (WL-SSGAN).
Specifically, we propose a joint feature matching loss by weighting the middle
layer features of the discriminator of semi-supervised generative adversarial
network. Furthermore, we propose the weighted loss of WL-SSGAN by linearly
weighting standard adversarial loss and joint feature matching loss. The
semi-supervised classification performance of WL-SSGAN is evaluated on a
sea-land clutter dataset. The experimental results show that WL-SSGAN can
improve the performance of the fully supervised classifier with only a small
number of labeled samples by utilizing a large number of unlabeled sea-land
clutter samples. Further, the proposed weighted loss is superior to both the
adversarial loss and the feature matching loss. Additionally, we compare
WL-SSGAN with conventional semi-supervised classification methods and
demonstrate that WL-SSGAN achieves the highest classification accuracy.
</p></li>
</ul>
<h3>Title: Learning Stochastic Dynamical System via Flow Map Operator. (arXiv:2305.03874v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03874">http://arxiv.org/abs/2305.03874</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03874] Learning Stochastic Dynamical System via Flow Map Operator](http://arxiv.org/abs/2305.03874) #generative</code></li>
<li>Summary: <p>We present a numerical framework for learning unknown stochastic dynamical
systems using measurement data. Termed stochastic flow map learning (sFML), the
new framework is an extension of flow map learning (FML) that was developed for
learning deterministic dynamical systems. For learning stochastic systems, we
define a stochastic flow map that is a superposition of two sub-flow maps: a
deterministic sub-map and a stochastic sub-map. The stochastic training data
are used to construct the deterministic sub-map first, followed by the
stochastic sub-map. The deterministic sub-map takes the form of residual
network (ResNet), similar to the work of FML for deterministic systems. For the
stochastic sub-map, we employ a generative model, particularly generative
adversarial networks (GANs) in this paper. The final constructed stochastic
flow map then defines a stochastic evolution model that is a weak
approximation, in term of distribution, of the unknown stochastic system. A
comprehensive set of numerical examples are presented to demonstrate the
flexibility and effectiveness of the proposed sFML method for various types of
stochastic systems.
</p></li>
</ul>
<h2>label correction</h2>
<h2>noise</h2>
<h3>Title: Weighted Point Cloud Normal Estimation. (arXiv:2305.04007v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04007">http://arxiv.org/abs/2305.04007</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04007] Weighted Point Cloud Normal Estimation](http://arxiv.org/abs/2305.04007) #noise</code></li>
<li>Summary: <p>Existing normal estimation methods for point clouds are often less robust to
severe noise and complex geometric structures. Also, they usually ignore the
contributions of different neighbouring points during normal estimation, which
leads to less accurate results. In this paper, we introduce a weighted normal
estimation method for 3D point cloud data. We innovate in two key points: 1) we
develop a novel weighted normal regression technique that predicts point-wise
weights from local point patches and use them for robust, feature-preserving
normal regression; 2) we propose to conduct contrastive learning between point
patches and the corresponding ground-truth normals of the patches' central
points as a pre-training process to facilitate normal regression. Comprehensive
experiments demonstrate that our method can robustly handle noisy and complex
point clouds, achieving state-of-the-art performance on both synthetic and
real-world datasets.
</p></li>
</ul>
<h3>Title: Unlocking the Power of Open Set : A New Perspective for Open-set Noisy Label Learning. (arXiv:2305.04203v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04203">http://arxiv.org/abs/2305.04203</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04203] Unlocking the Power of Open Set : A New Perspective for Open-set Noisy Label Learning](http://arxiv.org/abs/2305.04203) #noise</code></li>
<li>Summary: <p>Learning from noisy data has attracted much attention, where most methods
focus on closed-set label noise. However, a more common scenario in the real
world is the presence of both open-set and closed-set noise. Existing methods
typically identify and handle these two types of label noise separately by
designing a specific strategy for each type. However, in many real-world
scenarios, it would be challenging to identify open-set examples, especially
when the dataset has been severely corrupted. Unlike the previous works, we
explore how models behave when faced open-set examples, and find that a part of
open-set examples gradually get integrated into certain known classes, which is
beneficial for the seperation among known classes. Motivated by the phenomenon,
in this paper, we propose a novel two-step contrastive learning method called
CECL, which aims to deal with both types of label noise by exploiting the
useful information of open-set examples. Specifically, we incorporate some
open-set examples into closed-set classes to enhance performance while treating
others as delimiters to improve representative ability. Extensive experiments
on synthetic and real-world datasets with diverse label noise demonstrate that
CECL can outperform state-of-the-art methods.
</p></li>
</ul>
<h3>Title: SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition. (arXiv:2305.04076v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04076">http://arxiv.org/abs/2305.04076</a></li>
<li>Code URL: <a href="https://github.com/pkunlp-icler/santa">https://github.com/pkunlp-icler/santa</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04076] SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition](http://arxiv.org/abs/2305.04076) #noise</code></li>
<li>Summary: <p>Distantly-Supervised Named Entity Recognition effectively alleviates the
burden of time-consuming and expensive annotation in the supervised setting.
But the context-free matching process and the limited coverage of knowledge
bases introduce inaccurate and incomplete annotation noise respectively.
Previous studies either considered only incomplete annotation noise or
indiscriminately handle two types of noise with the same strategy. In this
paper, we argue that the different causes of two types of noise bring up the
requirement of different strategies in model architecture. Therefore, we
propose the SANTA to handle these two types of noise separately with (1)
Memory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity
problem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate
decision boundary shifting problem caused by incomplete annotation and a
noise-tolerant loss to improve the robustness. Benefiting from our separate
tailored strategies, we confirm in the experiment that the two types of noise
are well mitigated. SANTA also achieves a new state-of-the-art on five public
datasets.
</p></li>
</ul>
<h3>Title: Automated Spatio-Temporal Graph Contrastive Learning. (arXiv:2305.03920v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03920">http://arxiv.org/abs/2305.03920</a></li>
<li>Code URL: <a href="https://github.com/hkuds/autost">https://github.com/hkuds/autost</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03920] Automated Spatio-Temporal Graph Contrastive Learning](http://arxiv.org/abs/2305.03920) #noise</code></li>
<li>Summary: <p>Among various region embedding methods, graph-based region relation learning
models stand out, owing to their strong structure representation ability for
encoding spatial correlations with graph neural networks. Despite their
effectiveness, several key challenges have not been well addressed in existing
methods: i) Data noise and missing are ubiquitous in many spatio-temporal
scenarios due to a variety of factors. ii) Input spatio-temporal data (e.g.,
mobility traces) usually exhibits distribution heterogeneity across space and
time. In such cases, current methods are vulnerable to the quality of the
generated region graphs, which may lead to suboptimal performance. In this
paper, we tackle the above challenges by exploring the Automated
Spatio-Temporal graph contrastive learning paradigm (AutoST) over the
heterogeneous region graph generated from multi-view data sources. Our \model\
framework is built upon a heterogeneous graph neural architecture to capture
the multi-view region dependencies with respect to POI semantics, mobility flow
patterns and geographical positions. To improve the robustness of our GNN
encoder against data noise and distribution issues, we design an automated
spatio-temporal augmentation scheme with a parameterized contrastive view
generator. AutoST can adapt to the spatio-temporal heterogeneous graph with
multi-view semantics well preserved. Extensive experiments for three downstream
spatio-temporal mining tasks on several real-world datasets demonstrate the
significant performance gain achieved by our \model\ over a variety of
baselines. The code is publicly available at https://github.com/HKUDS/AutoST.
</p></li>
</ul>
<h3>Title: Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics. (arXiv:2305.04152v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04152">http://arxiv.org/abs/2305.04152</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04152] Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics](http://arxiv.org/abs/2305.04152) #noise</code></li>
<li>Summary: <p>The recent development of scalable Bayesian inference methods has renewed
interest in the adoption of Bayesian learning as an alternative to conventional
frequentist learning that offers improved model calibration via uncertainty
quantification. Recently, federated averaging Langevin dynamics (FALD) was
introduced as a variant of federated averaging that can efficiently implement
distributed Bayesian learning in the presence of noiseless communications. In
this paper, we propose wireless FALD (WFALD), a novel protocol that realizes
FALD in wireless systems by integrating over-the-air computation and
channel-driven sampling for Monte Carlo updates. Unlike prior work on wireless
Bayesian learning, WFALD enables (\emph{i}) multiple local updates between
communication rounds; and (\emph{ii}) stochastic gradients computed by
mini-batch. A convergence analysis is presented in terms of the 2-Wasserstein
distance between the samples produced by WFALD and the targeted global
posterior distribution. Analysis and experiments show that, when the
signal-to-noise ratio is sufficiently large, channel noise can be fully
repurposed for Monte Carlo sampling, thus entailing no loss in performance.
</p></li>
</ul>
<h2>diffusion</h2>
<h3>Title: DocDiff: Document Enhancement via Residual Diffusion Models. (arXiv:2305.03892v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03892">http://arxiv.org/abs/2305.03892</a></li>
<li>Code URL: <a href="https://github.com/Royalvice/DocDiff">https://github.com/Royalvice/DocDiff</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03892] DocDiff: Document Enhancement via Residual Diffusion Models](http://arxiv.org/abs/2305.03892) #diffusion</code></li>
<li>Summary: <p>Removing degradation from document images not only improves their visual
quality and readability, but also enhances the performance of numerous
automated document analysis and recognition tasks. However, existing
regression-based methods optimized for pixel-level distortion reduction tend to
suffer from significant loss of high-frequency information, leading to
distorted and blurred text edges. To compensate for this major deficiency, we
propose DocDiff, the first diffusion-based framework specifically designed for
diverse challenging document enhancement problems, including document
deblurring, denoising, and removal of watermarks and seals. DocDiff consists of
two modules: the Coarse Predictor (CP), which is responsible for recovering the
primary low-frequency content, and the High-Frequency Residual Refinement (HRR)
module, which adopts the diffusion models to predict the residual
(high-frequency information, including text edges), between the ground-truth
and the CP-predicted image. DocDiff is a compact and computationally efficient
model that benefits from a well-designed network architecture, an optimized
training loss objective, and a deterministic sampling process with short time
steps. Extensive experiments demonstrate that DocDiff achieves state-of-the-art
(SOTA) performance on multiple benchmark datasets, and can significantly
enhance the readability and recognizability of degraded document images.
Furthermore, our proposed HRR module in pre-trained DocDiff is plug-and-play
and ready-to-use, with only 4.17M parameters. It greatly sharpens the text
edges generated by SOTA deblurring methods without additional joint training.
Available codes: https://github.com/Royalvice/DocDiff
</p></li>
</ul>
<h3>Title: Towards Prompt-robust Face Privacy Protection via Adversarial Decoupling Augmentation Framework. (arXiv:2305.03980v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03980">http://arxiv.org/abs/2305.03980</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03980] Towards Prompt-robust Face Privacy Protection via Adversarial Decoupling Augmentation Framework](http://arxiv.org/abs/2305.03980) #diffusion</code></li>
<li>Summary: <p>Denoising diffusion models have shown remarkable potential in various
generation tasks. The open-source large-scale text-to-image model, Stable
Diffusion, becomes prevalent as it can generate realistic artistic or facial
images with personalization through fine-tuning on a limited number of new
samples. However, this has raised privacy concerns as adversaries can acquire
facial images online and fine-tune text-to-image models for malicious editing,
leading to baseless scandals, defamation, and disruption to victims' lives.
Prior research efforts have focused on deriving adversarial loss from
conventional training processes for facial privacy protection through
adversarial perturbations. However, existing algorithms face two issues: 1)
they neglect the image-text fusion module, which is the vital module of
text-to-image diffusion models, and 2) their defensive performance is unstable
against different attacker prompts. In this paper, we propose the Adversarial
Decoupling Augmentation Framework (ADAF), addressing these issues by targeting
the image-text fusion module to enhance the defensive performance of facial
privacy protection algorithms. ADAF introduces multi-level text-related
augmentations for defense stability against various attacker prompts.
Concretely, considering the vision, text, and common unit space, we propose
Vision-Adversarial Loss, Prompt-Robust Augmentation, and Attention-Decoupling
Loss. Extensive experiments on CelebA-HQ and VGGFace2 demonstrate ADAF's
promising performance, surpassing existing algorithms.
</p></li>
</ul>
<h3>Title: AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion. (arXiv:2305.04001v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04001">http://arxiv.org/abs/2305.04001</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04001] AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion](http://arxiv.org/abs/2305.04001) #diffusion</code></li>
<li>Summary: <p>Recent advances in diffusion models have showcased promising results in the
text-to-video (T2V) synthesis task. However, as these T2V models solely employ
text as the guidance, they tend to struggle in modeling detailed temporal
dynamics. In this paper, we introduce a novel T2V framework that additionally
employ audio signals to control the temporal dynamics, empowering an
off-the-shelf T2I diffusion to generate audio-aligned videos. We propose
audio-based regional editing and signal smoothing to strike a good balance
between the two contradicting desiderata of video synthesis, i.e., temporal
flexibility and coherence. We empirically demonstrate the effectiveness of our
method through experiments, and further present practical applications for
contents creation.
</p></li>
</ul>
<h3>Title: Exploring One-shot Semi-supervised Federated Learning with A Pre-trained Diffusion Model. (arXiv:2305.04063v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04063">http://arxiv.org/abs/2305.04063</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04063] Exploring One-shot Semi-supervised Federated Learning with A Pre-trained Diffusion Model](http://arxiv.org/abs/2305.04063) #diffusion</code></li>
<li>Summary: <p>Federated learning is a privacy-preserving collaborative learning approach.
Recently, some studies have proposed the semi-supervised federated learning
setting to handle the commonly seen real-world scenarios with labeled data on
the server and unlabeled data on the clients. However, existing methods still
face challenges such as high communication costs, training pressure on the
client devices, and distribution differences among the server and the clients.
In this paper, we introduce the powerful pre-trained diffusion models into
federated learning and propose FedDISC, a Federated Diffusion Inspired
Semi-supervised Co-training method, to address these challenges. Specifically,
we first extract prototypes from the labeled data on the server and send them
to the clients. The clients then use these prototypes to predict pseudo-labels
of the local data, and compute the cluster centroids and domain-specific
features to represent their personalized distributions. After adding noise, the
clients send these features and their corresponding pseudo-labels back to the
server, which uses a pre-trained diffusion model to conditionally generate
pseudo-samples complying with the client distributions and train an aggregated
model on them. Our method does not require local training and only involves
forward inference on the clients. Our extensive experiments on DomainNet,
Openimage, and NICO++ demonstrate that the proposed FedDISC method effectively
addresses the one-shot semi-supervised problem on Non-IID clients and
outperforms the compared SOTA methods. We also demonstrate through
visualization that it is of neglectable possibility for FedDISC to leak
privacy-sensitive information of the clients.
</p></li>
</ul>
<h3>Title: Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning. (arXiv:2305.04175v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04175">http://arxiv.org/abs/2305.04175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04175] Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning](http://arxiv.org/abs/2305.04175) #diffusion</code></li>
<li>Summary: <p>With the help of conditioning mechanisms, the state-of-the-art diffusion
models have achieved tremendous success in guided image generation,
particularly in text-to-image synthesis. To gain a better understanding of the
training process and potential risks of text-to-image synthesis, we perform a
systematic investigation of backdoor attack on text-to-image diffusion models
and propose BadT2I, a general multimodal backdoor attack framework that tampers
with image synthesis in diverse semantic levels. Specifically, we perform
backdoor attacks on three levels of the vision semantics: Pixel-Backdoor,
Object-Backdoor and Style-Backdoor. By utilizing a regularization loss, our
methods efficiently inject backdoors into a large-scale text-to-image diffusion
model while preserving its utility with benign inputs. We conduct empirical
experiments on Stable Diffusion, the widely-used text-to-image diffusion model,
demonstrating that the large-scale diffusion model can be easily backdoored
within a few fine-tuning steps. We conduct additional experiments to explore
the impact of different types of textual triggers. Besides, we discuss the
backdoor persistence during further training, the findings of which provide
insights for the development of backdoor defense methods.
</p></li>
</ul>
<h3>Title: Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation. (arXiv:2305.04044v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04044">http://arxiv.org/abs/2305.04044</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04044] Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation](http://arxiv.org/abs/2305.04044) #diffusion</code></li>
<li>Summary: <p>Recently, continuous diffusion models (CDM) have been introduced into
non-autoregressive (NAR) text-to-text generation. However, the discrete nature
of text increases the difficulty of CDM to generate coherent and fluent texts,
and also causes the incompatibility problem between CDM and advanced NLP
techniques, especially the popular pre-trained language models~(PLMs). To solve
it, we propose Diffusion-NAT, which introduces discrete diffusion models~(DDM)
into NAR text-to-text generation and integrates BART to improve the
performance. By revising the decoding process of BART and the typical settings
of DDM, we unify the inference process of BART and the denoising process of DDM
into the same NAR masked tokens recovering task. In this way, DDM can rely on
BART to perform denoising, which can benefit from both the rich pre-learned
knowledge of BART and the iterative refining paradigm of DDM. Besides, we also
propose the iterative self-prompting strategy to further improve the generation
quality. Experimental results on 7 datasets show that our approach can
outperform competitive NAR methods, and even surpass autoregressive methods.
Our code and data will be publicly released.
</p></li>
</ul>
<h3>Title: Physics-Informed Localized Learning for Advection-Diffusion-Reaction Systems. (arXiv:2305.03774v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03774">http://arxiv.org/abs/2305.03774</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03774] Physics-Informed Localized Learning for Advection-Diffusion-Reaction Systems](http://arxiv.org/abs/2305.03774) #diffusion</code></li>
<li>Summary: <p>The global push for new energy solutions, such as Geothermal, and Carbon
Capture and Sequestration initiatives has thrust new demands upon the current
state-of the-art subsurface fluid simulators. The requirement to be able to
simulate a large order of reservoir states simultaneously in a short period of
time has opened the door of opportunity for the application of machine learning
techniques for surrogate modelling. We propose a novel physics-informed and
boundary conditions-aware Localized Learning method which extends the
Embed-to-Control (E2C) and Embed-to-Control and Observed (E2CO) models to learn
local representations of global state variables in an Advection-Diffusion
Reaction system. We show that our model trained on reservoir simulation data is
able to predict future states of the system, given a set of controls, to a
great deal of accuracy with only a fraction of the available information, while
also reducing training times significantly compared to the original E2C and
E2CO models.
</p></li>
</ul>
<h3>Title: Synthesizing PET images from High-field and Ultra-high-field MR images Using Joint Diffusion Attention Model. (arXiv:2305.03901v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03901">http://arxiv.org/abs/2305.03901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03901] Synthesizing PET images from High-field and Ultra-high-field MR images Using Joint Diffusion Attention Model](http://arxiv.org/abs/2305.03901) #diffusion</code></li>
<li>Summary: <p>MRI and PET are crucial diagnostic tools for brain diseases, as they provide
complementary information on brain structure and function. However, PET
scanning is costly and involves radioactive exposure, resulting in a lack of
PET. Moreover, simultaneous PET and MRI at ultra-high-field are currently
hardly infeasible. Ultra-high-field imaging has unquestionably proven valuable
in both clinical and academic settings, especially in the field of cognitive
neuroimaging. These motivate us to propose a method for synthetic PET from
high-filed MRI and ultra-high-field MRI. From a statistical perspective, the
joint probability distribution (JPD) is the most direct and fundamental means
of portraying the correlation between PET and MRI. This paper proposes a novel
joint diffusion attention model which has the joint probability distribution
and attention strategy, named JDAM. JDAM has a diffusion process and a sampling
process. The diffusion process involves the gradual diffusion of PET to
Gaussian noise by adding Gaussian noise, while MRI remains fixed. JPD of MRI
and noise-added PET was learned in the diffusion process. The sampling process
is a predictor-corrector. PET images were generated from MRI by JPD of MRI and
noise-added PET. The predictor is a reverse diffusion process and the corrector
is Langevin dynamics. Experimental results on the public Alzheimer's Disease
Neuroimaging Initiative (ADNI) dataset demonstrate that the proposed method
outperforms state-of-the-art CycleGAN for high-field MRI (3T MRI). Finally,
synthetic PET images from the ultra-high-field (5T MRI and 7T MRI) be
attempted, providing a possibility for ultra-high-field PET-MRI imaging.
</p></li>
</ul>
<h3>Title: Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs. (arXiv:2305.03935v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03935">http://arxiv.org/abs/2305.03935</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03935] Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs](http://arxiv.org/abs/2305.03935) #diffusion</code></li>
<li>Summary: <p>Diffusion models have exhibited excellent performance in various domains. The
probability flow ordinary differential equation (ODE) of diffusion models
(i.e., diffusion ODEs) is a particular case of continuous normalizing flows
(CNFs), which enables deterministic inference and exact likelihood evaluation.
However, the likelihood estimation results by diffusion ODEs are still far from
those of the state-of-the-art likelihood-based generative models. In this work,
we propose several improved techniques for maximum likelihood estimation for
diffusion ODEs, including both training and evaluation perspectives. For
training, we propose velocity parameterization and explore variance reduction
techniques for faster convergence. We also derive an error-bounded high-order
flow matching objective for finetuning, which improves the ODE likelihood and
smooths its trajectory. For evaluation, we propose a novel training-free
truncated-normal dequantization to fill the training-evaluation gap commonly
existing in diffusion ODEs. Building upon these techniques, we achieve
state-of-the-art likelihood estimation results on image datasets (2.56 on
CIFAR-10, 3.43 on ImageNet-32) without variational dequantization or data
augmentation.
</p></li>
</ul>
<h3>Title: Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04111">http://arxiv.org/abs/2305.04111</a></li>
<li>Code URL: <a href="https://github.com/tufts-ml/graph-generation-edge">https://github.com/tufts-ml/graph-generation-edge</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04111] Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling](http://arxiv.org/abs/2305.04111) #diffusion</code></li>
<li>Summary: <p>Diffusion-based generative graph models have been proven effective in
generating high-quality small graphs. However, they need to be more scalable
for generating large graphs containing thousands of nodes desiring graph
statistics. In this work, we propose EDGE, a new diffusion-based generative
graph model that addresses generative tasks with large graphs. To improve
computation efficiency, we encourage graph sparsity by using a discrete
diffusion process that randomly removes edges at each time step and finally
obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at
each denoising step. It makes much fewer edge predictions than previous
diffusion-based models. Moreover, EDGE admits explicitly modeling the node
degrees of the graphs, further improving the model performance. The empirical
study shows that EDGE is much more efficient than competing methods and can
generate large graphs with thousands of nodes. It also outperforms baseline
models in generation quality: graphs generated by our approach have more
similar graph statistics to those of the training graphs.
</p></li>
</ul>
<h2>LLM</h2>
<h2>segmentation</h2>
<h3>Title: Prompt What You Need: Enhancing Segmentation in Rainy Scenes with Anchor-based Prompting. (arXiv:2305.03902v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03902">http://arxiv.org/abs/2305.03902</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03902] Prompt What You Need: Enhancing Segmentation in Rainy Scenes with Anchor-based Prompting](http://arxiv.org/abs/2305.03902) #segmentation</code></li>
<li>Summary: <p>Semantic segmentation in rainy scenes is a challenging task due to the
complex environment, class distribution imbalance, and limited annotated data.
To address these challenges, we propose a novel framework that utilizes
semi-supervised learning and pre-trained segmentation foundation model to
achieve superior performance. Specifically, our framework leverages the
semi-supervised model as the basis for generating raw semantic segmentation
results, while also serving as a guiding force to prompt pre-trained foundation
model to compensate for knowledge gaps with entropy-based anchors. In addition,
to minimize the impact of irrelevant segmentation masks generated by the
pre-trained foundation model, we also propose a mask filtering and fusion
mechanism that optimizes raw semantic segmentation results based on the
principle of minimum risk. The proposed framework achieves superior
segmentation performance on the Rainy WCity dataset and is awarded the first
prize in the sub-track of STRAIN in ICME 2023 Grand Challenges.
</p></li>
</ul>
<h3>Title: Annotation-efficient learning for OCT segmentation. (arXiv:2305.03936v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03936">http://arxiv.org/abs/2305.03936</a></li>
<li>Code URL: <a href="https://github.com/sjtu-intelligent-optics-lab/annotation-efficient-learning-for-oct-segmentation">https://github.com/sjtu-intelligent-optics-lab/annotation-efficient-learning-for-oct-segmentation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03936] Annotation-efficient learning for OCT segmentation](http://arxiv.org/abs/2305.03936) #segmentation</code></li>
<li>Summary: <p>Deep learning has been successfully applied to OCT segmentation. However, for
data from different manufacturers and imaging protocols, and for different
regions of interest (ROIs), it requires laborious and time-consuming data
annotation and training, which is undesirable in many scenarios, such as
surgical navigation and multi-center clinical trials. Here we propose an
annotation-efficient learning method for OCT segmentation that could
significantly reduce annotation costs. Leveraging self-supervised generative
learning, we train a Transformer-based model to learn the OCT imagery. Then we
connect the trained Transformer-based encoder to a CNN-based decoder, to learn
the dense pixel-wise prediction in OCT segmentation. These training phases use
open-access data and thus incur no annotation costs, and the pre-trained model
can be adapted to different data and ROIs without re-training. Based on the
greedy approximation for the k-center problem, we also introduce an algorithm
for the selective annotation of the target data. We verified our method on
publicly-available and private OCT datasets. Compared to the widely-used U-Net
model with 100% training data, our method only requires ~10% of the data for
achieving the same segmentation accuracy, and it speeds the training up to ~3.5
times. Furthermore, our proposed method outperforms other potential strategies
that could improve annotation efficiency. We think this emphasis on learning
efficiency may help improve the intelligence and application penetration of
OCT-based technologies. Our code and pre-trained model are publicly available
at
https://github.com/SJTU-Intelligent-Optics-Lab/Annotation-efficient-learning-for-OCT-segmentation.
</p></li>
</ul>
<h3>Title: Structural and Statistical Texture Knowledge Distillation for Semantic Segmentation. (arXiv:2305.03944v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03944">http://arxiv.org/abs/2305.03944</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03944] Structural and Statistical Texture Knowledge Distillation for Semantic Segmentation](http://arxiv.org/abs/2305.03944) #segmentation</code></li>
<li>Summary: <p>Existing knowledge distillation works for semantic segmentation mainly focus
on transferring high-level contextual knowledge from teacher to student.
However, low-level texture knowledge is also of vital importance for
characterizing the local structural pattern and global statistical property,
such as boundary, smoothness, regularity and color contrast, which may not be
well addressed by high-level deep features. In this paper, we are intended to
take full advantage of both structural and statistical texture knowledge and
propose a novel Structural and Statistical Texture Knowledge Distillation
(SSTKD) framework for semantic segmentation. Specifically, for structural
texture knowledge, we introduce a Contourlet Decomposition Module (CDM) that
decomposes low-level features with iterative Laplacian pyramid and directional
filter bank to mine the structural texture knowledge. For statistical
knowledge, we propose a Denoised Texture Intensity Equalization Module (DTIEM)
to adaptively extract and enhance statistical texture knowledge through
heuristics iterative quantization and denoised operation. Finally, each
knowledge learning is supervised by an individual loss function, forcing the
student network to mimic the teacher better from a broader perspective.
Experiments show that the proposed method achieves state-of-the-art performance
on Cityscapes, Pascal VOC 2012 and ADE20K datasets.
</p></li>
</ul>
<h2>object detection</h2>
<h3>Title: Context-Aware Chart Element Detection. (arXiv:2305.04151v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04151">http://arxiv.org/abs/2305.04151</a></li>
<li>Code URL: <a href="https://github.com/pengyu965/chartdete">https://github.com/pengyu965/chartdete</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04151] Context-Aware Chart Element Detection](http://arxiv.org/abs/2305.04151) #object detection</code></li>
<li>Summary: <p>As a prerequisite of chart data extraction, the accurate detection of chart
basic elements is essential and mandatory. In contrast to object detection in
the general image domain, chart element detection relies heavily on context
information as charts are highly structured data visualization formats. To
address this, we propose a novel method CACHED, which stands for Context-Aware
Chart Element Detection, by integrating a local-global context fusion module
consisting of visual context enhancement and positional context encoding with
the Cascade R-CNN framework. To improve the generalization of our method for
broader applicability, we refine the existing chart element categorization and
standardized 18 classes for chart basic elements, excluding plot elements. Our
CACHED method, with the updated category of chart elements, achieves
state-of-the-art performance in our experiments, underscoring the importance of
context in chart element detection. Extending our method to the bar plot
detection task, we obtain the best result on the PMC test dataset.
</p></li>
</ul>
<h3>Title: YOLOCS: Object Detection based on Dense Channel Compression for Feature Spatial Solidification. (arXiv:2305.04170v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04170">http://arxiv.org/abs/2305.04170</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04170] YOLOCS: Object Detection based on Dense Channel Compression for Feature Spatial Solidification](http://arxiv.org/abs/2305.04170) #object detection</code></li>
<li>Summary: <p>In this study, we examine the associations between channel features and
convolutional kernels during the processes of feature purification and gradient
backpropagation, with a focus on the forward and backward propagation within
the network. Consequently, we propose a method called Dense Channel Compression
for Feature Spatial Solidification. Drawing upon the central concept of this
method, we introduce two innovative modules for backbone and head networks: the
Dense Channel Compression for Feature Spatial Solidification Structure (DCFS)
and the Asymmetric Multi-Level Compression Decoupled Head (ADH). When
integrated into the YOLOv5 model, these two modules demonstrate exceptional
performance, resulting in a modified model referred to as YOLOCS. Evaluated on
the MSCOCO dataset, the large, medium, and small YOLOCS models yield AP of
50.1%, 47.6%, and 42.5%, respectively. Maintaining inference speeds remarkably
similar to those of the YOLOv5 model, the large, medium, and small YOLOCS
models surpass the YOLOv5 model's AP by 1.1%, 2.3%, and 5.2%, respectively.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-05-09]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
