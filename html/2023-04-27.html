<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: iMixer: hierarchical Hopfield network implies an invertible, implicit and iterative MLP-Mixer. (arXiv:2304.13061v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13061">http://arxiv.org/abs/2304.13061</a></li>
<li>Code URL: <a href="https://github.com/toshihiro-ota/imixer">https://github.com/toshihiro-ota/imixer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13061] iMixer: hierarchical Hopfield network implies an invertible, implicit and iterative MLP-Mixer](http://arxiv.org/abs/2304.13061) #transformer</code></li>
<li>Summary: <p>In the last few years, the success of Transformers in computer vision has
stimulated the discovery of many alternative models that compete with
Transformers, such as the MLP-Mixer. Despite their weak induced bias, these
models have achieved performance comparable to well-studied convolutional
neural networks. Recent studies on modern Hopfield networks suggest the
correspondence between certain energy-based associative memory models and
Transformers or MLP-Mixer, and shed some light on the theoretical background of
the Transformer-type architectures design. In this paper we generalize the
correspondence to the recently introduced hierarchical Hopfield network, and
find iMixer, a novel generalization of MLP-Mixer model. Unlike ordinary
feedforward neural networks, iMixer involves MLP layers that propagate forward
from the output side to the input side. We characterize the module as an
example of invertible, implicit, and iterative mixing module. We evaluate the
model performance with various datasets on image classification tasks, and find
that iMixer reasonably achieves the improvement compared to the baseline
vanilla MLP-Mixer. The results imply that the correspondence between the
Hopfield networks and the Mixer models serves as a principle for understanding
a broader class of Transformer-like architecture designs.
</p></li>
</ul>
<h3>Title: Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations. (arXiv:2304.13089v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13089">http://arxiv.org/abs/2304.13089</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13089] Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations](http://arxiv.org/abs/2304.13089) #transformer</code></li>
<li>Summary: <p>Joint-embedding based learning (e.g., SimCLR, MoCo, DINO) and
reconstruction-based learning (e.g., BEiT, SimMIM, MAE) are the two leading
paradigms for self-supervised learning of vision transformers, but they differ
substantially in their transfer performance. Here, we aim to explain these
differences by analyzing the impact of these objectives on the structure and
transferability of the learned representations. Our analysis reveals that
reconstruction-based learning features are significantly dissimilar to
joint-embedding based learning features and that models trained with similar
objectives learn similar features even across architectures. These differences
arise early in the network and are primarily driven by attention and
normalization layers. We find that joint-embedding features yield better linear
probe transfer for classification because the different objectives drive
different distributions of information and invariances in the learned
representation. These differences explain opposite trends in transfer
performance for downstream tasks that require spatial specificity in features.
Finally, we address how fine-tuning changes reconstructive representations to
enable better transfer, showing that fine-tuning re-organizes the information
to be more similar to pre-trained joint embedding models.
</p></li>
</ul>
<h3>Title: AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction. (arXiv:2304.13115v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13115">http://arxiv.org/abs/2304.13115</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13115] AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction](http://arxiv.org/abs/2304.13115) #transformer</code></li>
<li>Summary: <p>In this work, we present a multimodal solution to the problem of 4D face
reconstruction from monocular videos. 3D face reconstruction from 2D images is
an under-constrained problem due to the ambiguity of depth. State-of-the-art
methods try to solve this problem by leveraging visual information from a
single image or video, whereas 3D mesh animation approaches rely more on audio.
However, in most cases (e.g. AR/VR applications), videos include both visual
and speech information. We propose AVFace that incorporates both modalities and
accurately reconstructs the 4D facial and lip motion of any speaker, without
requiring any 3D ground truth for training. A coarse stage estimates the
per-frame parameters of a 3D morphable model, followed by a lip refinement, and
then a fine stage recovers facial geometric details. Due to the temporal audio
and video information captured by transformer-based modules, our method is
robust in cases when either modality is insufficient (e.g. face occlusions).
Extensive qualitative and quantitative evaluation demonstrates the superiority
of our method over the current state-of-the-art.
</p></li>
</ul>
<h3>Title: LEMaRT: Label-Efficient Masked Region Transform for Image Harmonization. (arXiv:2304.13166v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13166">http://arxiv.org/abs/2304.13166</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13166] LEMaRT: Label-Efficient Masked Region Transform for Image Harmonization](http://arxiv.org/abs/2304.13166) #transformer</code></li>
<li>Summary: <p>We present a simple yet effective self-supervised pre-training method for
image harmonization which can leverage large-scale unannotated image datasets.
To achieve this goal, we first generate pre-training data online with our
Label-Efficient Masked Region Transform (LEMaRT) pipeline. Given an image,
LEMaRT generates a foreground mask and then applies a set of transformations to
perturb various visual attributes, e.g., defocus blur, contrast, saturation, of
the region specified by the generated mask. We then pre-train image
harmonization models by recovering the original image from the perturbed image.
Secondly, we introduce an image harmonization model, namely SwinIH, by
retrofitting the Swin Transformer [27] with a combination of local and global
self-attention mechanisms. Pre-training SwinIH with LEMaRT results in a new
state of the art for image harmonization, while being label-efficient, i.e.,
consuming less annotated data for fine-tuning than existing methods. Notably,
on iHarmony4 dataset [8], SwinIH outperforms the state of the art, i.e., SCS-Co
[16] by a margin of 0.4 dB when it is fine-tuned on only 50% of the training
data, and by 1.0 dB when it is trained on the full training dataset.
</p></li>
</ul>
<h3>Title: StepFormer: Self-supervised Step Discovery and Localization in Instructional Videos. (arXiv:2304.13265v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13265">http://arxiv.org/abs/2304.13265</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13265] StepFormer: Self-supervised Step Discovery and Localization in Instructional Videos](http://arxiv.org/abs/2304.13265) #transformer</code></li>
<li>Summary: <p>Instructional videos are an important resource to learn procedural tasks from
human demonstrations. However, the instruction steps in such videos are
typically short and sparse, with most of the video being irrelevant to the
procedure. This motivates the need to temporally localize the instruction steps
in such videos, i.e. the task called key-step localization. Traditional methods
for key-step localization require video-level human annotations and thus do not
scale to large datasets. In this work, we tackle the problem with no human
supervision and introduce StepFormer, a self-supervised model that discovers
and localizes instruction steps in a video. StepFormer is a transformer decoder
that attends to the video with learnable queries, and produces a sequence of
slots capturing the key-steps in the video. We train our system on a large
dataset of instructional videos, using their automatically-generated subtitles
as the only source of supervision. In particular, we supervise our system with
a sequence of text narrations using an order-aware loss function that filters
out irrelevant phrases. We show that our model outperforms all previous
unsupervised and weakly-supervised approaches on step detection and
localization by a large margin on three challenging benchmarks. Moreover, our
model demonstrates an emergent property to solve zero-shot multi-step
localization and outperforms all relevant baselines at this task.
</p></li>
</ul>
<h3>Title: Learnable Ophthalmology SAM. (arXiv:2304.13425v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13425">http://arxiv.org/abs/2304.13425</a></li>
<li>Code URL: <a href="https://github.com/qsingle/learnablepromptsam">https://github.com/qsingle/learnablepromptsam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13425] Learnable Ophthalmology SAM](http://arxiv.org/abs/2304.13425) #transformer</code></li>
<li>Summary: <p>Segmentation is vital for ophthalmology image analysis. But its various modal
images hinder most of the existing segmentation algorithms applications, as
they rely on training based on a large number of labels or hold weak
generalization ability. Based on Segment Anything (SAM), we propose a simple
but effective learnable prompt layer suitable for multiple target segmentation
in ophthalmology multi-modal images, named Learnable Ophthalmology Segment
Anything (SAM). The learnable prompt layer learns medical prior knowledge from
each transformer layer. During training, we only train the prompt layer and
task head based on a one-shot mechanism. We demonstrate the effectiveness of
our thought based on four medical segmentation tasks based on nine publicly
available datasets. Moreover, we only provide a new improvement thought for
applying the existing fundamental CV models in the medical field. Our codes are
available at \href{https://github.com/Qsingle/LearnablePromptSAM}{website}.
</p></li>
</ul>
<h3>Title: Key-value information extraction from full handwritten pages. (arXiv:2304.13530v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13530">http://arxiv.org/abs/2304.13530</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13530] Key-value information extraction from full handwritten pages](http://arxiv.org/abs/2304.13530) #transformer</code></li>
<li>Summary: <p>We propose a Transformer-based approach for information extraction from
digitized handwritten documents. Our approach combines, in a single model, the
different steps that were so far performed by separate models: feature
extraction, handwriting recognition and named entity recognition. We compare
this integrated approach with traditional two-stage methods that perform
handwriting recognition before named entity recognition, and present results at
different levels: line, paragraph, and page. Our experiments show that
attention-based models are especially interesting when applied on full pages,
as they do not require any prior segmentation step. Finally, we show that they
are able to learn from key-value annotations: a list of important words with
their corresponding named entities. We compare our models to state-of-the-art
methods on three public databases (IAM, ESPOSALLES, and POPP) and outperform
previous performances on all three datasets.
</p></li>
</ul>
<h3>Title: SIMARA: a database for key-value information extraction from full pages. (arXiv:2304.13606v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13606">http://arxiv.org/abs/2304.13606</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13606] SIMARA: a database for key-value information extraction from full pages](http://arxiv.org/abs/2304.13606) #transformer</code></li>
<li>Summary: <p>We propose a new database for information extraction from historical
handwritten documents. The corpus includes 5,393 finding aids from six
different series, dating from the 18th-20th centuries. Finding aids are
handwritten documents that contain metadata describing older archives. They are
stored in the National Archives of France and are used by archivists to
identify and find archival documents. Each document is annotated at page-level,
and contains seven fields to retrieve. The localization of each field is not
available in such a way that this dataset encourages research on
segmentation-free systems for information extraction. We propose a model based
on the Transformer architecture trained for end-to-end information extraction
and provide three sets for training, validation and testing, to ensure fair
comparison with future works. The database is freely accessible at
https://zenodo.org/record/7868059.
</p></li>
</ul>
<h3>Title: PVP: Pre-trained Visual Parameter-Efficient Tuning. (arXiv:2304.13639v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13639">http://arxiv.org/abs/2304.13639</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13639] PVP: Pre-trained Visual Parameter-Efficient Tuning](http://arxiv.org/abs/2304.13639) #transformer</code></li>
<li>Summary: <p>Large-scale pre-trained transformers have demonstrated remarkable success in
various computer vision tasks. However, it is still highly challenging to fully
fine-tune these models for downstream tasks due to their high computational and
storage costs. Recently, Parameter-Efficient Tuning (PETuning) techniques,
e.g., Visual Prompt Tuning (VPT) and Low-Rank Adaptation (LoRA), have
significantly reduced the computation and storage cost by inserting lightweight
prompt modules into the pre-trained models and tuning these prompt modules with
a small number of trainable parameters, while keeping the transformer backbone
frozen. Although only a few parameters need to be adjusted, most PETuning
methods still require a significant amount of downstream task training data to
achieve good results. The performance is inadequate on low-data regimes,
especially when there are only one or two examples per class. To this end, we
first empirically identify the poor performance is mainly due to the
inappropriate way of initializing prompt modules, which has also been verified
in the pre-trained language models. Next, we propose a Pre-trained Visual
Parameter-efficient (PVP) Tuning framework, which pre-trains the
parameter-efficient tuning modules first and then leverages the pre-trained
modules along with the pre-trained transformer backbone to perform
parameter-efficient tuning on downstream tasks. Experiment results on five
Fine-Grained Visual Classification (FGVC) and VTAB-1k datasets demonstrate that
our proposed method significantly outperforms state-of-the-art PETuning
methods.
</p></li>
</ul>
<h3>Title: UniNeXt: Exploring A Unified Architecture for Vision Recognition. (arXiv:2304.13700v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13700">http://arxiv.org/abs/2304.13700</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13700] UniNeXt: Exploring A Unified Architecture for Vision Recognition](http://arxiv.org/abs/2304.13700) #transformer</code></li>
<li>Summary: <p>Vision Transformers have shown great potential in computer vision tasks. Most
recent works have focused on elaborating the spatial token mixer for
performance gains. However, we observe that a well-designed general
architecture can significantly improve the performance of the entire backbone,
regardless of which spatial token mixer is equipped. In this paper, we propose
UniNeXt, an improved general architecture for the vision backbone. To verify
its effectiveness, we instantiate the spatial token mixer with various typical
and modern designs, including both convolution and attention modules. Compared
with the architecture in which they are first proposed, our UniNeXt
architecture can steadily boost the performance of all the spatial token
mixers, and narrows the performance gap among them. Surprisingly, our UniNeXt
equipped with naive local window attention even outperforms the previous
state-of-the-art. Interestingly, the ranking of these spatial token mixers also
changes under our UniNeXt, suggesting that an excellent spatial token mixer may
be stifled due to a suboptimal general architecture, which further shows the
importance of the study on the general architecture of vision backbone. All
models and codes will be publicly available.
</p></li>
</ul>
<h3>Title: Pretrain on just structure: Understanding linguistic inductive biases using transfer learning. (arXiv:2304.13060v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13060">http://arxiv.org/abs/2304.13060</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13060] Pretrain on just structure: Understanding linguistic inductive biases using transfer learning](http://arxiv.org/abs/2304.13060) #transformer</code></li>
<li>Summary: <p>Both humans and transformer language models are able to learn language
without explicit structural supervision. What inductive learning biases make
this learning possible? In this study, we examine the effect of different
inductive learning biases by predisposing language models with structural
biases through pretraining on artificial structured data, and then evaluating
by fine-tuning on English. Our experimental setup gives us the ability to
actively control the inductive bias of language models. With our experiments,
we investigate the comparative success of three types of inductive bias: 1) an
inductive bias for recursive, hierarchical processing 2) an inductive bias for
unrestricted token-token dependencies that can't be modeled by context-free
grammars, and 3) an inductive bias for a Zipfian power-law vocabulary
distribution. We show that complex token-token interactions form the best
inductive biases, and that this is strongest in the non-context-free case. We
also show that a Zipfian vocabulary distribution forms a good inductive bias
independently from grammatical structure. Our study leverages the capabilities
of transformer models to run controlled language learning experiments that are
not possible to run in humans, and surfaces hypotheses about the structures
that facilitate language learning in both humans and machines.
</p></li>
</ul>
<h3>Title: The Closeness of In-Context Learning and Weight Shifting for Softmax Regression. (arXiv:2304.13276v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13276">http://arxiv.org/abs/2304.13276</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13276] The Closeness of In-Context Learning and Weight Shifting for Softmax Regression](http://arxiv.org/abs/2304.13276) #transformer</code></li>
<li>Summary: <p>Large language models (LLMs) are known for their exceptional performance in
natural language processing, making them highly effective in many human
life-related or even job-related tasks. The attention mechanism in the
Transformer architecture is a critical component of LLMs, as it allows the
model to selectively focus on specific input parts. The softmax unit, which is
a key part of the attention mechanism, normalizes the attention scores. Hence,
the performance of LLMs in various NLP tasks depends significantly on the
crucial role played by the attention mechanism with the softmax unit.
</p></li>
</ul>
<p>In-context learning, as one of the celebrated abilities of recent LLMs, is an
important concept in querying LLMs such as ChatGPT. Without further parameter
updates, Transformers can learn to predict based on few in-context examples.
However, the reason why Transformers becomes in-context learners is not well
understood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the
in-context learning from a mathematical perspective based on a linear
regression formulation $\min_x\| Ax - b \|_2$, which show Transformers'
capability of learning linear functions in context.
</p>
<p>In this work, we study the in-context learning based on a softmax regression
formulation $\min_{x} \| \langle \exp(Ax), {\bf 1}_n \rangle^{-1} \exp(Ax) - b
\|_2$ of Transformer's attention mechanism. We show the upper bounds of the
data transformations induced by a single self-attention layer and by
gradient-descent on a $\ell_2$ regression loss for softmax prediction function,
which imply that when training self-attention-only Transformers for fundamental
regression tasks, the models learned by gradient-descent and Transformers show
great similarity.
</p>

<h3>Title: Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13567">http://arxiv.org/abs/2304.13567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13567] Impact of Position Bias on Language Models in Token Classification](http://arxiv.org/abs/2304.13567) #transformer</code></li>
<li>Summary: <p>Language Models (LMs) have shown state-of-the-art performance in Natural
Language Processing (NLP) tasks. Downstream tasks such as Named Entity
Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data
imbalance issues, specifically in terms of the ratio of positive to negative
examples, and class imbalance. In this paper, we investigate an additional
specific issue for language models, namely the position bias of positive
examples in token classification tasks. Therefore, we conduct an in-depth
evaluation of the impact of position bias on the performance of LMs when
fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and
OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We
propose an evaluation approach to investigate position bias in Transformer
models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as
GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in
their performance. To mitigate this effect, we propose two methods: Random
Position Shifting and Context Perturbation, that we apply on batches during the
training process. The results show an improvement of $\approx$ 2\% in the
performance of the model on CoNLL03, UD_en, and TweeBank.
</p></li>
</ul>
<h3>Title: Tensor Decomposition for Model Reduction in Neural Networks: A Review. (arXiv:2304.13539v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13539">http://arxiv.org/abs/2304.13539</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13539] Tensor Decomposition for Model Reduction in Neural Networks: A Review](http://arxiv.org/abs/2304.13539) #transformer</code></li>
<li>Summary: <p>Modern neural networks have revolutionized the fields of computer vision (CV)
and Natural Language Processing (NLP). They are widely used for solving complex
CV tasks and NLP tasks such as image classification, image generation, and
machine translation. Most state-of-the-art neural networks are
over-parameterized and require a high computational cost. One straightforward
solution is to replace the layers of the networks with their low-rank tensor
approximations using different tensor decomposition methods. This paper reviews
six tensor decomposition methods and illustrates their ability to compress
model parameters of convolutional neural networks (CNNs), recurrent neural
networks (RNNs) and Transformers. The accuracy of some compressed models can be
higher than the original versions. Evaluations indicate that tensor
decompositions can achieve significant reductions in model size, run-time and
energy consumption, and are well suited for implementing neural networks on
edge devices.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: LumiGAN: Unconditional Generation of Relightable 3D Human Faces. (arXiv:2304.13153v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13153">http://arxiv.org/abs/2304.13153</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13153] LumiGAN: Unconditional Generation of Relightable 3D Human Faces](http://arxiv.org/abs/2304.13153) #generative</code></li>
<li>Summary: <p>Unsupervised learning of 3D human faces from unstructured 2D image data is an
active research area. While recent works have achieved an impressive level of
photorealism, they commonly lack control of lighting, which prevents the
generated assets from being deployed in novel environments. To this end, we
introduce LumiGAN, an unconditional Generative Adversarial Network (GAN) for 3D
human faces with a physically based lighting module that enables relighting
under novel illumination at inference time. Unlike prior work, LumiGAN can
create realistic shadow effects using an efficient visibility formulation that
is learned in a self-supervised manner. LumiGAN generates plausible physical
properties for relightable faces, including surface normals, diffuse albedo,
and specular tint without any ground truth data. In addition to relightability,
we demonstrate significantly improved geometry generation compared to
state-of-the-art non-relightable 3D GANs and notably better photorealism than
existing relightable GANs.
</p></li>
</ul>
<h3>Title: Training-Free Location-Aware Text-to-Image Synthesis. (arXiv:2304.13427v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13427">http://arxiv.org/abs/2304.13427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13427] Training-Free Location-Aware Text-to-Image Synthesis](http://arxiv.org/abs/2304.13427) #generative</code></li>
<li>Summary: <p>Current large-scale generative models have impressive efficiency in
generating high-quality images based on text prompts. However, they lack the
ability to precisely control the size and position of objects in the generated
image. In this study, we analyze the generative mechanism of the stable
diffusion model and propose a new interactive generation paradigm that allows
users to specify the position of generated objects without additional training.
Moreover, we propose an object detection-based evaluation metric to assess the
control capability of location aware generation task. Our experimental results
show that our method outperforms state-of-the-art methods on both control
capacity and image quality.
</p></li>
</ul>
<h3>Title: Controllable Image Generation via Collage Representations. (arXiv:2304.13722v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13722">http://arxiv.org/abs/2304.13722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13722] Controllable Image Generation via Collage Representations](http://arxiv.org/abs/2304.13722) #generative</code></li>
<li>Summary: <p>Recent advances in conditional generative image models have enabled
impressive results. On the one hand, text-based conditional models have
achieved remarkable generation quality, by leveraging large-scale datasets of
image-text pairs. To enable fine-grained controllability, however, text-based
models require long prompts, whose details may be ignored by the model. On the
other hand, layout-based conditional models have also witnessed significant
advances. These models rely on bounding boxes or segmentation maps for precise
spatial conditioning in combination with coarse semantic labels. The semantic
labels, however, cannot be used to express detailed appearance characteristics.
In this paper, we approach fine-grained scene controllability through image
collages which allow a rich visual description of the desired scene as well as
the appearance and location of the objects therein, without the need of class
nor attribute labels. We introduce "mixing and matching scenes" (M&amp;Ms), an
approach that consists of an adversarially trained generative image model which
is conditioned on appearance features and spatial positions of the different
elements in a collage, and integrates these into a coherent image. We train our
model on the OpenImages (OI) dataset and evaluate it on collages derived from
OI and MS-COCO datasets. Our experiments on the OI dataset show that M&amp;Ms
outperforms baselines in terms of fine-grained scene controllability while
being very competitive in terms of image quality and sample diversity. On the
MS-COCO dataset, we highlight the generalization ability of our model by
outperforming DALL-E in terms of the zero-shot FID metric, despite using two
magnitudes fewer parameters and data. Collage based generative models have the
potential to advance content creation in an efficient and effective way as they
are intuitive to use and yield high quality generations.
</p></li>
</ul>
<h3>Title: Directed Chain Generative Adversarial Networks. (arXiv:2304.13131v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13131">http://arxiv.org/abs/2304.13131</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13131] Directed Chain Generative Adversarial Networks](http://arxiv.org/abs/2304.13131) #generative</code></li>
<li>Summary: <p>Real-world data can be multimodal distributed, e.g., data describing the
opinion divergence in a community, the interspike interval distribution of
neurons, and the oscillators natural frequencies. Generating multimodal
distributed real-world data has become a challenge to existing generative
adversarial networks (GANs). For example, neural stochastic differential
equations (Neural SDEs), treated as infinite-dimensional GANs, have
demonstrated successful performance mainly in generating unimodal time series
data. In this paper, we propose a novel time series generator, named directed
chain GANs (DC-GANs), which inserts a time series dataset (called a
neighborhood process of the directed chain or input) into the drift and
diffusion coefficients of the directed chain SDEs with distributional
constraints. DC-GANs can generate new time series of the same distribution as
the neighborhood process, and the neighborhood process will provide the key
step in learning and generating multimodal distributed time series. The
proposed DC-GANs are examined on four datasets, including two stochastic models
from social sciences and computational neuroscience, and two real-world
datasets on stock prices and energy consumption. To our best knowledge, DC-GANs
are the first work that can generate multimodal time series data and
consistently outperforms state-of-the-art benchmarks with respect to measures
of distribution, data similarity, and predictive ability.
</p></li>
</ul>
<h3>Title: Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation. (arXiv:2304.13224v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13224">http://arxiv.org/abs/2304.13224</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13224] Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation](http://arxiv.org/abs/2304.13224) #generative</code></li>
<li>Summary: <p>The proposed BSDE-based diffusion model represents a novel approach to
diffusion modeling, which extends the application of stochastic differential
equations (SDEs) in machine learning. Unlike traditional SDE-based diffusion
models, our model can determine the initial conditions necessary to reach a
desired terminal distribution by adapting an existing score function. We
demonstrate the theoretical guarantees of the model, the benefits of using
Lipschitz networks for score matching, and its potential applications in
various areas such as diffusion inversion, conditional diffusion, and
uncertainty quantification. Our work represents a contribution to the field of
score-based generative learning and offers a promising direction for solving
real-world problems.
</p></li>
</ul>
<h2>label correction</h2>
<h2>noise</h2>
<h3>Title: Towards Reliable Colorectal Cancer Polyps Classification via Vision Based Tactile Sensing and Confidence-Calibrated Neural Networks. (arXiv:2304.13192v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13192">http://arxiv.org/abs/2304.13192</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13192] Towards Reliable Colorectal Cancer Polyps Classification via Vision Based Tactile Sensing and Confidence-Calibrated Neural Networks](http://arxiv.org/abs/2304.13192) #noise</code></li>
<li>Summary: <p>In this study, toward addressing the over-confident outputs of existing
artificial intelligence-based colorectal cancer (CRC) polyp classification
techniques, we propose a confidence-calibrated residual neural network.
Utilizing a novel vision-based tactile sensing (VS-TS) system and unique CRC
polyp phantoms, we demonstrate that traditional metrics such as accuracy and
precision are not sufficient to encapsulate model performance for handling a
sensitive CRC polyp diagnosis. To this end, we develop a residual neural
network classifier and address its over-confident outputs for CRC polyps
classification via the post-processing method of temperature scaling. To
evaluate the proposed method, we introduce noise and blur to the obtained
textural images of the VS-TS and test the model's reliability for non-ideal
inputs through reliability diagrams and other statistical metrics.
</p></li>
</ul>
<h3>Title: Discrepancy-Guided Reconstruction Learning for Image Forgery Detection. (arXiv:2304.13349v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13349">http://arxiv.org/abs/2304.13349</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13349] Discrepancy-Guided Reconstruction Learning for Image Forgery Detection](http://arxiv.org/abs/2304.13349) #noise</code></li>
<li>Summary: <p>In this paper, we propose a novel image forgery detection paradigm for
boosting the model learning capacity on both forgery-sensitive and genuine
compact visual patterns. Compared to the existing methods that only focus on
the discrepant-specific patterns (\eg, noises, textures, and frequencies), our
method has a greater generalization. Specifically, we first propose a
Discrepancy-Guided Encoder (DisGE) to extract forgery-sensitive visual
patterns. DisGE consists of two branches, where the mainstream backbone branch
is used to extract general semantic features, and the accessorial discrepant
external attention branch is used to extract explicit forgery cues. Besides, a
Double-Head Reconstruction (DouHR) module is proposed to enhance genuine
compact visual patterns in different granular spaces. Under DouHR, we further
introduce a Discrepancy-Aggregation Detector (DisAD) to aggregate these genuine
compact visual patterns, such that the forgery detection capability on unknown
patterns can be improved. Extensive experimental results on four challenging
datasets validate the effectiveness of our proposed method against
state-of-the-art competitors.
</p></li>
</ul>
<h3>Title: Non-rigid Point Cloud Registration for Middle Ear Diagnostics with Endoscopic Optical Coherence Tomography. (arXiv:2304.13618v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13618">http://arxiv.org/abs/2304.13618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13618] Non-rigid Point Cloud Registration for Middle Ear Diagnostics with Endoscopic Optical Coherence Tomography](http://arxiv.org/abs/2304.13618) #noise</code></li>
<li>Summary: <p>Purpose: Middle ear infection is the most prevalent inflammatory disease,
especially among the pediatric population. Current diagnostic methods are
subjective and depend on visual cues from an otoscope, which is limited for
otologists to identify pathology. To address this shortcoming, endoscopic
optical coherence tomography (OCT) provides both morphological and functional
in-vivo measurements of the middle ear. However, due to the shadow of prior
structures, interpretation of OCT images is challenging and time-consuming. To
facilitate fast diagnosis and measurement, improvement in the readability of
OCT data is achieved by merging morphological knowledge from ex-vivo middle ear
models with OCT volumetric data, so that OCT applications can be further
promoted in daily clinical settings. Methods: We propose C2P-Net: a two-staged
non-rigid registration pipeline for complete to partial point clouds, which are
sampled from ex-vivo and in-vivo OCT models, respectively. To overcome the lack
of labeled training data, a fast and effective generation pipeline in Blender3D
is designed to simulate middle ear shapes and extract in-vivo noisy and partial
point clouds. Results: We evaluate the performance of C2P-Net through
experiments on both synthetic and real OCT datasets. The results demonstrate
that C2P-Net is generalized to unseen middle ear point clouds and capable of
handling realistic noise and incompleteness in synthetic and real OCT data.
Conclusion: In this work, we aim to enable diagnosis of middle ear structures
with the assistance of OCT images. We propose C2P-Net: a two-staged non-rigid
registration pipeline for point clouds to support the interpretation of in-vivo
noisy and partial OCT images for the first time. Code is available at:
https://gitlab.com/nct_tso_public/c2p-net.
</p></li>
</ul>
<h3>Title: ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode. (arXiv:2304.13140v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13140">http://arxiv.org/abs/2304.13140</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13140] ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode](http://arxiv.org/abs/2304.13140) #noise</code></li>
<li>Summary: <p>The challenges faced by text classification with large tag systems in natural
language processing tasks include multiple tag systems, uneven data
distribution, and high noise. To address these problems, the ESimCSE
unsupervised comparative learning and UDA semi-supervised comparative learning
models are combined through the use of joint training techniques in the
models.The ESimCSE model efficiently learns text vector representations using
unlabeled data to achieve better classification results, while UDA is trained
using unlabeled data through semi-supervised learning methods to improve the
prediction performance of the models and stability, and further improve the
generalization ability of the model. In addition, adversarial training
techniques FGM and PGD are used in the model training process to improve the
robustness and reliability of the model. The experimental results show that
there is an 8% and 10% accuracy improvement relative to Baseline on the public
dataset Ruesters as well as on the operational dataset, respectively, and a 15%
improvement in manual validation accuracy can be achieved on the operational
dataset, indicating that the method is effective.
</p></li>
</ul>
<h3>Title: HeySQuAD: A Spoken Question Answering Dataset. (arXiv:2304.13689v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13689">http://arxiv.org/abs/2304.13689</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13689] HeySQuAD: A Spoken Question Answering Dataset](http://arxiv.org/abs/2304.13689) #noise</code></li>
<li>Summary: <p>Human-spoken questions are critical to evaluating the performance of spoken
question answering (SQA) systems that serve several real-world use cases
including digital assistants. We present a new large-scale community-shared SQA
dataset, HeySQuAD that consists of 76k human-spoken questions and 97k
machine-generated questions and corresponding textual answers derived from the
SQuAD QA dataset. The goal of HeySQuAD is to measure the ability of machines to
understand noisy spoken questions and answer the questions accurately. To this
end, we run extensive benchmarks on the human-spoken and machine-generated
questions to quantify the differences in noise from both sources and its
subsequent impact on the model and answering accuracy. Importantly, for the
task of SQA, where we want to answer human-spoken questions, we observe that
training using the transcribed human-spoken and original SQuAD questions leads
to significant improvements (12.51%) over training using only the original
SQuAD textual questions.
</p></li>
</ul>
<h3>Title: LSTM-based Load Forecasting Robustness Against Noise Injection Attack in Microgrid. (arXiv:2304.13104v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13104">http://arxiv.org/abs/2304.13104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13104] LSTM-based Load Forecasting Robustness Against Noise Injection Attack in Microgrid](http://arxiv.org/abs/2304.13104) #noise</code></li>
<li>Summary: <p>In this paper, we investigate the robustness of an LSTM neural network
against noise injection attacks for electric load forecasting in an ideal
microgrid. The performance of the LSTM model is investigated under a black-box
Gaussian noise attack with different SNRs. It is assumed that attackers have
just access to the input data of the LSTM model. The results show that the
noise attack affects the performance of the LSTM model. The load prediction
means absolute error (MAE) is 0.047 MW for a healthy prediction, while this
value increases up to 0.097 MW for a Gaussian noise insertion with SNR= 6 dB.
To robustify the LSTM model against noise attack, a low-pass filter with
optimal cut-off frequency is applied at the model's input to remove the noise
attack. The filter performs better in case of noise with lower SNR and is less
promising for small noises.
</p></li>
</ul>
<h3>Title: Killing Two Birds with One Stone: Quantization Achieves Privacy in Distributed Learning. (arXiv:2304.13545v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13545">http://arxiv.org/abs/2304.13545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13545] Killing Two Birds with One Stone: Quantization Achieves Privacy in Distributed Learning](http://arxiv.org/abs/2304.13545) #noise</code></li>
<li>Summary: <p>Communication efficiency and privacy protection are two critical issues in
distributed machine learning. Existing methods tackle these two issues
separately and may have a high implementation complexity that constrains their
application in a resource-limited environment. We propose a comprehensive
quantization-based solution that could simultaneously achieve communication
efficiency and privacy protection, providing new insights into the correlated
nature of communication and privacy. Specifically, we demonstrate the
effectiveness of our proposed solutions in the distributed stochastic gradient
descent (SGD) framework by adding binomial noise to the uniformly quantized
gradients to reach the desired differential privacy level but with a minor
sacrifice in communication efficiency. We theoretically capture the new
trade-offs between communication, privacy, and learning performance.
</p></li>
</ul>
<h3>Title: Dynamic Datasets and Market Environments for Financial Reinforcement Learning. (arXiv:2304.13174v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13174">http://arxiv.org/abs/2304.13174</a></li>
<li>Code URL: <a href="https://github.com/ai4finance-foundation/finrl-meta">https://github.com/ai4finance-foundation/finrl-meta</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13174] Dynamic Datasets and Market Environments for Financial Reinforcement Learning](http://arxiv.org/abs/2304.13174) #noise</code></li>
<li>Summary: <p>The financial market is a particularly challenging playground for deep
reinforcement learning due to its unique feature of dynamic datasets. Building
high-quality market environments for training financial reinforcement learning
(FinRL) agents is difficult due to major factors such as the low
signal-to-noise ratio of financial data, survivorship bias of historical data,
and model overfitting. In this paper, we present FinRL-Meta, a data-centric and
openly accessible library that processes dynamic datasets from real-world
markets into gym-style market environments and has been actively maintained by
the AI4Finance community. First, following a DataOps paradigm, we provide
hundreds of market environments through an automatic data curation pipeline.
Second, we provide homegrown examples and reproduce popular research papers as
stepping stones for users to design new trading strategies. We also deploy the
library on cloud platforms so that users can visualize their own results and
assess the relative performance via community-wise competitions. Third, we
provide dozens of Jupyter/Python demos organized into a curriculum and a
documentation website to serve the rapidly growing community. The open-source
codes for the data curation pipeline are available at
https://github.com/AI4Finance-Foundation/FinRL-Meta
</p></li>
</ul>
<h3>Title: Regression with Sensor Data Containing Incomplete Observations. (arXiv:2304.13415v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13415">http://arxiv.org/abs/2304.13415</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13415] Regression with Sensor Data Containing Incomplete Observations](http://arxiv.org/abs/2304.13415) #noise</code></li>
<li>Summary: <p>This paper addresses a regression problem in which output label values are
the results of sensing the magnitude of a phenomenon. A low value of such
labels can mean either that the actual magnitude of the phenomenon was low or
that the sensor made an incomplete observation. This leads to a bias toward
lower values in labels and its resultant learning because labels may have lower
values due to incomplete observations, even if the actual magnitude of the
phenomenon was high. Moreover, because an incomplete observation does not
provide any tags indicating incompleteness, we cannot eliminate or impute them.
To address this issue, we propose a learning algorithm that explicitly models
incomplete observations corrupted with an asymmetric noise that always has a
negative value. We show that our algorithm is unbiased as if it were learned
from uncorrupted data that does not involve incomplete observations. We
demonstrate the advantages of our algorithm through numerical experiments.
</p></li>
</ul>
<h2>diffusion</h2>
<h3>Title: Single-View Height Estimation with Conditional Diffusion Probabilistic Models. (arXiv:2304.13214v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13214">http://arxiv.org/abs/2304.13214</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13214] Single-View Height Estimation with Conditional Diffusion Probabilistic Models](http://arxiv.org/abs/2304.13214) #diffusion</code></li>
<li>Summary: <p>Digital Surface Models (DSM) offer a wealth of height information for
understanding the Earth's surface as well as monitoring the existence or change
in natural and man-made structures. Classical height estimation requires
multi-view geospatial imagery or LiDAR point clouds which can be expensive to
acquire. Single-view height estimation using neural network based models shows
promise however it can struggle with reconstructing high resolution features.
The latest advancements in diffusion models for high resolution image synthesis
and editing have yet to be utilized for remote sensing imagery, particularly
height estimation. Our approach involves training a generative diffusion model
to learn the joint distribution of optical and DSM images across both domains
as a Markov chain. This is accomplished by minimizing a denoising score
matching objective while being conditioned on the source image to generate
realistic high resolution 3D surfaces. In this paper we experiment with
conditional denoising diffusion probabilistic models (DDPM) for height
estimation from a single remotely sensed image and show promising results on
the Vaihingen benchmark dataset.
</p></li>
</ul>
<h3>Title: Diffusion Probabilistic Model Based Accurate and High-Degree-of-Freedom Metasurface Inverse Design. (arXiv:2304.13038v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13038">http://arxiv.org/abs/2304.13038</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13038] Diffusion Probabilistic Model Based Accurate and High-Degree-of-Freedom Metasurface Inverse Design](http://arxiv.org/abs/2304.13038) #diffusion</code></li>
<li>Summary: <p>Conventional meta-atom designs rely heavily on researchers' prior knowledge
and trial-and-error searches using full-wave simulations, resulting in
time-consuming and inefficient processes. Inverse design methods based on
optimization algorithms, such as evolutionary algorithms, and topological
optimizations, have been introduced to design metamaterials. However, none of
these algorithms are general enough to fulfill multi-objective tasks. Recently,
deep learning methods represented by Generative Adversarial Networks (GANs)
have been applied to inverse design of metamaterials, which can directly
generate high-degree-of-freedom meta-atoms based on S-parameter requirements.
However, the adversarial training process of GANs makes the network unstable
and results in high modeling costs. This paper proposes a novel metamaterial
inverse design method based on the diffusion probability theory. By learning
the Markov process that transforms the original structure into a Gaussian
distribution, the proposed method can gradually remove the noise starting from
the Gaussian distribution and generate new high-degree-of-freedom meta-atoms
that meet S-parameter conditions, which avoids the model instability introduced
by the adversarial training process of GANs and ensures more accurate and
high-quality generation results. Experiments have proven that our method is
superior to representative methods of GANs in terms of model convergence speed,
generation accuracy, and quality.
</p></li>
</ul>
<h2>LLM</h2>
<h2>segmentation</h2>
<h3>Title: Exploiting CNNs for Semantic Segmentation with Pascal VOC. (arXiv:2304.13216v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13216">http://arxiv.org/abs/2304.13216</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13216] Exploiting CNNs for Semantic Segmentation with Pascal VOC](http://arxiv.org/abs/2304.13216) #segmentation</code></li>
<li>Summary: <p>In this paper, we present a comprehensive study on semantic segmentation with
the Pascal VOC dataset. Here, we have to label each pixel with a class which in
turn segments the entire image based on the objects/entities present. To tackle
this, we firstly use a Fully Convolution Network (FCN) baseline which gave
71.31% pixel accuracy and 0.0527 mean IoU. We analyze its performance and
working and subsequently address the issues in the baseline with three
improvements: a) cosine annealing learning rate scheduler(pixel accuracy:
72.86%, IoU: 0.0529), b) data augmentation(pixel accuracy: 69.88%, IoU: 0.0585)
c) class imbalance weights(pixel accuracy: 68.98%, IoU: 0.0596). Apart from
these changes in training pipeline, we also explore three different
architectures: a) Our proposed model -- Advanced FCN (pixel accuracy: 67.20%,
IoU: 0.0602) b) Transfer Learning with ResNet (Best performance) (pixel
accuracy: 71.33%, IoU: 0.0926 ) c) U-Net(pixel accuracy: 72.15%, IoU: 0.0649).
We observe that the improvements help in greatly improving the performance, as
reflected both, in metrics and segmentation maps. Interestingly, we observe
that among the improvements, dataset augmentation has the greatest
contribution. Also, note that transfer learning model performs the best on the
pascal dataset. We analyse the performance of these using loss, accuracy and
IoU plots along with segmentation maps, which help us draw valuable insights
about the working of the models.
</p></li>
</ul>
<h3>Title: Compensation Learning in Semantic Segmentation. (arXiv:2304.13428v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13428">http://arxiv.org/abs/2304.13428</a></li>
<li>Code URL: <a href="https://github.com/tnt-LUH/compensation_learning">https://github.com/tnt-LUH/compensation_learning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13428] Compensation Learning in Semantic Segmentation](http://arxiv.org/abs/2304.13428) #segmentation</code></li>
<li>Summary: <p>Label noise and ambiguities between similar classes are challenging problems
in developing new models and annotating new data for semantic segmentation. In
this paper, we propose Compensation Learning in Semantic Segmentation, a
framework to identify and compensate ambiguities as well as label noise. More
specifically, we add a ground truth depending and globally learned bias to the
classification logits and introduce a novel uncertainty branch for neural
networks to induce the compensation bias only to relevant regions. Our method
is employed into state-of-the-art segmentation frameworks and several
experiments demonstrate that our proposed compensation learns inter-class
relations that allow global identification of challenging ambiguities as well
as the exact localization of subsequent label noise. Additionally, it enlarges
robustness against label noise during training and allows target-oriented
manipulation during inference. We evaluate the proposed method on %the widely
used datasets Cityscapes, KITTI-STEP, ADE20k, and COCO-stuff10k.
</p></li>
</ul>
<h3>Title: Effect of latent space distribution on the segmentation of images with multiple annotations. (arXiv:2304.13476v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13476">http://arxiv.org/abs/2304.13476</a></li>
<li>Code URL: <a href="https://github.com/ishaanb92/generalizedprobabilisticunet">https://github.com/ishaanb92/generalizedprobabilisticunet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13476] Effect of latent space distribution on the segmentation of images with multiple annotations](http://arxiv.org/abs/2304.13476) #segmentation</code></li>
<li>Summary: <p>We propose the Generalized Probabilistic U-Net, which extends the
Probabilistic U-Net by allowing more general forms of the Gaussian distribution
as the latent space distribution that can better approximate the uncertainty in
the reference segmentations. We study the effect the choice of latent space
distribution has on capturing the variation in the reference segmentations for
lung tumors and white matter hyperintensities in the brain. We show that the
choice of distribution affects the sample diversity of the predictions and
their overlap with respect to the reference segmentations. We have made our
implementation available at
https://github.com/ishaanb92/GeneralizedProbabilisticUNet
</p></li>
</ul>
<h3>Title: EasyPortrait - Face Parsing and Portrait Segmentation Dataset. (arXiv:2304.13509v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13509">http://arxiv.org/abs/2304.13509</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13509] EasyPortrait - Face Parsing and Portrait Segmentation Dataset](http://arxiv.org/abs/2304.13509) #segmentation</code></li>
<li>Summary: <p>Recently, due to COVID-19 and the growing demand for remote work, video
conferencing apps have become especially widespread. The most valuable features
of video chats are real-time background removal and face beautification. While
solving these tasks, computer vision researchers face the problem of having
relevant data for the training stage. There is no large dataset with
high-quality labeled and diverse images of people in front of a laptop or
smartphone camera to train a lightweight model without additional approaches.
To boost the progress in this area, we provide a new image dataset,
EasyPortrait, for portrait segmentation and face parsing tasks. It contains
20,000 primarily indoor photos of 8,377 unique users, and fine-grained
segmentation masks separated into 9 classes. Images are collected and labeled
from crowdsourcing platforms. Unlike most face parsing datasets, in
EasyPortrait, the beard is not considered part of the skin mask, and the inside
area of the mouth is separated from the teeth. These features allow using
EasyPortrait for skin enhancement and teeth whitening tasks. This paper
describes the pipeline for creating a large-scale and clean image segmentation
dataset using crowdsourcing platforms without additional synthetic data.
Moreover, we trained several models on EasyPortrait and showed experimental
results. Proposed dataset and trained models are publicly available.
</p></li>
</ul>
<h3>Title: Cluster Entropy: Active Domain Adaptation in Pathological Image Segmentation. (arXiv:2304.13513v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13513">http://arxiv.org/abs/2304.13513</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13513] Cluster Entropy: Active Domain Adaptation in Pathological Image Segmentation](http://arxiv.org/abs/2304.13513) #segmentation</code></li>
<li>Summary: <p>The domain shift in pathological segmentation is an important problem, where
a network trained by a source domain (collected at a specific hospital) does
not work well in the target domain (from different hospitals) due to the
different image features. Due to the problems of class imbalance and different
class prior of pathology, typical unsupervised domain adaptation methods do not
work well by aligning the distribution of source domain and target domain. In
this paper, we propose a cluster entropy for selecting an effective whole slide
image (WSI) that is used for semi-supervised domain adaptation. This approach
can measure how the image features of the WSI cover the entire distribution of
the target domain by calculating the entropy of each cluster and can
significantly improve the performance of domain adaptation. Our approach
achieved competitive results against the prior arts on datasets collected from
two hospitals.
</p></li>
</ul>
<h3>Title: Domain Adaptive and Generalizable Network Architectures and Training Strategies for Semantic Image Segmentation. (arXiv:2304.13615v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13615">http://arxiv.org/abs/2304.13615</a></li>
<li>Code URL: <a href="https://github.com/lhoyer/hrda">https://github.com/lhoyer/hrda</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13615] Domain Adaptive and Generalizable Network Architectures and Training Strategies for Semantic Image Segmentation](http://arxiv.org/abs/2304.13615) #segmentation</code></li>
<li>Summary: <p>Unsupervised domain adaptation (UDA) and domain generalization (DG) enable
machine learning models trained on a source domain to perform well on unlabeled
or even unseen target domains. As previous UDA&amp;DG semantic segmentation methods
are mostly based on outdated networks, we benchmark more recent architectures,
reveal the potential of Transformers, and design the DAFormer network tailored
for UDA&amp;DG. It is enabled by three training strategies to avoid overfitting to
the source domain: While (1) Rare Class Sampling mitigates the bias toward
common source domain classes, (2) a Thing-Class ImageNet Feature Distance and
(3) a learning rate warmup promote feature transfer from ImageNet pretraining.
As UDA&amp;DG are usually GPU memory intensive, most previous methods downscale or
crop images. However, low-resolution predictions often fail to preserve fine
details while models trained with cropped images fall short in capturing
long-range, domain-robust context information. Therefore, we propose HRDA, a
multi-resolution framework for UDA&amp;DG, that combines the strengths of small
high-resolution crops to preserve fine segmentation details and large
low-resolution crops to capture long-range context dependencies with a learned
scale attention. DAFormer and HRDA significantly improve the state-of-the-art
UDA&amp;DG by more than 10 mIoU on 5 different benchmarks. The implementation is
available at https://github.com/lhoyer/HRDA.
</p></li>
</ul>
<h3>Title: FVP: Fourier Visual Prompting for Source-Free Unsupervised Domain Adaptation of Medical Image Segmentation. (arXiv:2304.13672v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13672">http://arxiv.org/abs/2304.13672</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13672] FVP: Fourier Visual Prompting for Source-Free Unsupervised Domain Adaptation of Medical Image Segmentation](http://arxiv.org/abs/2304.13672) #segmentation</code></li>
<li>Summary: <p>Medical image segmentation methods normally perform poorly when there is a
domain shift between training and testing data. Unsupervised Domain Adaptation
(UDA) addresses the domain shift problem by training the model using both
labeled data from the source domain and unlabeled data from the target domain.
Source-Free UDA (SFUDA) was recently proposed for UDA without requiring the
source data during the adaptation, due to data privacy or data transmission
issues, which normally adapts the pre-trained deep model in the testing stage.
However, in real clinical scenarios of medical image segmentation, the trained
model is normally frozen in the testing stage. In this paper, we propose
Fourier Visual Prompting (FVP) for SFUDA of medical image segmentation.
Inspired by prompting learning in natural language processing, FVP steers the
frozen pre-trained model to perform well in the target domain by adding a
visual prompt to the input target data. In FVP, the visual prompt is
parameterized using only a small amount of low-frequency learnable parameters
in the input frequency space, and is learned by minimizing the segmentation
loss between the predicted segmentation of the prompted target image and
reliable pseudo segmentation label of the target image under the frozen model.
To our knowledge, FVP is the first work to apply visual prompts to SFUDA for
medical image segmentation. The proposed FVP is validated using three public
datasets, and experiments demonstrate that FVP yields better segmentation
results, compared with various existing methods.
</p></li>
</ul>
<h2>object detection</h2>
<h3>Title: Group Equivariant BEV for 3D Object Detection. (arXiv:2304.13390v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13390">http://arxiv.org/abs/2304.13390</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13390] Group Equivariant BEV for 3D Object Detection](http://arxiv.org/abs/2304.13390) #object detection</code></li>
<li>Summary: <p>Recently, 3D object detection has attracted significant attention and
achieved continuous improvement in real road scenarios. The environmental
information is collected from a single sensor or multi-sensor fusion to detect
interested objects. However, most of the current 3D object detection approaches
focus on developing advanced network architectures to improve the detection
precision of the object rather than considering the dynamic driving scenes,
where data collected from sensors equipped in the vehicle contain various
perturbation features. As a result, existing work cannot still tackle the
perturbation issue. In order to solve this problem, we propose a group
equivariant bird's eye view network (GeqBevNet) based on the group equivariant
theory, which introduces the concept of group equivariant into the BEV fusion
object detection network. The group equivariant network is embedded into the
fused BEV feature map to facilitate the BEV-level rotational equivariant
feature extraction, thus leading to lower average orientation error. In order
to demonstrate the effectiveness of the GeqBevNet, the network is verified on
the nuScenes validation dataset in which mAOE can be decreased to 0.325.
Experimental results demonstrate that GeqBevNet can extract more rotational
equivariant features in the 3D object detection of the actual road scene and
improve the performance of object orientation prediction.
</p></li>
</ul>
<h3>Title: From Chaos Comes Order: Ordering Event Representations for Object Detection. (arXiv:2304.13455v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13455">http://arxiv.org/abs/2304.13455</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13455] From Chaos Comes Order: Ordering Event Representations for Object Detection](http://arxiv.org/abs/2304.13455) #object detection</code></li>
<li>Summary: <p>Today, state-of-the-art deep neural networks that process events first
convert them into dense, grid-like input representations before using an
off-the-shelf network. However, selecting the appropriate representation for
the task traditionally requires training a neural network for each
representation and selecting the best one based on the validation score, which
is very time-consuming. In this work, we eliminate this bottleneck by selecting
the best representation based on the Gromov-Wasserstein Discrepancy (GWD)
between the raw events and their representation. It is approximately 200 times
faster to compute than training a neural network and preserves the task
performance ranking of event representations across multiple representations,
network backbones, and datasets. This means that finding a representation with
a high task score is equivalent to finding a representation with a low GWD. We
use this insight to, for the first time, perform a hyperparameter search on a
large family of event representations, revealing new and powerful
representations that exceed the state-of-the-art. On object detection, our
optimized representation outperforms existing representations by 1.9% mAP on
the 1 Mpx dataset and 8.6% mAP on the Gen1 dataset and even outperforms the
state-of-the-art by 1.8% mAP on Gen1 and state-of-the-art feed-forward methods
by 6.0% mAP on the 1 Mpx dataset. This work opens a new unexplored field of
explicit representation optimization for event-based learning methods.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-04-27]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
