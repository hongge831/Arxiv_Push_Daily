## data-free
## transformer
### Title: Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction. (arXiv:2304.06819v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06819](http://arxiv.org/abs/2304.06819)
* Code URL: [https://github.com/ajv012/survpath](https://github.com/ajv012/survpath)
* Copy Paste: `<input type="checkbox">[[2304.06819] Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction](http://arxiv.org/abs/2304.06819) #transformer`
* Summary: <p>Integrating whole-slide images (WSIs) and bulk transcriptomics for predicting
patient survival can improve our understanding of patient prognosis. However,
this multimodal task is particularly challenging due to the different nature of
these data: WSIs represent a very high-dimensional spatial description of a
tumor, while bulk transcriptomics represent a global description of gene
expression levels within that tumor. In this context, our work aims to address
two key challenges: (1) how can we tokenize transcriptomics in a semantically
meaningful and interpretable way?, and (2) how can we capture dense multimodal
interactions between these two modalities? Specifically, we propose to learn
biological pathway tokens from transcriptomics that can encode specific
cellular functions. Together with histology patch tokens that encode the
different morphological patterns in the WSI, we argue that they form
appropriate reasoning units for downstream interpretability analyses. We
propose fusing both modalities using a memory-efficient multimodal Transformer
that can model interactions between pathway and histology patch tokens. Our
proposed model, SURVPATH, achieves state-of-the-art performance when evaluated
against both unimodal and multimodal baselines on five datasets from The Cancer
Genome Atlas. Our interpretability framework identifies key multimodal
prognostic factors, and, as such, can provide valuable insights into the
interaction between genotype and phenotype, enabling a deeper understanding of
the underlying biological mechanisms at play. We make our code public at:
https://github.com/ajv012/SurvPath.
</p>

### Title: Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding. (arXiv:2304.06906v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06906](http://arxiv.org/abs/2304.06906)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06906] Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding](http://arxiv.org/abs/2304.06906) #transformer`
* Summary: <p>Pretrained backbones with fine-tuning have been widely adopted in 2D vision
and natural language processing tasks and demonstrated significant advantages
to task-specific networks. In this paper, we present a pretrained 3D backbone,
named {\SST}, which first outperforms all state-of-the-art methods in
downstream 3D indoor scene understanding tasks. Our backbone network is based
on a 3D Swin transformer and carefully designed to efficiently conduct
self-attention on sparse voxels with linear memory complexity and capture the
irregularity of point signals via generalized contextual relative positional
embedding. Based on this backbone design, we pretrained a large {\SST} model on
a synthetic Structed3D dataset that is 10 times larger than the ScanNet dataset
and fine-tuned the pretrained model in various downstream real-world indoor
scene understanding tasks. The results demonstrate that our model pretrained on
the synthetic dataset not only exhibits good generality in both downstream
segmentation and detection on real 3D point datasets, but also surpasses the
state-of-the-art methods on downstream tasks after fine-tuning with +2.3 mIoU
and +2.2 mIoU on S3DIS Area5 and 6-fold semantic segmentation, +2.1 mIoU on
ScanNet segmentation (val), +1.9 mAP@0.5 on ScanNet detection, +8.1 mAP@0.5 on
S3DIS detection. Our method demonstrates the great potential of pretrained 3D
backbones with fine-tuning for 3D understanding tasks. The code and models are
available at https://github.com/microsoft/Swin3D .
</p>

### Title: A Unified HDR Imaging Method with Pixel and Patch Level. (arXiv:2304.06943v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06943](http://arxiv.org/abs/2304.06943)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06943] A Unified HDR Imaging Method with Pixel and Patch Level](http://arxiv.org/abs/2304.06943) #transformer`
* Summary: <p>Mapping Low Dynamic Range (LDR) images with different exposures to High
Dynamic Range (HDR) remains nontrivial and challenging on dynamic scenes due to
ghosting caused by object motion or camera jitting. With the success of Deep
Neural Networks (DNNs), several DNNs-based methods have been proposed to
alleviate ghosting, they cannot generate approving results when motion and
saturation occur. To generate visually pleasing HDR images in various cases, we
propose a hybrid HDR deghosting network, called HyHDRNet, to learn the
complicated relationship between reference and non-reference images. The
proposed HyHDRNet consists of a content alignment subnetwork and a
Transformer-based fusion subnetwork. Specifically, to effectively avoid
ghosting from the source, the content alignment subnetwork uses patch
aggregation and ghost attention to integrate similar content from other
non-reference images with patch level and suppress undesired components with
pixel level. To achieve mutual guidance between patch-level and pixel-level, we
leverage a gating module to sufficiently swap useful information both in
ghosted and saturated regions. Furthermore, to obtain a high-quality HDR image,
the Transformer-based fusion subnetwork uses a Residual Deformable Transformer
Block (RDTB) to adaptively merge information for different exposed regions. We
examined the proposed method on four widely used public HDR image deghosting
datasets. Experiments demonstrate that HyHDRNet outperforms state-of-the-art
methods both quantitatively and qualitatively, achieving appealing HDR
visualization with unified textures and colors.
</p>

### Title: Preserving Locality in Vision Transformers for Class Incremental Learning. (arXiv:2304.06971v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.06971](http://arxiv.org/abs/2304.06971)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06971] Preserving Locality in Vision Transformers for Class Incremental Learning](http://arxiv.org/abs/2304.06971) #transformer`
* Summary: <p>Learning new classes without forgetting is crucial for real-world
applications for a classification model. Vision Transformers (ViT) recently
achieve remarkable performance in Class Incremental Learning (CIL). Previous
works mainly focus on block design and model expansion for ViTs. However, in
this paper, we find that when the ViT is incrementally trained, the attention
layers gradually lose concentration on local features. We call this interesting
phenomenon as \emph{Locality Degradation} in ViTs for CIL. Since the low-level
local information is crucial to the transferability of the representation, it
is beneficial to preserve the locality in attention layers. In this paper, we
encourage the model to preserve more local information as the training
procedure goes on and devise a Locality-Preserved Attention (LPA) layer to
emphasize the importance of local features. Specifically, we incorporate the
local information directly into the vanilla attention and control the initial
gradients of the vanilla attention by weighting it with a small initial value.
Extensive experiments show that the representations facilitated by LPA capture
more low-level general information which is easier to transfer to follow-up
tasks. The improved model gets consistently better performance on CIFAR100 and
ImageNet100.
</p>

### Title: DeePoint: Pointing Recognition and Direction Estimation From A Fixed View. (arXiv:2304.06977v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06977](http://arxiv.org/abs/2304.06977)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06977] DeePoint: Pointing Recognition and Direction Estimation From A Fixed View](http://arxiv.org/abs/2304.06977) #transformer`
* Summary: <p>In this paper, we realize automatic visual recognition and direction
estimation of pointing. We introduce the first neural pointing understanding
method based on two key contributions. The first is the introduction of a
first-of-its-kind large-scale dataset for pointing recognition and direction
estimation, which we refer to as the DP Dataset. DP Dataset consists of more
than 2 million frames of over 33 people pointing in various styles annotated
for each frame with pointing timings and 3D directions. The second is DeePoint,
a novel deep network model for joint recognition and 3D direction estimation of
pointing. DeePoint is a Transformer-based network which fully leverages the
spatio-temporal coordination of the body parts, not just the hands. Through
extensive experiments, we demonstrate the accuracy and efficiency of DeePoint.
We believe DP Dataset and DeePoint will serve as a sound foundation for visual
human intention understanding.
</p>

### Title: CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction. (arXiv:2304.07072v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07072](http://arxiv.org/abs/2304.07072)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07072] CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction](http://arxiv.org/abs/2304.07072) #transformer`
* Summary: <p>Structured reconstruction is a non-trivial dense prediction problem, which
extracts structural information (\eg, building corners and edges) from a raster
image, then reconstructs it to a 2D planar graph accordingly. Compared with
common segmentation or detection problems, it significantly relays on the
capability that leveraging holistic geometric information for structural
reasoning. Current transformer-based approaches tackle this challenging problem
in a two-stage manner, which detect corners in the first model and classify the
proposed edges (corner-pairs) in the second model. However, they separate
two-stage into different models and only share the backbone encoder. Unlike the
existing modeling strategies, we present an enhanced corner representation
method: 1) It fuses knowledge between the corner detection and edge prediction
by sharing feature in different granularity; 2) Corner candidates are proposed
in four heatmap channels w.r.t its direction. Both qualitative and quantitative
evaluations demonstrate that our proposed method can better reconstruct
fine-grained structures, such as adjacent corners and tiny edges. Consequently,
it outperforms the state-of-the-art model by +1.9\%@F-1 on Corner and
+3.0\%@F-1 on Edge.
</p>

### Title: Sub-meter resolution canopy height maps using self-supervised learning and a vision transformer trained on Aerial and GEDI Lidar. (arXiv:2304.07213v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07213](http://arxiv.org/abs/2304.07213)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07213] Sub-meter resolution canopy height maps using self-supervised learning and a vision transformer trained on Aerial and GEDI Lidar](http://arxiv.org/abs/2304.07213) #transformer`
* Summary: <p>Vegetation structure mapping is critical for understanding the global carbon
cycle and monitoring nature-based approaches to climate adaptation and
mitigation. Repeat measurements of these data allow for the observation of
deforestation or degradation of existing forests, natural forest regeneration,
and the implementation of sustainable agricultural practices like agroforestry.
Assessments of tree canopy height and crown projected area at a high spatial
resolution are also important for monitoring carbon fluxes and assessing
tree-based land uses, since forest structures can be highly spatially
heterogeneous, especially in agroforestry systems. Very high resolution
satellite imagery (less than one meter (1m) ground sample distance) makes it
possible to extract information at the tree level while allowing monitoring at
a very large scale. This paper presents the first high-resolution canopy height
map concurrently produced for multiple sub-national jurisdictions.
Specifically, we produce canopy height maps for the states of California and
S\~{a}o Paolo, at sub-meter resolution, a significant improvement over the ten
meter (10m) resolution of previous Sentinel / GEDI based worldwide maps of
canopy height. The maps are generated by applying a vision transformer to
features extracted from a self-supervised model in Maxar imagery from 2017 to
2020, and are trained against aerial lidar and GEDI observations. We evaluate
the proposed maps with set-aside validation lidar data as well as by comparing
with other remotely sensed maps and field-collected data, and find our model
produces an average Mean Absolute Error (MAE) within set-aside validation areas
of 3.0 meters.
</p>

### Title: PARFormer: Transformer-based Multi-Task Network for Pedestrian Attribute Recognition. (arXiv:2304.07230v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07230](http://arxiv.org/abs/2304.07230)
* Code URL: [https://github.com/xwf199/parformer](https://github.com/xwf199/parformer)
* Copy Paste: `<input type="checkbox">[[2304.07230] PARFormer: Transformer-based Multi-Task Network for Pedestrian Attribute Recognition](http://arxiv.org/abs/2304.07230) #transformer`
* Summary: <p>Pedestrian attribute recognition (PAR) has received increasing attention
because of its wide application in video surveillance and pedestrian analysis.
Extracting robust feature representation is one of the key challenges in this
task. The existing methods mainly use the convolutional neural network (CNN) as
the backbone network to extract features. However, these methods mainly focus
on small discriminative regions while ignoring the global perspective. To
overcome these limitations, we propose a pure transformer-based multi-task PAR
network named PARFormer, which includes four modules. In the feature extraction
module, we build a transformer-based strong baseline for feature extraction,
which achieves competitive results on several PAR benchmarks compared with the
existing CNN-based baseline methods. In the feature processing module, we
propose an effective data augmentation strategy named batch random mask (BRM)
block to reinforce the attentive feature learning of random patches.
Furthermore, we propose a multi-attribute center loss (MACL) to enhance the
inter-attribute discriminability in the feature representations. In the
viewpoint perception module, we explore the impact of viewpoints on pedestrian
attributes, and propose a multi-view contrastive loss (MCVL) that enables the
network to exploit the viewpoint information. In the attribute recognition
module, we alleviate the negative-positive imbalance problem to generate the
attribute predictions. The above modules interact and jointly learn a highly
discriminative feature space, and supervise the generation of the final
features. Extensive experimental results show that the proposed PARFormer
network performs well compared to the state-of-the-art methods on several
public datasets, including PETA, RAP, and PA100K. Code will be released at
https://github.com/xwf199/PARFormer.
</p>

### Title: Dynamic Mobile-Former: Strengthening Dynamic Convolution with Attention and Residual Connection in Kernel Space. (arXiv:2304.07254v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07254](http://arxiv.org/abs/2304.07254)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07254] Dynamic Mobile-Former: Strengthening Dynamic Convolution with Attention and Residual Connection in Kernel Space](http://arxiv.org/abs/2304.07254) #transformer`
* Summary: <p>We introduce Dynamic Mobile-Former(DMF), maximizes the capabilities of
dynamic convolution by harmonizing it with efficient operators.Our Dynamic
MobileFormer effectively utilizes the advantages of Dynamic MobileNet
(MobileNet equipped with dynamic convolution) using global information from
light-weight attention.A Transformer in Dynamic Mobile-Former only requires a
few randomly initialized tokens to calculate global features, making it
computationally efficient.And a bridge between Dynamic MobileNet and
Transformer allows for bidirectional integration of local and global
features.We also simplify the optimization process of vanilla dynamic
convolution by splitting the convolution kernel into an input-agnostic kernel
and an input-dependent kernel.This allows for optimization in a wider kernel
space, resulting in enhanced capacity.By integrating lightweight attention and
enhanced dynamic convolution, our Dynamic Mobile-Former achieves not only high
efficiency, but also strong performance.We benchmark the Dynamic Mobile-Former
on a series of vision tasks, and showcase that it achieves impressive
performance on image classification, COCO detection, and instanace
segmentation.For example, our DMF hits the top-1 accuracy of 79.4% on
ImageNet-1K, much higher than PVT-Tiny by 4.3% with only 1/4
FLOPs.Additionally,our proposed DMF-S model performed well on challenging
vision datasets such as COCO, achieving a 39.0% mAP,which is 1% higher than
that of the Mobile-Former 508M model, despite using 3 GFLOPs less
computations.Code and models are available at https://github.com/ysj9909/DMF
</p>

### Title: SimpLex: a lexical text simplification architecture. (arXiv:2304.07002v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.07002](http://arxiv.org/abs/2304.07002)
* Code URL: [https://github.com/elena-apostol/simplex](https://github.com/elena-apostol/simplex)
* Copy Paste: `<input type="checkbox">[[2304.07002] SimpLex: a lexical text simplification architecture](http://arxiv.org/abs/2304.07002) #transformer`
* Summary: <p>Text simplification (TS) is the process of generating easy-to-understand
sentences from a given sentence or piece of text. The aim of TS is to reduce
both the lexical (which refers to vocabulary complexity and meaning) and
syntactic (which refers to the sentence structure) complexity of a given text
or sentence without the loss of meaning or nuance. In this paper, we present
\textsc{SimpLex}, a novel simplification architecture for generating simplified
English sentences. To generate a simplified sentence, the proposed architecture
uses either word embeddings (i.e., Word2Vec) and perplexity, or sentence
transformers (i.e., BERT, RoBERTa, and GPT2) and cosine similarity. The
solution is incorporated into a user-friendly and simple-to-use software. We
evaluate our system using two metrics, i.e., SARI, and Perplexity Decrease.
Experimentally, we observe that the transformer models outperform the other
models in terms of the SARI score. However, in terms of Perplexity, the
Word-Embeddings-based models achieve the biggest decrease. Thus, the main
contributions of this paper are: (1) We propose a new Word Embedding and
Transformer based algorithm for text simplification; (2) We design
\textsc{SimpLex} -- a modular novel text simplification system -- that can
provide a baseline for further research; and (3) We perform an in-depth
analysis of our solution and compare our results with two state-of-the-art
models, i.e., LightLS [19] and NTS-w2v [44]. We also make the code publicly
available online.
</p>

## generative
### Title: RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.06767](http://arxiv.org/abs/2304.06767)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06767] RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment](http://arxiv.org/abs/2304.06767) #generative`
* Summary: <p>Generative foundation models are susceptible to implicit biases that can
arise from extensive unsupervised training data. Such biases can produce
suboptimal samples, skewed outcomes, and unfairness, with potentially
significant repercussions. Consequently, aligning these models with human
ethics and preferences is an essential step toward ensuring their responsible
and effective deployment in real-world applications. Prior research has
primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means
of addressing this problem, wherein generative models are fine-tuned using RL
algorithms guided by a human-feedback-informed reward model. However, the
inefficiencies and instabilities associated with RL algorithms frequently
present substantial obstacles to the successful alignment of generative models,
necessitating the development of a more robust and streamlined approach. To
this end, we introduce a new framework, Reward rAnked FineTuning (RAFT),
designed to align generative models more effectively. Utilizing a reward model
and a sufficient number of samples, our approach selects the high-quality
samples, discarding those that exhibit undesired behavior, and subsequently
assembles a streaming dataset. This dataset serves as the basis for aligning
the generative model and can be employed under both offline and online
settings. Notably, the sample generation process within RAFT is gradient-free,
rendering it compatible with black-box generators. Through extensive
experiments, we demonstrate that our proposed algorithm exhibits strong
performance in the context of both large language models and diffusion models.
</p>

### Title: Inpaint Anything: Segment Anything Meets Image Inpainting. (arXiv:2304.06790v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06790](http://arxiv.org/abs/2304.06790)
* Code URL: [https://github.com/geekyutao/inpaint-anything](https://github.com/geekyutao/inpaint-anything)
* Copy Paste: `<input type="checkbox">[[2304.06790] Inpaint Anything: Segment Anything Meets Image Inpainting](http://arxiv.org/abs/2304.06790) #generative`
* Summary: <p>Modern image inpainting systems, despite the significant progress, often
struggle with mask selection and holes filling. Based on Segment-Anything Model
(SAM), we make the first attempt to the mask-free image inpainting and propose
a new paradigm of ``clicking and filling'', which is named as Inpaint Anything
(IA). The core idea behind IA is to combine the strengths of different models
in order to build a very powerful and user-friendly pipeline for solving
inpainting-related problems. IA supports three main features: (i) Remove
Anything: users could click on an object and IA will remove it and smooth the
``hole'' with the context; (ii) Fill Anything: after certain objects removal,
users could provide text-based prompts to IA, and then it will fill the hole
with the corresponding generative content via driving AIGC models like Stable
Diffusion; (iii) Replace Anything: with IA, users have another option to retain
the click-selected object and replace the remaining background with the newly
generated scenes. We are also very willing to help everyone share and promote
new projects based on our Inpaint Anything (IA). Our codes are available at
https://github.com/geekyutao/Inpaint-Anything.
</p>

### Title: Delta Denoising Score. (arXiv:2304.07090v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07090](http://arxiv.org/abs/2304.07090)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07090] Delta Denoising Score](http://arxiv.org/abs/2304.07090) #generative`
* Summary: <p>We introduce Delta Denoising Score (DDS), a novel scoring function for
text-based image editing that guides minimal modifications of an input image
towards the content described in a target prompt. DDS leverages the rich
generative prior of text-to-image diffusion models and can be used as a loss
term in an optimization problem to steer an image towards a desired direction
dictated by a text. DDS utilizes the Score Distillation Sampling (SDS)
mechanism for the purpose of image editing. We show that using only SDS often
produces non-detailed and blurry outputs due to noisy gradients. To address
this issue, DDS uses a prompt that matches the input image to identify and
remove undesired erroneous directions of SDS. Our key premise is that SDS
should be zero when calculated on pairs of matched prompts and images, meaning
that if the score is non-zero, its gradients can be attributed to the erroneous
component of SDS. Our analysis demonstrates the competence of DDS for text
based image-to-image translation. We further show that DDS can be used to train
an effective zero-shot image translation model. Experimental results indicate
that DDS outperforms existing methods in terms of stability and quality,
highlighting its potential for real-world applications in text-based image
editing.
</p>

### Title: A Comparative Study on Generative Models for High Resolution Solar Observation Imaging. (arXiv:2304.07169v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07169](http://arxiv.org/abs/2304.07169)
* Code URL: [https://github.com/slampai/generative-models-for-highres-solar-images](https://github.com/slampai/generative-models-for-highres-solar-images)
* Copy Paste: `<input type="checkbox">[[2304.07169] A Comparative Study on Generative Models for High Resolution Solar Observation Imaging](http://arxiv.org/abs/2304.07169) #generative`
* Summary: <p>Solar activity is one of the main drivers of variability in our solar system
and the key source of space weather phenomena that affect Earth and near Earth
space. The extensive record of high resolution extreme ultraviolet (EUV)
observations from the Solar Dynamics Observatory (SDO) offers an unprecedented,
very large dataset of solar images. In this work, we make use of this
comprehensive dataset to investigate capabilities of current state-of-the-art
generative models to accurately capture the data distribution behind the
observed solar activity states. Starting from StyleGAN-based methods, we
uncover severe deficits of this model family in handling fine-scale details of
solar images when training on high resolution samples, contrary to training on
natural face images. When switching to the diffusion based generative model
family, we observe strong improvements of fine-scale detail generation. For the
GAN family, we are able to achieve similar improvements in fine-scale
generation when turning to ProjectedGANs, which uses multi-scale discriminators
with a pre-trained frozen feature extractor. We conduct ablation studies to
clarify mechanisms responsible for proper fine-scale handling. Using
distributed training on supercomputers, we are able to train generative models
for up to 1024x1024 resolution that produce high quality samples
indistinguishable to human experts, as suggested by the evaluation we conduct.
We make all code, models and workflows used in this study publicly available at
\url{https://github.com/SLAMPAI/generative-models-for-highres-solar-images}.
</p>

## label correction
## noise
### Title: Obfuscation of Discrete Data. (arXiv:2304.07092v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2304.07092](http://arxiv.org/abs/2304.07092)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07092] Obfuscation of Discrete Data](http://arxiv.org/abs/2304.07092) #noise`
* Summary: <p>Data obfuscation deals with the problem of masking a data-set in such a way
that the utility of the data is maximized while minimizing the risk of the
disclosure of sensitive information. To protect data we address some ways that
may as well retain its statistical uses to some extent. One such way is to mask
a data with additive noise and revert to certain desired parameters of the
original distribution from the knowledge of the noise distribution and masked
data. In this project, we discuss the estimation of any desired quantile and
range of a quantitative data set masked with additive noise.
</p>

## diffusion
### Title: Soundini: Sound-Guided Diffusion for Natural Video Editing. (arXiv:2304.06818v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06818](http://arxiv.org/abs/2304.06818)
* Code URL: [https://github.com/kuai-lab/soundini-official](https://github.com/kuai-lab/soundini-official)
* Copy Paste: `<input type="checkbox">[[2304.06818] Soundini: Sound-Guided Diffusion for Natural Video Editing](http://arxiv.org/abs/2304.06818) #diffusion`
* Summary: <p>We propose a method for adding sound-guided visual effects to specific
regions of videos with a zero-shot setting. Animating the appearance of the
visual effect is challenging because each frame of the edited video should have
visual changes while maintaining temporal consistency. Moreover, existing video
editing solutions focus on temporal consistency across frames, ignoring the
visual style variations over time, e.g., thunderstorm, wave, fire crackling. To
overcome this limitation, we utilize temporal sound features for the dynamic
style. Specifically, we guide denoising diffusion probabilistic models with an
audio latent representation in the audio-visual latent space. To the best of
our knowledge, our work is the first to explore sound-guided natural video
editing from various sound sources with sound-specialized properties, such as
intensity, timbre, and volume. Additionally, we design optical flow-based
guidance to generate temporally consistent video frames, capturing the
pixel-wise relationship between adjacent frames. Experimental results show that
our method outperforms existing video editing techniques, producing more
realistic visual effects that reflect the properties of sound. Please visit our
page: https://kuai-lab.github.io/soundini-gallery/.
</p>

### Title: DCFace: Synthetic Face Generation with Dual Condition Diffusion Model. (arXiv:2304.07060v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07060](http://arxiv.org/abs/2304.07060)
* Code URL: [https://github.com/mk-minchul/dcface](https://github.com/mk-minchul/dcface)
* Copy Paste: `<input type="checkbox">[[2304.07060] DCFace: Synthetic Face Generation with Dual Condition Diffusion Model](http://arxiv.org/abs/2304.07060) #diffusion`
* Summary: <p>Generating synthetic datasets for training face recognition models is
challenging because dataset generation entails more than creating high fidelity
images. It involves generating multiple images of same subjects under different
factors (\textit{e.g.}, variations in pose, illumination, expression, aging and
occlusion) which follows the real image conditional distribution. Previous
works have studied the generation of synthetic datasets using GAN or 3D models.
In this work, we approach the problem from the aspect of combining subject
appearance (ID) and external factor (style) conditions. These two conditions
provide a direct way to control the inter-class and intra-class variations. To
this end, we propose a Dual Condition Face Generator (DCFace) based on a
diffusion model. Our novel Patch-wise style extractor and Time-step dependent
ID loss enables DCFace to consistently produce face images of the same subject
under different styles with precise control. Face recognition models trained on
synthetic images from the proposed DCFace provide higher verification
accuracies compared to previous works by $6.11\%$ on average in $4$ out of $5$
test datasets, LFW, CFP-FP, CPLFW, AgeDB and CALFW. Code is available at
https://github.com/mk-minchul/dcface
</p>

### Title: Memory Efficient Diffusion Probabilistic Models via Patch-based Generation. (arXiv:2304.07087v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07087](http://arxiv.org/abs/2304.07087)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07087] Memory Efficient Diffusion Probabilistic Models via Patch-based Generation](http://arxiv.org/abs/2304.07087) #diffusion`
* Summary: <p>Diffusion probabilistic models have been successful in generating
high-quality and diverse images. However, traditional models, whose input and
output are high-resolution images, suffer from excessive memory requirements,
making them less practical for edge devices. Previous approaches for generative
adversarial networks proposed a patch-based method that uses positional
encoding and global content information. Nevertheless, designing a patch-based
approach for diffusion probabilistic models is non-trivial. In this paper, we
resent a diffusion probabilistic model that generates images on a
patch-by-patch basis. We propose two conditioning methods for a patch-based
generation. First, we propose position-wise conditioning using one-hot
representation to ensure patches are in proper positions. Second, we propose
Global Content Conditioning (GCC) to ensure patches have coherent content when
concatenated together. We evaluate our model qualitatively and quantitatively
on CelebA and LSUN bedroom datasets and demonstrate a moderate trade-off
between maximum memory consumption and generated image quality. Specifically,
when an entire image is divided into 2 x 2 patches, our proposed approach can
reduce the maximum memory consumption by half while maintaining comparable
image quality.
</p>

### Title: Towards Controllable Diffusion Models via Reward-Guided Exploration. (arXiv:2304.07132v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.07132](http://arxiv.org/abs/2304.07132)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07132] Towards Controllable Diffusion Models via Reward-Guided Exploration](http://arxiv.org/abs/2304.07132) #diffusion`
* Summary: <p>By formulating data samples' formation as a Markov denoising process,
diffusion models achieve state-of-the-art performances in a collection of
tasks. Recently, many variants of diffusion models have been proposed to enable
controlled sample generation. Most of these existing methods either formulate
the controlling information as an input (i.e.,: conditional representation) for
the noise approximator, or introduce a pre-trained classifier in the test-phase
to guide the Langevin dynamic towards the conditional goal. However, the former
line of methods only work when the controlling information can be formulated as
conditional representations, while the latter requires the pre-trained guidance
classifier to be differentiable. In this paper, we propose a novel framework
named RGDM (Reward-Guided Diffusion Model) that guides the training-phase of
diffusion models via reinforcement learning (RL). The proposed training
framework bridges the objective of weighted log-likelihood and maximum entropy
RL, which enables calculating policy gradients via samples from a pay-off
distribution proportional to exponential scaled rewards, rather than from
policies themselves. Such a framework alleviates the high gradient variances
and enables diffusion models to explore for highly rewarded samples in the
reverse process. Experiments on 3D shape and molecule generation tasks show
significant improvements over existing conditional diffusion models.
</p>

## LLM
## segmentation
### Title: A contrastive method based on elevation data for remote sensing with scarce and high level semantic labels. (arXiv:2304.06857v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06857](http://arxiv.org/abs/2304.06857)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06857] A contrastive method based on elevation data for remote sensing with scarce and high level semantic labels](http://arxiv.org/abs/2304.06857) #segmentation`
* Summary: <p>This work proposes a hybrid unsupervised/supervised learning method to
pretrain models applied in earth observation downstream tasks where only a
handful of labels denoting very general semantic concepts are available. We
combine a contrastive approach to pretrain models with a pretext task to
predict spatially coarse elevation maps which are commonly available worldwide.
The intuition behind is that there is generally some correlation between the
elevation and targets in many remote sensing tasks, allowing the model to
pre-learn useful representations. We assess the performance of our approach on
a segmentation downstream task on labels gathering many possible subclasses
(pixel level classification of farmlands vs. other) and an image binary
classification task derived from the former, on a dataset on the north-east of
Colombia. On both cases we pretrain our models with 39K unlabeled images, fine
tune the downstream task only with 80 labeled images and test it with 2944
labeled images. Our experiments show that our methods, GLCNet+Elevation for
segmentation and SimCLR+Elevation for classification, outperform their
counterparts without the elevation pretext task in terms of accuracy and
macro-average F1, which supports the notion that including additional
information correlated to targets in downstream tasks can lead to improved
performance.
</p>

### Title: MVP-SEG: Multi-View Prompt Learning for Open-Vocabulary Semantic Segmentation. (arXiv:2304.06957v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06957](http://arxiv.org/abs/2304.06957)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06957] MVP-SEG: Multi-View Prompt Learning for Open-Vocabulary Semantic Segmentation](http://arxiv.org/abs/2304.06957) #segmentation`
* Summary: <p>CLIP (Contrastive Language-Image Pretraining) is well-developed for
open-vocabulary zero-shot image-level recognition, while its applications in
pixel-level tasks are less investigated, where most efforts directly adopt CLIP
features without deliberative adaptations. In this work, we first demonstrate
the necessity of image-pixel CLIP feature adaption, then provide Multi-View
Prompt learning (MVP-SEG) as an effective solution to achieve image-pixel
adaptation and to solve open-vocabulary semantic segmentation. Concretely,
MVP-SEG deliberately learns multiple prompts trained by our Orthogonal
Constraint Loss (OCLoss), by which each prompt is supervised to exploit CLIP
feature on different object parts, and collaborative segmentation masks
generated by all prompts promote better segmentation. Moreover, MVP-SEG
introduces Global Prompt Refining (GPR) to further eliminate class-wise
segmentation noise. Experiments show that the multi-view prompts learned from
seen categories have strong generalization to unseen categories, and MVP-SEG+
which combines the knowledge transfer stage significantly outperforms previous
methods on several benchmarks. Moreover, qualitative results justify that
MVP-SEG does lead to better focus on different local parts.
</p>

### Title: Self-Supervised Learning based Depth Estimation from Monocular Images. (arXiv:2304.06966v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06966](http://arxiv.org/abs/2304.06966)
* Code URL: [https://github.com/nyu-ce-projects/depth-estimation](https://github.com/nyu-ce-projects/depth-estimation)
* Copy Paste: `<input type="checkbox">[[2304.06966] Self-Supervised Learning based Depth Estimation from Monocular Images](http://arxiv.org/abs/2304.06966) #segmentation`
* Summary: <p>Depth Estimation has wide reaching applications in the field of Computer
vision such as target tracking, augmented reality, and self-driving cars. The
goal of Monocular Depth Estimation is to predict the depth map, given a 2D
monocular RGB image as input. The traditional depth estimation methods are
based on depth cues and used concepts like epipolar geometry. With the
evolution of Convolutional Neural Networks, depth estimation has undergone
tremendous strides. In this project, our aim is to explore possible extensions
to existing SoTA Deep Learning based Depth Estimation Models and to see whether
performance metrics could be further improved. In a broader sense, we are
looking at the possibility of implementing Pose Estimation, Efficient Sub-Pixel
Convolution Interpolation, Semantic Segmentation Estimation techniques to
further enhance our proposed architecture and to provide fine-grained and more
globally coherent depth map predictions. We also plan to do away with camera
intrinsic parameters during training and apply weather augmentations to further
generalize our model.
</p>

### Title: Learning Semantic-Aware Knowledge Guidance for Low-Light Image Enhancement. (arXiv:2304.07039v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07039](http://arxiv.org/abs/2304.07039)
* Code URL: [https://github.com/langmanbusi/semantic-aware-low-light-image-enhancement](https://github.com/langmanbusi/semantic-aware-low-light-image-enhancement)
* Copy Paste: `<input type="checkbox">[[2304.07039] Learning Semantic-Aware Knowledge Guidance for Low-Light Image Enhancement](http://arxiv.org/abs/2304.07039) #segmentation`
* Summary: <p>Low-light image enhancement (LLIE) investigates how to improve illumination
and produce normal-light images. The majority of existing methods improve
low-light images via a global and uniform manner, without taking into account
the semantic information of different regions. Without semantic priors, a
network may easily deviate from a region's original color. To address this
issue, we propose a novel semantic-aware knowledge-guided framework (SKF) that
can assist a low-light enhancement model in learning rich and diverse priors
encapsulated in a semantic segmentation model. We concentrate on incorporating
semantic knowledge from three key aspects: a semantic-aware embedding module
that wisely integrates semantic priors in feature representation space, a
semantic-guided color histogram loss that preserves color consistency of
various instances, and a semantic-guided adversarial loss that produces more
natural textures by semantic priors. Our SKF is appealing in acting as a
general framework in LLIE task. Extensive experiments show that models equipped
with the SKF significantly outperform the baselines on multiple datasets and
our SKF generalizes to different models and scenes well. The code is available
at Semantic-Aware-Low-Light-Image-Enhancement.
</p>

### Title: Tailored Multi-Organ Segmentation with Model Adaptation and Ensemble. (arXiv:2304.07123v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07123](http://arxiv.org/abs/2304.07123)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07123] Tailored Multi-Organ Segmentation with Model Adaptation and Ensemble](http://arxiv.org/abs/2304.07123) #segmentation`
* Summary: <p>Multi-organ segmentation, which identifies and separates different organs in
medical images, is a fundamental task in medical image analysis. Recently, the
immense success of deep learning motivated its wide adoption in multi-organ
segmentation tasks. However, due to expensive labor costs and expertise, the
availability of multi-organ annotations is usually limited and hence poses a
challenge in obtaining sufficient training data for deep learning-based
methods. In this paper, we aim to address this issue by combining off-the-shelf
single-organ segmentation models to develop a multi-organ segmentation model on
the target dataset, which helps get rid of the dependence on annotated data for
multi-organ segmentation. To this end, we propose a novel dual-stage method
that consists of a Model Adaptation stage and a Model Ensemble stage. The first
stage enhances the generalization of each off-the-shelf segmentation model on
the target domain, while the second stage distills and integrates knowledge
from multiple adapted single-organ segmentation models. Extensive experiments
on four abdomen datasets demonstrate that our proposed method can effectively
leverage off-the-shelf single-organ segmentation models to obtain a tailored
model for multi-organ segmentation with high accuracy.
</p>

### Title: TUM-FA\c{C}ADE: Reviewing and enriching point cloud benchmarks for fa\c{c}ade segmentation. (arXiv:2304.07140v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07140](http://arxiv.org/abs/2304.07140)
* Code URL: [https://github.com/oloocki/tum-facade](https://github.com/oloocki/tum-facade)
* Copy Paste: `<input type="checkbox">[[2304.07140] TUM-FA\c{C}ADE: Reviewing and enriching point cloud benchmarks for fa\c{c}ade segmentation](http://arxiv.org/abs/2304.07140) #segmentation`
* Summary: <p>Point clouds are widely regarded as one of the best dataset types for urban
mapping purposes. Hence, point cloud datasets are commonly investigated as
benchmark types for various urban interpretation methods. Yet, few researchers
have addressed the use of point cloud benchmarks for fa\c{c}ade segmentation.
Robust fa\c{c}ade segmentation is becoming a key factor in various applications
ranging from simulating autonomous driving functions to preserving cultural
heritage. In this work, we present a method of enriching existing point cloud
datasets with fa\c{c}ade-related classes that have been designed to facilitate
fa\c{c}ade segmentation testing. We propose how to efficiently extend existing
datasets and comprehensively assess their potential for fa\c{c}ade
segmentation. We use the method to create the TUM-FA\c{C}ADE dataset, which
extends the capabilities of TUM-MLS-2016. Not only can TUM-FA\c{C}ADE
facilitate the development of point-cloud-based fa\c{c}ade segmentation tasks,
but our procedure can also be applied to enrich further datasets.
</p>

### Title: CROVIA: Seeing Drone Scenes from Car Perspective via Cross-View Adaptation. (arXiv:2304.07199v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07199](http://arxiv.org/abs/2304.07199)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07199] CROVIA: Seeing Drone Scenes from Car Perspective via Cross-View Adaptation](http://arxiv.org/abs/2304.07199) #segmentation`
* Summary: <p>Understanding semantic scene segmentation of urban scenes captured from the
Unmanned Aerial Vehicles (UAV) perspective plays a vital role in building a
perception model for UAV. With the limitations of large-scale densely labeled
data, semantic scene segmentation for UAV views requires a broad understanding
of an object from both its top and side views. Adapting from well-annotated
autonomous driving data to unlabeled UAV data is challenging due to the
cross-view differences between the two data types. Our work proposes a novel
Cross-View Adaptation (CROVIA) approach to effectively adapt the knowledge
learned from on-road vehicle views to UAV views. First, a novel geometry-based
constraint to cross-view adaptation is introduced based on the geometry
correlation between views. Second, cross-view correlations from image space are
effectively transferred to segmentation space without any requirement of paired
on-road and UAV view data via a new Geometry-Constraint Cross-View (GeiCo)
loss. Third, the multi-modal bijective networks are introduced to enforce the
global structural modeling across views. Experimental results on new cross-view
adaptation benchmarks introduced in this work, i.e., SYNTHIA to UAVID and GTA5
to UAVID, show the State-of-the-Art (SOTA) performance of our approach over
prior adaptation methods
</p>

## object detection
### Title: YOLO-Drone:Airborne real-time detection of dense small objects from high-altitude perspective. (arXiv:2304.06925v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06925](http://arxiv.org/abs/2304.06925)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06925] YOLO-Drone:Airborne real-time detection of dense small objects from high-altitude perspective](http://arxiv.org/abs/2304.06925) #object detection`
* Summary: <p>Unmanned Aerial Vehicles (UAVs), specifically drones equipped with remote
sensing object detection technology, have rapidly gained a broad spectrum of
applications and emerged as one of the primary research focuses in the field of
computer vision. Although UAV remote sensing systems have the ability to detect
various objects, small-scale objects can be challenging to detect reliably due
to factors such as object size, image degradation, and real-time limitations.
To tackle these issues, a real-time object detection algorithm (YOLO-Drone) is
proposed and applied to two new UAV platforms as well as a specific light
source (silicon-based golden LED). YOLO-Drone presents several novelties: 1)
including a new backbone Darknet59; 2) a new complex feature aggregation module
MSPP-FPN that incorporated one spatial pyramid pooling and three atrous spatial
pyramid pooling modules; 3) and the use of Generalized Intersection over Union
(GIoU) as the loss function. To evaluate performance, two benchmark datasets,
UAVDT and VisDrone, along with one homemade dataset acquired at night under
silicon-based golden LEDs, are utilized. The experimental results show that, in
both UAVDT and VisDrone, the proposed YOLO-Drone outperforms state-of-the-art
(SOTA) object detection methods by improving the mAP of 10.13% and 8.59%,
respectively. With regards to UAVDT, the YOLO-Drone exhibits both high
real-time inference speed of 53 FPS and a maximum mAP of 34.04%. Notably,
YOLO-Drone achieves high performance under the silicon-based golden LEDs, with
a mAP of up to 87.71%, surpassing the performance of YOLO series under ordinary
light sources. To conclude, the proposed YOLO-Drone is a highly effective
solution for object detection in UAV applications, particularly for night
detection tasks where silicon-based golden light LED technology exhibits
significant superiority.
</p>

### Title: DETR with Additional Global Aggregation for Cross-domain Weakly Supervised Object Detection. (arXiv:2304.07082v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07082](http://arxiv.org/abs/2304.07082)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07082] DETR with Additional Global Aggregation for Cross-domain Weakly Supervised Object Detection](http://arxiv.org/abs/2304.07082) #object detection`
* Summary: <p>This paper presents a DETR-based method for cross-domain weakly supervised
object detection (CDWSOD), aiming at adapting the detector from source to
target domain through weak supervision. We think DETR has strong potential for
CDWSOD due to an insight: the encoder and the decoder in DETR are both based on
the attention mechanism and are thus capable of aggregating semantics across
the entire image. The aggregation results, i.e., image-level predictions, can
naturally exploit the weak supervision for domain alignment. Such motivated, we
propose DETR with additional Global Aggregation (DETR-GA), a CDWSOD detector
that simultaneously makes "instance-level + image-level" predictions and
utilizes "strong + weak" supervisions. The key point of DETR-GA is very simple:
for the encoder / decoder, we respectively add multiple class queries / a
foreground query to aggregate the semantics into image-level predictions. Our
query-based aggregation has two advantages. First, in the encoder, the
weakly-supervised class queries are capable of roughly locating the
corresponding positions and excluding the distraction from non-relevant
regions. Second, through our design, the object queries and the foreground
query in the decoder share consensus on the class semantics, therefore making
the strong and weak supervision mutually benefit each other for domain
alignment. Extensive experiments on four popular cross-domain benchmarks show
that DETR-GA significantly improves CSWSOD and advances the states of the art
(e.g., 29.0% --&gt; 79.4% mAP on PASCAL VOC --&gt; Clipart_all dataset).
</p>

### Title: Directly Optimizing IoU for Bounding Box Localization. (arXiv:2304.07256v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07256](http://arxiv.org/abs/2304.07256)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07256] Directly Optimizing IoU for Bounding Box Localization](http://arxiv.org/abs/2304.07256) #object detection`
* Summary: <p>Object detection has seen remarkable progress in recent years with the
introduction of Convolutional Neural Networks (CNN). Object detection is a
multi-task learning problem where both the position of the objects in the
images as well as their classes needs to be correctly identified. The idea here
is to maximize the overlap between the ground-truth bounding boxes and the
predictions i.e. the Intersection over Union (IoU). In the scope of work seen
currently in this domain, IoU is approximated by using the Huber loss as a
proxy but this indirect method does not leverage the IoU information and treats
the bounding box as four independent, unrelated terms of regression. This is
not true for a bounding box where the four coordinates are highly correlated
and hold a semantic meaning when taken together. The direct optimization of the
IoU is not possible due to its non-convex and non-differentiable nature. In
this paper, we have formulated a novel loss namely, the Smooth IoU, which
directly optimizes the IoUs for the bounding boxes. This loss has been
evaluated on the Oxford IIIT Pets, Udacity self-driving car, PASCAL VOC, and
VWFS Car Damage datasets and has shown performance gains over the standard
Huber loss.
</p>

