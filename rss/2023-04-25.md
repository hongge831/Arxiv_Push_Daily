## data-free
## transformer
### Title: eWaSR -- an embedded-compute-ready maritime obstacle detection network. (arXiv:2304.11249v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11249](http://arxiv.org/abs/2304.11249)
* Code URL: [https://github.com/tersekmatija/ewasr](https://github.com/tersekmatija/ewasr)
* Copy Paste: `<input type="checkbox">[[2304.11249] eWaSR -- an embedded-compute-ready maritime obstacle detection network](http://arxiv.org/abs/2304.11249) #transformer`
* Summary: <p>Maritime obstacle detection is critical for safe navigation of autonomous
surface vehicles (ASVs). While the accuracy of image-based detection methods
has advanced substantially, their computational and memory requirements
prohibit deployment on embedded devices. In this paper we analyze the currently
best-performing maritime obstacle detection network WaSR. Based on the analysis
we then propose replacements for the most computationally intensive stages and
propose its embedded-compute-ready variant eWaSR. In particular, the new design
follows the most recent advancements of transformer-based lightweight networks.
eWaSR achieves comparable detection results to state-of-the-art WaSR with only
0.52% F1 score performance drop and outperforms other state-of-the-art
embedded-ready architectures by over 9.74% in F1 score. On a standard GPU,
eWaSR runs 10x faster than the original WaSR (115 FPS vs 11 FPS). Tests on a
real embedded device OAK-D show that, while WaSR cannot run due to memory
restrictions, eWaSR runs comfortably at 5.5 FPS. This makes eWaSR the first
practical embedded-compute-ready maritime obstacle detection network. The
source code and trained eWaSR models are publicly available here:
https://github.com/tersekmatija/eWaSR.
</p>

### Title: Self-supervised Learning by View Synthesis. (arXiv:2304.11330v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11330](http://arxiv.org/abs/2304.11330)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11330] Self-supervised Learning by View Synthesis](http://arxiv.org/abs/2304.11330) #transformer`
* Summary: <p>We present view-synthesis autoencoders (VSA) in this paper, which is a
self-supervised learning framework designed for vision transformers. Different
from traditional 2D pretraining methods, VSA can be pre-trained with multi-view
data. In each iteration, the input to VSA is one view (or multiple views) of a
3D object and the output is a synthesized image in another target pose. The
decoder of VSA has several cross-attention blocks, which use the source view as
value, source pose as key, and target pose as query. They achieve
cross-attention to synthesize the target view. This simple approach realizes
large-angle view synthesis and learns spatial invariant representation, where
the latter is decent initialization for transformers on downstream tasks, such
as 3D classification on ModelNet40, ShapeNet Core55, and ScanObjectNN. VSA
outperforms existing methods significantly for linear probing and is
competitive for fine-tuning. The code will be made publicly available.
</p>

### Title: Two Birds, One Stone: A Unified Framework for Joint Learning of Image and Video Style Transfers. (arXiv:2304.11335v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11335](http://arxiv.org/abs/2304.11335)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11335] Two Birds, One Stone: A Unified Framework for Joint Learning of Image and Video Style Transfers](http://arxiv.org/abs/2304.11335) #transformer`
* Summary: <p>Current arbitrary style transfer models are limited to either image or video
domains. In order to achieve satisfying image and video style transfers, two
different models are inevitably required with separate training processes on
image and video domains, respectively. In this paper, we show that this can be
precluded by introducing UniST, a Unified Style Transfer framework for both
images and videos. At the core of UniST is a domain interaction transformer
(DIT), which first explores context information within the specific domain and
then interacts contextualized domain information for joint learning. In
particular, DIT enables exploration of temporal information from videos for the
image style transfer task and meanwhile allows rich appearance texture from
images for video style transfer, thus leading to mutual benefits. Considering
heavy computation of traditional multi-head self-attention, we present a simple
yet effective axial multi-head self-attention (AMSA) for DIT, which improves
computational efficiency while maintains style transfer performance. To verify
the effectiveness of UniST, we conduct extensive experiments on both image and
video style transfer tasks and show that UniST performs favorably against
state-of-the-art approaches on both tasks. Our code and results will be
released.
</p>

### Title: Incomplete Multimodal Learning for Remote Sensing Data Fusion. (arXiv:2304.11381v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11381](http://arxiv.org/abs/2304.11381)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11381] Incomplete Multimodal Learning for Remote Sensing Data Fusion](http://arxiv.org/abs/2304.11381) #transformer`
* Summary: <p>The mechanism of connecting multimodal signals through self-attention
operation is a key factor in the success of multimodal Transformer networks in
remote sensing data fusion tasks. However, traditional approaches assume access
to all modalities during both training and inference, which can lead to severe
degradation when dealing with modal-incomplete inputs in downstream
applications. To address this limitation, our proposed approach introduces a
novel model for incomplete multimodal learning in the context of remote sensing
data fusion. This approach can be used in both supervised and self-supervised
pretraining paradigms and leverages the additional learned fusion tokens in
combination with Bi-LSTM attention and masked self-attention mechanisms to
collect multimodal signals. The proposed approach employs reconstruction and
contrastive loss to facilitate fusion in pre-training while allowing for random
modality combinations as inputs in network training. Our approach delivers
state-of-the-art performance on two multimodal datasets for tasks such as
building instance / semantic segmentation and land-cover mapping tasks when
dealing with incomplete inputs during inference.
</p>

### Title: Dilated-UNet: A Fast and Accurate Medical Image Segmentation Approach using a Dilated Transformer and U-Net Architecture. (arXiv:2304.11450v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11450](http://arxiv.org/abs/2304.11450)
* Code URL: [https://github.com/Omid-Nejati/Dilated_Unet](https://github.com/Omid-Nejati/Dilated_Unet)
* Copy Paste: `<input type="checkbox">[[2304.11450] Dilated-UNet: A Fast and Accurate Medical Image Segmentation Approach using a Dilated Transformer and U-Net Architecture](http://arxiv.org/abs/2304.11450) #transformer`
* Summary: <p>Medical image segmentation is crucial for the development of computer-aided
diagnostic and therapeutic systems, but still faces numerous difficulties. In
recent years, the commonly used encoder-decoder architecture based on CNNs has
been applied effectively in medical image segmentation, but has limitations in
terms of learning global context and spatial relationships. Some researchers
have attempted to incorporate transformers into both the decoder and encoder
components, with promising results, but this approach still requires further
improvement due to its high computational complexity. This paper introduces
Dilated-UNet, which combines a Dilated Transformer block with the U-Net
architecture for accurate and fast medical image segmentation. Image patches
are transformed into tokens and fed into the U-shaped encoder-decoder
architecture, with skip-connections for local-global semantic feature learning.
The encoder uses a hierarchical Dilated Transformer with a combination of
Neighborhood Attention and Dilated Neighborhood Attention Transformer to
extract local and sparse global attention. The results of our experiments show
that Dilated-UNet outperforms other models on several challenging medical image
segmentation datasets, such as ISIC and Synapse.
</p>

### Title: Vision Transformers, a new approach for high-resolution and large-scale mapping of canopy heights. (arXiv:2304.11487v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11487](http://arxiv.org/abs/2304.11487)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11487] Vision Transformers, a new approach for high-resolution and large-scale mapping of canopy heights](http://arxiv.org/abs/2304.11487) #transformer`
* Summary: <p>Accurate and timely monitoring of forest canopy heights is critical for
assessing forest dynamics, biodiversity, carbon sequestration as well as forest
degradation and deforestation. Recent advances in deep learning techniques,
coupled with the vast amount of spaceborne remote sensing data offer an
unprecedented opportunity to map canopy height at high spatial and temporal
resolutions. Current techniques for wall-to-wall canopy height mapping
correlate remotely sensed 2D information from optical and radar sensors to the
vertical structure of trees using LiDAR measurements. While studies using deep
learning algorithms have shown promising performances for the accurate mapping
of canopy heights, they have limitations due to the type of architectures and
loss functions employed. Moreover, mapping canopy heights over tropical forests
remains poorly studied, and the accurate height estimation of tall canopies is
a challenge due to signal saturation from optical and radar sensors, persistent
cloud covers and sometimes the limited penetration capabilities of LiDARs.
Here, we map heights at 10 m resolution across the diverse landscape of Ghana
with a new vision transformer (ViT) model optimized concurrently with a
classification (discrete) and a regression (continuous) loss function. This
model achieves better accuracy than previously used convolutional based
approaches (ConvNets) optimized with only a continuous loss function. The ViT
model results show that our proposed discrete/continuous loss significantly
increases the sensitivity for very tall trees (i.e., &gt; 35m), for which other
approaches show saturation effects. The height maps generated by the ViT also
have better ground sampling distance and better sensitivity to sparse
vegetation in comparison to a convolutional model. Our ViT model has a RMSE of
3.12m in comparison to a reference dataset while the ConvNet model has a RMSE
of 4.3m.
</p>

### Title: TransFlow: Transformer as Flow Learner. (arXiv:2304.11523v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11523](http://arxiv.org/abs/2304.11523)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11523] TransFlow: Transformer as Flow Learner](http://arxiv.org/abs/2304.11523) #transformer`
* Summary: <p>Optical flow is an indispensable building block for various important
computer vision tasks, including motion estimation, object tracking, and
disparity measurement. In this work, we propose TransFlow, a pure transformer
architecture for optical flow estimation. Compared to dominant CNN-based
methods, TransFlow demonstrates three advantages. First, it provides more
accurate correlation and trustworthy matching in flow estimation by utilizing
spatial self-attention and cross-attention mechanisms between adjacent frames
to effectively capture global dependencies; Second, it recovers more
compromised information (e.g., occlusion and motion blur) in flow estimation
through long-range temporal association in dynamic scenes; Third, it enables a
concise self-learning paradigm and effectively eliminate the complex and
laborious multi-stage pre-training procedures. We achieve the state-of-the-art
results on the Sintel, KITTI-15, as well as several downstream tasks, including
video object detection, interpolation and stabilization. For its efficacy, we
hope TransFlow could serve as a flexible baseline for optical flow estimation.
</p>

### Title: Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens. (arXiv:2304.11389v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.11389](http://arxiv.org/abs/2304.11389)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11389] Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens](http://arxiv.org/abs/2304.11389) #transformer`
* Summary: <p>Recent psycholinguistic studies have drawn conflicting conclusions about the
relationship between the quality of a language model and the ability of its
surprisal estimates to predict human reading times, which has been speculated
to be due to the large gap in both the amount of training data and model
capacity across studies. The current work aims to consolidate these findings by
evaluating surprisal estimates from Transformer-based language model variants
that vary systematically in the amount of training data and model capacity on
their ability to predict human reading times. The results show that surprisal
estimates from most variants with contemporary model capacities provide the
best fit after seeing about two billion training tokens, after which they begin
to diverge from humanlike expectations. Additionally, newly-trained smaller
model variants reveal a 'tipping point' at convergence, after which the
decrease in language model perplexity begins to result in poorer fits to human
reading times. These results suggest that the massive amount of training data
is mainly responsible for the poorer fit achieved by surprisal from larger
pre-trained language models, and that a certain degree of model capacity is
necessary for Transformer-based language models to capture humanlike
expectations.
</p>

## generative
### Title: Fast GraspNeXt: A Fast Self-Attention Neural Network Architecture for Multi-task Learning in Computer Vision Tasks for Robotic Grasping on the Edge. (arXiv:2304.11196v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11196](http://arxiv.org/abs/2304.11196)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11196] Fast GraspNeXt: A Fast Self-Attention Neural Network Architecture for Multi-task Learning in Computer Vision Tasks for Robotic Grasping on the Edge](http://arxiv.org/abs/2304.11196) #generative`
* Summary: <p>Multi-task learning has shown considerable promise for improving the
performance of deep learning-driven vision systems for the purpose of robotic
grasping. However, high architectural and computational complexity can result
in poor suitability for deployment on embedded devices that are typically
leveraged in robotic arms for real-world manufacturing and warehouse
environments. As such, the design of highly efficient multi-task deep neural
network architectures tailored for computer vision tasks for robotic grasping
on the edge is highly desired for widespread adoption in manufacturing
environments. Motivated by this, we propose Fast GraspNeXt, a fast
self-attention neural network architecture tailored for embedded multi-task
learning in computer vision tasks for robotic grasping. To build Fast
GraspNeXt, we leverage a generative network architecture search strategy with a
set of architectural constraints customized to achieve a strong balance between
multi-task learning performance and embedded inference efficiency. Experimental
results on the MetaGraspNet benchmark dataset show that the Fast GraspNeXt
network design achieves the highest performance (average precision (AP),
accuracy, and mean squared error (MSE)) across multiple computer vision tasks
when compared to other efficient multi-task network architecture designs, while
having only 17.8M parameters (about &gt;5x smaller), 259 GFLOPs (as much as &gt;5x
lower) and as much as &gt;3.15x faster on a NVIDIA Jetson TX2 embedded processor.
</p>

### Title: BiTrackGAN: Cascaded CycleGANs to Constraint Face Aging. (arXiv:2304.11313v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11313](http://arxiv.org/abs/2304.11313)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11313] BiTrackGAN: Cascaded CycleGANs to Constraint Face Aging](http://arxiv.org/abs/2304.11313) #generative`
* Summary: <p>With the increased accuracy of modern computer vision technology, many access
control systems are equipped with face recognition functions for faster
identification. In order to maintain high recognition accuracy, it is necessary
to keep the face database up-to-date. However, it is impractical to collect the
latest facial picture of the system's user through human effort. Thus, we
propose a bottom-up training method for our proposed network to address this
challenge. Essentially, our proposed network is a translation pipeline that
cascades two CycleGAN blocks (a widely used unpaired image-to-image translation
generative adversarial network) called BiTrackGAN. By bottom-up training, it
induces an ideal intermediate state between these two CycleGAN blocks, namely
the constraint mechanism. Experimental results show that BiTrackGAN achieves
more reasonable and diverse cross-age facial synthesis than other
CycleGAN-related methods. As far as we know, it is a novel and effective
constraint mechanism for more reason and accurate aging synthesis through the
CycleGAN approach.
</p>

### Title: Spectral normalized dual contrastive regularization for image-to-image translation. (arXiv:2304.11319v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11319](http://arxiv.org/abs/2304.11319)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11319] Spectral normalized dual contrastive regularization for image-to-image translation](http://arxiv.org/abs/2304.11319) #generative`
* Summary: <p>Existing image-to-image(I2I) translation methods achieve state-of-the-art
performance by incorporating the patch-wise contrastive learning into
Generative Adversarial Networks. However, patch-wise contrastive learning only
focuses on the local content similarity but neglects the global structure
constraint, which affects the quality of the generated images. In this paper,
we propose a new unpaired I2I translation framework based on dual contrastive
regularization and spectral normalization, namely SN-DCR. To maintain
consistency of the global structure and texture, we design the dual contrastive
regularization using different feature spaces respectively. In order to improve
the global structure information of the generated images, we formulate a
semantically contrastive loss to make the global semantic structure of the
generated images similar to the real images from the target domain in the
semantic feature space. We use Gram Matrices to extract the style of texture
from images. Similarly, we design style contrastive loss to improve the global
texture information of the generated images. Moreover, to enhance the stability
of model, we employ the spectral normalized convolutional network in the design
of our generator. We conduct the comprehensive experiments to evaluate the
effectiveness of SN-DCR, and the results prove that our method achieves SOTA in
multiple tasks.
</p>

### Title: NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent Semantic Navigation. (arXiv:2304.11342v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11342](http://arxiv.org/abs/2304.11342)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11342] NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent Semantic Navigation](http://arxiv.org/abs/2304.11342) #generative`
* Summary: <p>3D representation disentanglement aims to identify, decompose, and manipulate
the underlying explanatory factors of 3D data, which helps AI fundamentally
understand our 3D world. This task is currently under-explored and poses great
challenges: (i) the 3D representations are complex and in general contains much
more information than 2D image; (ii) many 3D representations are not well
suited for gradient-based optimization, let alone disentanglement. To address
these challenges, we use NeRF as a differentiable 3D representation, and
introduce a self-supervised Navigation to identify interpretable semantic
directions in the latent space. To our best knowledge, this novel method,
dubbed NaviNeRF, is the first work to achieve fine-grained 3D disentanglement
without any priors or supervisions. Specifically, NaviNeRF is built upon the
generative NeRF pipeline, and equipped with an Outer Navigation Branch and an
Inner Refinement Branch. They are complementary -- the outer navigation is to
identify global-view semantic directions, and the inner refinement dedicates to
fine-grained attributes. A synergistic loss is further devised to coordinate
two branches. Extensive experiments demonstrate that NaviNeRF has a superior
fine-grained 3D disentanglement ability than the previous 3D-aware models. Its
performance is also comparable to editing-oriented models relying on semantic
or geometry priors.
</p>

### Title: Medium. Permeation: SARS-COV-2 Painting Creation by Generative Model. (arXiv:2304.11354v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11354](http://arxiv.org/abs/2304.11354)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11354] Medium](http://arxiv.org/abs/2304.11354) #generative`
* Summary: <p>Airborne particles are the medium for SARS-CoV-2 to invade the human body.
Light also reflects through suspended particles in the air, allowing people to
see a colorful world. Impressionism is the most prominent art school that
explores the spectrum of color created through color reflection of light. We
find similarities of color structure and color stacking in the Impressionist
paintings and the illustrations of the novel coronavirus by artists around the
world. With computerized data analysis through the main tones, the way of color
layout, and the way of color stacking in the paintings of the Impressionists,
we train computers to draw the novel coronavirus in an Impressionist style
using a Generative Adversarial Network to create our artwork "Medium.
Permeation". This artwork is composed of 196 randomly generated viral pictures
arranged in a 14 by 14 matrix to form a large-scale painting. In addition, we
have developed an extended work: Gradual Change, which is presented as video
art. We use Graph Neural Network to present 196 paintings of the new
coronavirus to the audience one by one in a gradual manner. In front of LED TV
screen, audience will find 196 virus paintings whose colors will change
continuously. This large video painting symbolizes that worldwide 196 countries
have been invaded by the epidemic, and every nation continuously pops up mutant
viruses. The speed of vaccine development cannot keep up with the speed of
virus mutation. This is also the first generative art in the world based on the
common features and a metaphorical symbiosis between Impressionist art and the
novel coronavirus. This work warns us of the unprecedented challenges posed by
the SARS-CoV-2, implying that the world should not ignore the invisible enemy
who uses air as a medium.
</p>

### Title: Child Face Recognition at Scale: Synthetic Data Generation and Performance Benchmark. (arXiv:2304.11685v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11685](http://arxiv.org/abs/2304.11685)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11685] Child Face Recognition at Scale: Synthetic Data Generation and Performance Benchmark](http://arxiv.org/abs/2304.11685) #generative`
* Summary: <p>We address the need for a large-scale database of children's faces by using
generative adversarial networks (GANs) and face age progression (FAP) models to
synthesize a realistic dataset referred to as HDA-SynChildFaces. To this end,
we proposed a processing pipeline that initially utilizes StyleGAN3 to sample
adult subjects, which are subsequently progressed to children of varying ages
using InterFaceGAN. Intra-subject variations, such as facial expression and
pose, are created by further manipulating the subjects in their latent space.
Additionally, the presented pipeline allows to evenly distribute the races of
subjects, allowing to generate a balanced and fair dataset with respect to race
distribution. The created HDA-SynChildFaces consists of 1,652 subjects and a
total of 188,832 images, each subject being present at various ages and with
many different intra-subject variations. Subsequently, we evaluates the
performance of various facial recognition systems on the generated database and
compare the results of adults and children at different ages. The study reveals
that children consistently perform worse than adults, on all tested systems,
and the degradation in performance is proportional to age. Additionally, our
study uncovers some biases in the recognition systems, with Asian and Black
subjects and females performing worse than White and Latino Hispanic subjects
and males.
</p>

### Title: Learn What NOT to Learn: Towards Generative Safety in Chatbots. (arXiv:2304.11220v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.11220](http://arxiv.org/abs/2304.11220)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11220] Learn What NOT to Learn: Towards Generative Safety in Chatbots](http://arxiv.org/abs/2304.11220) #generative`
* Summary: <p>Conversational models that are generative and open-domain are particularly
susceptible to generating unsafe content since they are trained on web-based
social data. Prior approaches to mitigating this issue have drawbacks, such as
disrupting the flow of conversation, limited generalization to unseen toxic
input contexts, and sacrificing the quality of the dialogue for the sake of
safety. In this paper, we present a novel framework, named "LOT" (Learn NOT
to), that employs a contrastive loss to enhance generalization by learning from
both positive and negative training signals. Our approach differs from the
standard contrastive learning framework in that it automatically obtains
positive and negative signals from the safe and unsafe language distributions
that have been learned beforehand. The LOT framework utilizes divergence to
steer the generations away from the unsafe subspace and towards the safe
subspace while sustaining the flow of conversation. Our approach is memory and
time-efficient during decoding and effectively reduces toxicity while
preserving engagingness and fluency. Empirical results indicate that LOT
reduces toxicity by up to four-fold while achieving four to six-fold higher
rates of engagingness and fluency compared to baseline models. Our findings are
further corroborated by human evaluation.
</p>

### Title: Time Series Classification for Detecting Parkinson's Disease from Wrist Motions. (arXiv:2304.11265v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.11265](http://arxiv.org/abs/2304.11265)
* Code URL: [https://github.com/cedricdonie/tsc-for-wrist-motion-pd-detection](https://github.com/cedricdonie/tsc-for-wrist-motion-pd-detection)
* Copy Paste: `<input type="checkbox">[[2304.11265] Time Series Classification for Detecting Parkinson's Disease from Wrist Motions](http://arxiv.org/abs/2304.11265) #generative`
* Summary: <p>Parkinson's disease (PD) is a neurodegenerative disease with frequently
changing motor symptoms where continuous symptom monitoring enables more
targeted treatment. Classical time series classification (TSC) and deep
learning techniques have limited performance for PD symptom monitoring using
wearable accelerometer data because PD movement patterns are complex, but
datasets are small. We investigate InceptionTime and RandOm Convolutional
KErnel Transform (ROCKET) because they are state-of-the-art for TSC and
promising for PD symptom monitoring: InceptionTime's high learning capacity is
suited to modeling complex movement patterns while ROCKET is suited to small
datasets. We used a random search to find the highest-scoring InceptionTime
architecture and compared it to ROCKET with a ridge classifier and a
multi-layer perceptron (MLP) on wrist motions of PD patients. We find that all
approaches are suitable for estimating tremor severity and bradykinesia
presence but struggle with detecting dyskinesia. ROCKET performs better for
dyskinesia, whereas InceptionTime is slightly better for tremor and
bradykinesia but has much higher variability in performance. Both outperform
the MLP. In conclusion, both InceptionTime and ROCKET are suitable for
continuous symptom monitoring, with the choice depending on the symptom of
interest and desired robustness.
</p>

### Title: Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders. (arXiv:2304.11336v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.11336](http://arxiv.org/abs/2304.11336)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11336] Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders](http://arxiv.org/abs/2304.11336) #generative`
* Summary: <p>Synthetic data has been hailed as the silver bullet for privacy preserving
data analysis. If a record is not real, then how could it violate a person's
privacy? In addition, deep-learning based generative models are employed
successfully to approximate complex high-dimensional distributions from data
and draw realistic samples from this learned distribution. It is often
overlooked though that generative models are prone to memorising many details
of individual training records and often generate synthetic data that too
closely resembles the underlying sensitive training data, hence violating
strong privacy regulations as, e.g., encountered in health care. Differential
privacy is the well-known state-of-the-art framework for guaranteeing
protection of sensitive individuals' data, allowing aggregate statistics and
even machine learning models to be released publicly without compromising
privacy. The training mechanisms however often add too much noise during the
training process, and thus severely compromise the utility of these private
models. Even worse, the tight privacy budgets do not allow for many training
epochs so that model quality cannot be properly controlled in practice. In this
paper we explore an alternative approach for privately generating data that
makes direct use of the inherent stochasticity in generative models, e.g.,
variational autoencoders. The main idea is to appropriately constrain the
continuity modulus of the deep models instead of adding another noise mechanism
on top. For this approach, we derive mathematically rigorous privacy guarantees
and illustrate its effectiveness with practical experiments.
</p>

### Title: Learning Symbolic Representations Through Joint GEnerative and DIscriminative Training. (arXiv:2304.11357v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.11357](http://arxiv.org/abs/2304.11357)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11357] Learning Symbolic Representations Through Joint GEnerative and DIscriminative Training](http://arxiv.org/abs/2304.11357) #generative`
* Summary: <p>We introduce GEDI, a Bayesian framework that combines existing
self-supervised learning objectives with likelihood-based generative models.
This framework leverages the benefits of both GEnerative and DIscriminative
approaches, resulting in improved symbolic representations over standalone
solutions. Additionally, GEDI can be easily integrated and trained jointly with
existing neuro-symbolic frameworks without the need for additional supervision
or costly pre-training steps. We demonstrate through experiments on real-world
data, including SVHN, CIFAR10, and CIFAR100, that GEDI outperforms existing
self-supervised learning strategies in terms of clustering performance by a
significant margin. The symbolic component further allows it to leverage
knowledge in the form of logical constraints to improve performance in the
small data regime.
</p>

### Title: Physics-guided generative adversarial network to learn physical models. (arXiv:2304.11488v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.11488](http://arxiv.org/abs/2304.11488)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11488] Physics-guided generative adversarial network to learn physical models](http://arxiv.org/abs/2304.11488) #generative`
* Summary: <p>This short note describes the concept of guided training of deep neural
networks (DNNs) to learn physically reasonable solutions. DNNs are being widely
used to predict phenomena in physics and mechanics. One of the issues of DNNs
is that their output does not always satisfy physical equations. One approach
to consider physical equations is adding a residual of equations into the loss
function; this is called physics-informed neural network (PINN). One feature of
PINNs is that the physical equations and corresponding residual must be
implemented as part of a neural network model. In addition, the residual does
not always converge to a small value. The proposed model is a physics-guided
generative adversarial network (PG-GAN) that uses a GAN architecture in which
physical equations are used to judge whether the neural network's output is
consistent with physics. The proposed method was applied to a simple problem to
assess its potential usability.
</p>

## label correction
## noise
### Title: Detecting Adversarial Faces Using Only Real Face Self-Perturbations. (arXiv:2304.11359v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11359](http://arxiv.org/abs/2304.11359)
* Code URL: [https://github.com/cc13qq/sapd](https://github.com/cc13qq/sapd)
* Copy Paste: `<input type="checkbox">[[2304.11359] Detecting Adversarial Faces Using Only Real Face Self-Perturbations](http://arxiv.org/abs/2304.11359) #noise`
* Summary: <p>Adversarial attacks aim to disturb the functionality of a target system by
adding specific noise to the input samples, bringing potential threats to
security and robustness when applied to facial recognition systems. Although
existing defense techniques achieve high accuracy in detecting some specific
adversarial faces (adv-faces), new attack methods especially GAN-based attacks
with completely different noise patterns circumvent them and reach a higher
attack success rate. Even worse, existing techniques require attack data before
implementing the defense, making it impractical to defend newly emerging
attacks that are unseen to defenders. In this paper, we investigate the
intrinsic generality of adv-faces and propose to generate pseudo adv-faces by
perturbing real faces with three heuristically designed noise patterns. We are
the first to train an adv-face detector using only real faces and their
self-perturbations, agnostic to victim facial recognition systems, and agnostic
to unseen attacks. By regarding adv-faces as out-of-distribution data, we then
naturally introduce a novel cascaded system for adv-face detection, which
consists of training data self-perturbations, decision boundary regularization,
and a max-pooling-based binary classifier focusing on abnormal local color
aberrations. Experiments conducted on LFW and CelebA-HQ datasets with eight
gradient-based and two GAN-based attacks validate that our method generalizes
to a variety of unseen adversarial attacks.
</p>

### Title: LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation. (arXiv:2304.11379v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11379](http://arxiv.org/abs/2304.11379)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11379] LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation](http://arxiv.org/abs/2304.11379) #noise`
* Summary: <p>Semantic map construction under bird's-eye view (BEV) plays an essential role
in autonomous driving. In contrast to camera image, LiDAR provides the accurate
3D observations to project the captured 3D features onto BEV space inherently.
However, the vanilla LiDAR-based BEV feature often contains many indefinite
noises, where the spatial features have little texture and semantic cues. In
this paper, we propose an effective LiDAR-based method to build semantic map.
Specifically, we introduce a BEV pyramid feature decoder that learns the robust
multi-scale BEV features for semantic map construction, which greatly boosts
the accuracy of the LiDAR-based method. To mitigate the defects caused by
lacking semantic cues in LiDAR data, we present an online Camera-to-LiDAR
distillation scheme to facilitate the semantic learning from image to point
cloud. Our distillation scheme consists of feature-level and logit-level
distillation to absorb the semantic information from camera in BEV. The
experimental results on challenging nuScenes dataset demonstrate the efficacy
of our proposed LiDAR2Map on semantic map construction, which significantly
outperforms the previous LiDAR-based methods over 27.9% mIoU and even performs
better than the state-of-the-art camera-based approaches. Source code is
available at: https://github.com/songw-zju/LiDAR2Map.
</p>

### Title: SSN: Stockwell Scattering Network for SAR Image Change Detection. (arXiv:2304.11404v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11404](http://arxiv.org/abs/2304.11404)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11404] SSN: Stockwell Scattering Network for SAR Image Change Detection](http://arxiv.org/abs/2304.11404) #noise`
* Summary: <p>Recently, synthetic aperture radar (SAR) image change detection has become an
interesting yet challenging direction due to the presence of speckle noise.
Although both traditional and modern learning-driven methods attempted to
overcome this challenge, deep convolutional neural networks (DCNNs)-based
methods are still hindered by the lack of interpretability and the requirement
of large computation power. To overcome this drawback, wavelet scattering
network (WSN) and Fourier scattering network (FSN) are proposed. Combining
respective merits of WSN and FSN, we propose Stockwell scattering network (SSN)
based on Stockwell transform which is widely applied against noisy signals and
shows advantageous characteristics in speckle reduction. The proposed SSN
provides noise-resilient feature representation and obtains state-of-art
performance in SAR image change detection as well as high computational
efficiency. Experimental results on three real SAR image datasets demonstrate
the effectiveness of the proposed method.
</p>

### Title: Evading DeepFake Detectors via Adversarial Statistical Consistency. (arXiv:2304.11670v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11670](http://arxiv.org/abs/2304.11670)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11670] Evading DeepFake Detectors via Adversarial Statistical Consistency](http://arxiv.org/abs/2304.11670) #noise`
* Summary: <p>In recent years, as various realistic face forgery techniques known as
DeepFake improves by leaps and bounds,more and more DeepFake detection
techniques have been proposed. These methods typically rely on detecting
statistical differences between natural (i.e., real) and DeepFakegenerated
images in both spatial and frequency domains. In this work, we propose to
explicitly minimize the statistical differences to evade state-of-the-art
DeepFake detectors. To this end, we propose a statistical consistency attack
(StatAttack) against DeepFake detectors, which contains two main parts. First,
we select several statistical-sensitive natural degradations (i.e., exposure,
blur, and noise) and add them to the fake images in an adversarial way. Second,
we find that the statistical differences between natural and DeepFake images
are positively associated with the distribution shifting between the two kinds
of images, and we propose to use a distribution-aware loss to guide the
optimization of different degradations. As a result, the feature distributions
of generated adversarial examples is close to the natural images.Furthermore,
we extend the StatAttack to a more powerful version, MStatAttack, where we
extend the single-layer degradation to multi-layer degradations sequentially
and use the loss to tune the combination weights jointly. Comprehensive
experimental results on four spatial-based detectors and two frequency-based
detectors with four datasets demonstrate the effectiveness of our proposed
attack method in both white-box and black-box settings.
</p>

### Title: DP-Adam: Correcting DP Bias in Adam's Second Moment Estimation. (arXiv:2304.11208v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.11208](http://arxiv.org/abs/2304.11208)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11208] DP-Adam: Correcting DP Bias in Adam's Second Moment Estimation](http://arxiv.org/abs/2304.11208) #noise`
* Summary: <p>We observe that the traditional use of DP with the Adam optimizer introduces
a bias in the second moment estimation, due to the addition of independent
noise in the gradient computation. This bias leads to a different scaling for
low variance parameter updates, that is inconsistent with the behavior of
non-private Adam, and Adam's sign descent interpretation. Empirically,
correcting the bias introduced by DP noise significantly improves the
optimization performance of DP-Adam.
</p>

### Title: Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.11171](http://arxiv.org/abs/2304.11171)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11171] Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method](http://arxiv.org/abs/2304.11171) #noise`
* Summary: <p>Human cognition has a ``large-scale first'' cognitive mechanism, therefore
possesses adaptive multi-granularity description capabilities. This results in
computational characteristics such as efficiency, robustness, and
interpretability. Although most existing artificial intelligence learning
methods have certain multi-granularity features, they do not fully align with
the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball
computing is an important model method developed in recent years. This method
can use granular-balls of different sizes to adaptively represent and cover the
sample space, and perform learning based on granular-balls. Since the number of
coarse-grained "granular-ball" is smaller than the number of sample points,
granular-ball computing is more efficient; the coarse-grained characteristics
of granular-balls are less likely to be affected by fine-grained sample points,
making them more robust; the multi-granularity structure of granular-balls can
produce topological structures and coarse-grained descriptions, providing
natural interpretability. Granular-ball computing has now been effectively
extended to various fields of artificial intelligence, developing theoretical
methods such as granular-ball classifiers, granular-ball clustering methods,
granular-ball neural networks, granular-ball rough sets, and granular-ball
evolutionary computation, significantly improving the efficiency, noise
robustness, and interpretability of existing methods. It has good innovation,
practicality, and development potential. This article provides a systematic
introduction to these methods and analyzes the main problems currently faced by
granular-ball computing, discussing both the primary applicable scenarios for
granular-ball computing and offering references and suggestions for future
researchers to improve this theory.
</p>

### Title: Automatically identifying dynamical systems from data. (arXiv:2304.11182v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.11182](http://arxiv.org/abs/2304.11182)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11182] Automatically identifying dynamical systems from data](http://arxiv.org/abs/2304.11182) #noise`
* Summary: <p>Discovering nonlinear differential equations that describe system dynamics
from empirical data is a fundamental challenge in contemporary science. Here,
we propose a methodology to automatically identify dynamical laws by
integrating denoising techniques, sparse regression, and bootstrap confidence
intervals. We evaluate our method on well-known ordinary differential equations
with an ensemble of random initial conditions, time series of increasing
length, and varying signal-to-noise ratios. Our algorithm consistently
identifies three-dimensional systems, given moderately-sized time series and
high signal quality levels relative to background noise. By accurately
identifying dynamical systems, our methodology has the potential to impact
diverse fields, such as the physical and biological sciences, as well as
engineering, where understanding complex systems is crucial.
</p>

### Title: Probabilistic selection and design of concrete using machine learning. (arXiv:2304.11226v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.11226](http://arxiv.org/abs/2304.11226)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11226] Probabilistic selection and design of concrete using machine learning](http://arxiv.org/abs/2304.11226) #noise`
* Summary: <p>Development of robust concrete mixes with a lower environmental impact is
challenging due to natural variability in constituent materials and a multitude
of possible combinations of mix proportions. Making reliable property
predictions with machine learning can facilitate performance-based
specification of concrete, reducing material inefficiencies and improving the
sustainability of concrete construction. In this work, we develop a machine
learning algorithm that can utilize intermediate target variables and their
associated noise to predict the final target variable. We apply the methodology
to specify a concrete mix that has high resistance to carbonation, and another
concrete mix that has low environmental impact. Both mixes also fulfill targets
on the strength, density, and cost. The specified mixes are experimentally
validated against their predictions. Our generic methodology enables the
exploitation of noise in machine learning, which has a broad range of
applications in structural engineering and beyond.
</p>

## diffusion
### Title: Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations. (arXiv:2304.11267v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11267](http://arxiv.org/abs/2304.11267)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11267] Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations](http://arxiv.org/abs/2304.11267) #diffusion`
* Summary: <p>The rapid development and application of foundation models have
revolutionized the field of artificial intelligence. Large diffusion models
have gained significant attention for their ability to generate photorealistic
images and support various tasks. On-device deployment of these models provides
benefits such as lower server costs, offline functionality, and improved user
privacy. However, common large diffusion models have over 1 billion parameters
and pose challenges due to restricted computational and memory resources on
devices. We present a series of implementation optimizations for large
diffusion models that achieve the fastest reported inference latency to-date
(under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung
S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile
devices. These enhancements broaden the applicability of generative AI and
improve the overall user experience across a wide range of devices.
</p>

### Title: Fast Diffusion Probabilistic Model Sampling through the lens of Backward Error Analysis. (arXiv:2304.11446v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11446](http://arxiv.org/abs/2304.11446)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11446] Fast Diffusion Probabilistic Model Sampling through the lens of Backward Error Analysis](http://arxiv.org/abs/2304.11446) #diffusion`
* Summary: <p>Denoising diffusion probabilistic models (DDPMs) are a class of powerful
generative models. The past few years have witnessed the great success of DDPMs
in generating high-fidelity samples. A significant limitation of the DDPMs is
the slow sampling procedure. DDPMs generally need hundreds or thousands of
sequential function evaluations (steps) of neural networks to generate a
sample. This paper aims to develop a fast sampling method for DDPMs requiring
much fewer steps while retaining high sample quality. The inference process of
DDPMs approximates solving the corresponding diffusion ordinary differential
equations (diffusion ODEs) in the continuous limit. This work analyzes how the
backward error affects the diffusion ODEs and the sample quality in DDPMs. We
propose fast sampling through the \textbf{Restricting Backward Error schedule
(RBE schedule)} based on dynamically moderating the long-time backward error.
Our method accelerates DDPMs without any further training. Our experiments show
that sampling with an RBE schedule generates high-quality samples within only 8
to 20 function evaluations on various benchmark datasets. We achieved 12.01 FID
in 8 function evaluations on the ImageNet $128\times128$, and a $20\times$
speedup compared with previous baseline samplers.
</p>

### Title: LaMD: Latent Motion Diffusion for Video Generation. (arXiv:2304.11603v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11603](http://arxiv.org/abs/2304.11603)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11603] LaMD: Latent Motion Diffusion for Video Generation](http://arxiv.org/abs/2304.11603) #diffusion`
* Summary: <p>Generating coherent and natural movement is the key challenge in video
generation. This research proposes to condense video generation into a problem
of motion generation, to improve the expressiveness of motion and make video
generation more manageable. This can be achieved by breaking down the video
generation process into latent motion generation and video reconstruction. We
present a latent motion diffusion (LaMD) framework, which consists of a
motion-decomposed video autoencoder and a diffusion-based motion generator, to
implement this idea. Through careful design, the motion-decomposed video
autoencoder can compress patterns in movement into a concise latent motion
representation. Meanwhile, the diffusion-based motion generator is able to
efficiently generate realistic motion on a continuous latent space under
multi-modal conditions, at a cost that is similar to that of image diffusion
models. Results show that LaMD generates high-quality videos with a wide range
of motions, from stochastic dynamics to highly controllable movements. It
achieves new state-of-the-art performance on benchmark datasets, including
BAIR, Landscape and CATER-GENs, for Image-to-Video (I2V) and
Text-Image-to-Video (TI2V) generation. The source code of LaMD will be made
available soon.
</p>

### Title: On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation. (arXiv:2304.11328v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.11328](http://arxiv.org/abs/2304.11328)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11328] On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation](http://arxiv.org/abs/2304.11328) #diffusion`
* Summary: <p>One popular diffusion-based sampling strategy attempts to solve the reverse
ordinary differential equations (ODEs) effectively. The coefficients of the
obtained ODE solvers are pre-determined by the ODE formulation, the reverse
discrete timesteps, and the employed ODE methods. In this paper, we consider
accelerating several popular ODE-based sampling processes by optimizing certain
coefficients via improved integration approximation (IIA). At each reverse
timestep, we propose to minimize a mean squared error (MSE) function with
respect to certain selected coefficients. The MSE is constructed by applying
the original ODE solver for a set of fine-grained timesteps which in principle
provides a more accurate integration approximation in predicting the next
diffusion hidden state. Given a pre-trained diffusion model, the procedure for
IIA for a particular number of neural functional evaluations (NFEs) only needs
to be conducted once over a batch of samples. The obtained optimal solutions
for those selected coefficients via minimum MSE (MMSE) can be restored and
reused later on to accelerate the sampling process. Extensive experiments on
EDM and DDIM show the IIA technique leads to significant performance gain when
the numbers of NFEs are small.
</p>

### Title: Conditional Denoising Diffusion for Sequential Recommendation. (arXiv:2304.11433v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.11433](http://arxiv.org/abs/2304.11433)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11433] Conditional Denoising Diffusion for Sequential Recommendation](http://arxiv.org/abs/2304.11433) #diffusion`
* Summary: <p>Generative models have attracted significant interest due to their ability to
handle uncertainty by learning the inherent data distributions. However, two
prominent generative models, namely Generative Adversarial Networks (GANs) and
Variational AutoEncoders (VAEs), exhibit challenges that impede achieving
optimal performance in sequential recommendation tasks. Specifically, GANs
suffer from unstable optimization, while VAEs are prone to posterior collapse
and over-smoothed generations. The sparse and noisy nature of sequential
recommendation further exacerbates these issues. In response to these
limitations, we present a conditional denoising diffusion model, which includes
a sequence encoder, a cross-attentive denoising decoder, and a step-wise
diffuser. This approach streamlines the optimization and generation process by
dividing it into easier and tractable steps in a conditional autoregressive
manner. Furthermore, we introduce a novel optimization schema that incorporates
both cross-divergence loss and contrastive loss. This novel training schema
enables the model to generate high-quality sequence/item representations and
meanwhile precluding collapse. We conducted comprehensive experiments on four
benchmark datasets, and the superior performance achieved by our model attests
to its efficacy.
</p>

### Title: Diffusion Model for GPS Trajectory Generation. (arXiv:2304.11582v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.11582](http://arxiv.org/abs/2304.11582)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11582] Diffusion Model for GPS Trajectory Generation](http://arxiv.org/abs/2304.11582) #diffusion`
* Summary: <p>With the deployment of GPS-enabled devices and data acquisition technology,
the massively generated GPS trajectory data provide a core support for
advancing spatial-temporal data mining research. Nonetheless, GPS trajectories
comprise personal geo-location information, rendering inevitable privacy
concerns on plain data. One promising solution to this problem is trajectory
generation, replacing the original data with the generated privacy-free ones.
However, owing to the complex and stochastic behavior of human activities,
generating high-quality trajectories is still in its infancy. To achieve the
objective, we propose a diffusion-based trajectory generation (Diff-Traj)
framework, effectively integrating the generation capability of the diffusion
model and learning from the spatial-temporal features of trajectories.
Specifically, we gradually convert real trajectories to noise through a forward
trajectory noising process. Then, Diff-Traj reconstructs forged trajectories
from the noise by a reverse trajectory denoising process. In addition, we
design a trajectory UNet (Traj-UNet) structure to extract trajectory features
for noise level prediction during the reverse process. Experiments on two
real-world datasets show that Diff-Traj can be intuitively applied to generate
high-quality trajectories while retaining the original distribution.
</p>

## LLM
## segmentation
### Title: SSS3D: Fast Neural Architecture Search For Efficient Three-Dimensional Semantic Segmentation. (arXiv:2304.11207v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11207](http://arxiv.org/abs/2304.11207)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11207] SSS3D: Fast Neural Architecture Search For Efficient Three-Dimensional Semantic Segmentation](http://arxiv.org/abs/2304.11207) #segmentation`
* Summary: <p>We present SSS3D, a fast multi-objective NAS framework designed to find
computationally efficient 3D semantic scene segmentation networks. It uses
RandLA-Net, an off-the-shelf point-based network, as a super-network to enable
weight sharing and reduce search time by 99.67% for single-stage searches.
SSS3D has a complex search space composed of sampling and architectural
parameters that can form 2.88 * 10^17 possible networks. To further reduce
search time, SSS3D splits the complete search space and introduces a two-stage
search that finds optimal subnetworks in 54% of the time required by
single-stage searches.
</p>

### Title: Advances in Deep Concealed Scene Understanding. (arXiv:2304.11234v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11234](http://arxiv.org/abs/2304.11234)
* Code URL: [https://github.com/dengpingfan/csu](https://github.com/dengpingfan/csu)
* Copy Paste: `<input type="checkbox">[[2304.11234] Advances in Deep Concealed Scene Understanding](http://arxiv.org/abs/2304.11234) #segmentation`
* Summary: <p>Concealed scene understanding (CSU) is a hot computer vision topic aiming to
perceive objects with camouflaged properties. The current boom in its advanced
techniques and novel applications makes it timely to provide an up-to-date
survey to enable researchers to understand the global picture of the CSU field,
including both current achievements and major challenges. This paper makes four
contributions: (1) For the first time, we present a comprehensive survey of the
deep learning techniques oriented at CSU, including a background with its
taxonomy, task-unique challenges, and a review of its developments in the deep
learning era via surveying existing datasets and deep techniques. (2) For a
quantitative comparison of the state-of-the-art, we contribute the largest and
latest benchmark for Concealed Object Segmentation (COS). (3) To evaluate the
transferability of deep CSU in practical scenarios, we re-organize the largest
concealed defect segmentation dataset termed CDS2K with the hard cases from
diversified industrial scenarios, on which we construct a comprehensive
benchmark. (4) We discuss open problems and potential research directions for
this community. Our code and datasets are available at
https://github.com/DengPingFan/CSU, which will be updated continuously to watch
and summarize the advancements in this rapidly evolving field.
</p>

### Title: Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model. (arXiv:2304.11332v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11332](http://arxiv.org/abs/2304.11332)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11332] Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model](http://arxiv.org/abs/2304.11332) #segmentation`
* Summary: <p>The Segment Anything Model (SAM) is a recently developed large model for
general-purpose segmentation for computer vision tasks. SAM was trained using
11 million images with over 1 billion masks and can produce segmentation
results for a wide range of objects in natural scene images. SAM can be viewed
as a general perception model for segmentation (partitioning images into
semantically meaningful regions). Thus, how to utilize such a large foundation
model for medical image segmentation is an emerging research target. This paper
shows that although SAM does not immediately give high-quality segmentation for
medical images, its generated masks, features, and stability scores are useful
for building and training better medical image segmentation models. In
particular, we demonstrate how to use SAM to augment image inputs for a
commonly-used medical image segmentation model (e.g., U-Net). Experiments on
two datasets show the effectiveness of our proposed method.
</p>

### Title: Single-stage Multi-human Parsing via Point Sets and Center-based Offsets. (arXiv:2304.11356v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11356](http://arxiv.org/abs/2304.11356)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11356] Single-stage Multi-human Parsing via Point Sets and Center-based Offsets](http://arxiv.org/abs/2304.11356) #segmentation`
* Summary: <p>This work studies the multi-human parsing problem. Existing methods, either
following top-down or bottom-up two-stage paradigms, usually involve expensive
computational costs. We instead present a high-performance Single-stage
Multi-human Parsing (SMP) deep architecture that decouples the multi-human
parsing problem into two fine-grained sub-problems, i.e., locating the human
body and parts. SMP leverages the point features in the barycenter positions to
obtain their segmentation and then generates a series of offsets from the
barycenter of the human body to the barycenters of parts, thus performing human
body and parts matching without the grouping process. Within the SMP
architecture, we propose a Refined Feature Retain module to extract the global
feature of instances through generated mask attention and a Mask of Interest
Reclassify module as a trainable plug-in module to refine the classification
results with the predicted segmentation. Extensive experiments on the MHPv2.0
dataset demonstrate the best effectiveness and efficiency of the proposed
method, surpassing the state-of-the-art method by 2.1% in AP50p, 1.0% in
APvolp, and 1.2% in PCP50. In particular, the proposed method requires fewer
training epochs and a less complex model architecture. We will release our
source codes, pretrained models, and online demos to facilitate further
studies.
</p>

### Title: Knowledge Distillation from 3D to Bird's-Eye-View for LiDAR Semantic Segmentation. (arXiv:2304.11393v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11393](http://arxiv.org/abs/2304.11393)
* Code URL: [https://github.com/fengjiang5/knowledge-distillation-from-cylinder3d-to-polarnet](https://github.com/fengjiang5/knowledge-distillation-from-cylinder3d-to-polarnet)
* Copy Paste: `<input type="checkbox">[[2304.11393] Knowledge Distillation from 3D to Bird's-Eye-View for LiDAR Semantic Segmentation](http://arxiv.org/abs/2304.11393) #segmentation`
* Summary: <p>LiDAR point cloud segmentation is one of the most fundamental tasks for
autonomous driving scene understanding. However, it is difficult for existing
models to achieve both high inference speed and accuracy simultaneously. For
example, voxel-based methods perform well in accuracy, while Bird's-Eye-View
(BEV)-based methods can achieve real-time inference. To overcome this issue, we
develop an effective 3D-to-BEV knowledge distillation method that transfers
rich knowledge from 3D voxel-based models to BEV-based models. Our framework
mainly consists of two modules: the voxel-to-pillar distillation module and the
label-weight distillation module. Voxel-to-pillar distillation distills sparse
3D features to BEV features for middle layers to make the BEV-based model aware
of more structural and geometric information. Label-weight distillation helps
the model pay more attention to regions with more height information. Finally,
we conduct experiments on the SemanticKITTI dataset and Paris-Lille-3D. The
results on SemanticKITTI show more than 5% improvement on the test set,
especially for classes such as motorcycle and person, with more than 15%
improvement. The code can be accessed at
https://github.com/fengjiang5/Knowledge-Distillation-from-Cylinder3D-to-PolarNet.
</p>

### Title: SACANet: scene-aware class attention network for semantic segmentation of remote sensing images. (arXiv:2304.11424v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11424](http://arxiv.org/abs/2304.11424)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11424] SACANet: scene-aware class attention network for semantic segmentation of remote sensing images](http://arxiv.org/abs/2304.11424) #segmentation`
* Summary: <p>Spatial attention mechanism has been widely used in semantic segmentation of
remote sensing images given its capability to model long-range dependencies.
Many methods adopting spatial attention mechanism aggregate contextual
information using direct relationships between pixels within an image, while
ignoring the scene awareness of pixels (i.e., being aware of the global context
of the scene where the pixels are located and perceiving their relative
positions). Given the observation that scene awareness benefits context
modeling with spatial correlations of ground objects, we design a scene-aware
attention module based on a refined spatial attention mechanism embedding scene
awareness. Besides, we present a local-global class attention mechanism to
address the problem that general attention mechanism introduces excessive
background noises while hardly considering the large intra-class variance in
remote sensing images. In this paper, we integrate both scene-aware and class
attentions to propose a scene-aware class attention network (SACANet) for
semantic segmentation of remote sensing images. Experimental results on three
datasets show that SACANet outperforms other state-of-the-art methods and
validate its effectiveness. Code is available at
https://github.com/xwmaxwma/rssegmentation.
</p>

### Title: Semi-Supervised Semantic Segmentation With Region Relevance. (arXiv:2304.11539v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11539](http://arxiv.org/abs/2304.11539)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11539] Semi-Supervised Semantic Segmentation With Region Relevance](http://arxiv.org/abs/2304.11539) #segmentation`
* Summary: <p>Semi-supervised semantic segmentation aims to learn from a small amount of
labeled data and plenty of unlabeled ones for the segmentation task. The most
common approach is to generate pseudo-labels for unlabeled images to augment
the training data. However, the noisy pseudo-labels will lead to cumulative
classification errors and aggravate the local inconsistency in prediction. This
paper proposes a Region Relevance Network (RRN) to alleviate the problem
mentioned above. Specifically, we first introduce a local pseudo-label
filtering module that leverages discriminator networks to assess the accuracy
of the pseudo-label at the region level. A local selection loss is proposed to
mitigate the negative impact of wrong pseudo-labels in consistency
regularization training. In addition, we propose a dynamic region-loss
correction module, which takes the merit of network diversity to further rate
the reliability of pseudo-labels and correct the convergence direction of the
segmentation network with a dynamic region loss. Extensive experiments are
conducted on PASCAL VOC 2012 and Cityscapes datasets with varying amounts of
labeled data, demonstrating that our proposed approach achieves
state-of-the-art performance compared to current counterparts.
</p>

### Title: Segment Anything in Non-Euclidean Domains: Challenges and Opportunities. (arXiv:2304.11595v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11595](http://arxiv.org/abs/2304.11595)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11595] Segment Anything in Non-Euclidean Domains: Challenges and Opportunities](http://arxiv.org/abs/2304.11595) #segmentation`
* Summary: <p>The recent work known as Segment Anything (SA) has made significant strides
in pushing the boundaries of semantic segmentation into the era of foundation
models. The impact of SA has sparked extremely active discussions and ushered
in an encouraging new wave of developing foundation models for the diverse
tasks in the Euclidean domain, such as object detection and image inpainting.
Despite the promising advances led by SA, the concept has yet to be extended to
the non-Euclidean graph domain. In this paper, we explore a novel Segment
Non-Euclidean Anything (SNA) paradigm that strives to develop foundation models
that can handle the diverse range of graph data within the non-Euclidean
domain, seeking to expand the scope of SA and lay the groundwork for future
research in this direction. To achieve this goal, we begin by discussing the
recent achievements in foundation models associated with SA. We then shed light
on the unique challenges that arise when applying the SA concept to graph
analysis, which involves understanding the differences between the Euclidean
and non-Euclidean domains from both the data and task perspectives. Motivated
by these observations, we present several preliminary solutions to tackle the
challenges of SNA and detail their corresponding limitations, along with
several potential directions to pave the way for future SNA research.
Experiments on five Open Graph Benchmark (OGB) datasets across various tasks,
including graph property classification and regression, as well as multi-label
prediction, demonstrate that the performance of the naive SNA solutions has
considerable room for improvement, pointing towards a promising avenue for
future exploration of Graph General Intelligence.
</p>

### Title: PiClick: Picking the desired mask in click-based interactive segmentation. (arXiv:2304.11609v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11609](http://arxiv.org/abs/2304.11609)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11609] PiClick: Picking the desired mask in click-based interactive segmentation](http://arxiv.org/abs/2304.11609) #segmentation`
* Summary: <p>Click-based interactive segmentation enables productive pixel-level
annotation and image editing with simple user clicks, whereas target ambiguity
remains a problem hindering precise segmentation. That is, in scenes with rich
context, one click may refer to multiple potential targets residing in
corresponding masks, while most interactive segmentors can only generate one
single mask and fail to capture the rich context. To resolve target ambiguity,
we here propose PiClick to produce semantically diversified masks. PiClick
leverages a transformer network design wherein mutually interactive mask
queries are integrated to infuse target priors. Moreover, a Target Reasoning
Module is designed in PiClick to automatically imply the best-matched mask from
all proposals, significantly relieving target ambiguity as well as extra human
intervention. Extensive experiments conducted on all 9 interactive segmentation
datasets not only demonstrate the state-of-the-art segmentation performance of
PiClick, but also reduces human interventions with multiple proposal generation
and target reasoning. To promote direct usage and future endeavors, we release
the source code of PiClick together with a plug-and-play annotation tool at
https://github.com/cilinyan/PiClick.
</p>

## object detection
### Title: OmniLabel: A Challenging Benchmark for Language-Based Object Detection. (arXiv:2304.11463v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11463](http://arxiv.org/abs/2304.11463)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11463] OmniLabel: A Challenging Benchmark for Language-Based Object Detection](http://arxiv.org/abs/2304.11463) #object detection`
* Summary: <p>Language-based object detection is a promising direction towards building a
natural interface to describe objects in images that goes far beyond plain
category names. While recent methods show great progress in that direction,
proper evaluation is lacking. With OmniLabel, we propose a novel task
definition, dataset, and evaluation metric. The task subsumes standard- and
open-vocabulary detection as well as referring expressions. With more than 28K
unique object descriptions on over 25K images, OmniLabel provides a challenging
benchmark with diverse and complex object descriptions in a naturally
open-vocabulary setting. Moreover, a key differentiation to existing benchmarks
is that our object descriptions can refer to one, multiple or even no object,
hence, providing negative examples in free-form text. The proposed evaluation
handles the large label space and judges performance via a modified average
precision metric, which we validate by evaluating strong language-based
baselines. OmniLabel indeed provides a challenging test bed for future research
on language-based detection.
</p>

### Title: A Framework for Benchmarking Real-Time Embedded Object Detection. (arXiv:2304.11580v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.11580](http://arxiv.org/abs/2304.11580)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.11580] A Framework for Benchmarking Real-Time Embedded Object Detection](http://arxiv.org/abs/2304.11580) #object detection`
* Summary: <p>Object detection is one of the key tasks in many applications of computer
vision. Deep Neural Networks (DNNs) are undoubtedly a well-suited approach for
object detection. However, such DNNs need highly adapted hardware together with
hardware-specific optimization to guarantee high efficiency during inference.
This is especially the case when aiming for efficient object detection in video
streaming applications on limited hardware such as edge devices. Comparing
vendor-specific hardware and related optimization software pipelines in a fair
experimental setup is a challenge. In this paper, we propose a framework that
uses a host computer with a host software application together with a
light-weight interface based on the Message Queuing Telemetry Transport (MQTT)
protocol. Various different target devices with target apps can be connected
via MQTT with this host computer. With well-defined and standardized MQTT
messages, object detection results can be reported to the host computer, where
the results are evaluated without harming or influencing the processing on the
device. With this quite generic framework, we can measure the object detection
performance, the runtime, and the energy efficiency at the same time. The
effectiveness of this framework is demonstrated in multiple experiments that
offer deep insights into the optimization of DNNs.
</p>

