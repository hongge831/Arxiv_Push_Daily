## data-free
### Title: eTag: Class-Incremental Learning with Embedding Distillation and Task-Oriented Generation. (arXiv:2304.10103v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10103](http://arxiv.org/abs/2304.10103)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10103] eTag: Class-Incremental Learning with Embedding Distillation and Task-Oriented Generation](http://arxiv.org/abs/2304.10103) #data-free`
* Summary: <p>Class-Incremental Learning (CIL) aims to solve the neural networks'
catastrophic forgetting problem, which refers to the fact that once the network
updates on a new task, its performance on previously-learned tasks drops
dramatically. Most successful CIL methods incrementally train a feature
extractor with the aid of stored exemplars, or estimate the feature
distribution with the stored prototypes. However, the stored exemplars would
violate the data privacy concerns, while the stored prototypes might not
reasonably be consistent with a proper feature distribution, hindering the
exploration of real-world CIL applications. In this paper, we propose a method
of \textit{e}mbedding distillation and \textit{Ta}sk-oriented
\textit{g}eneration (\textit{eTag}) for CIL, which requires neither the
exemplar nor the prototype. Instead, eTag achieves a data-free manner to train
the neural networks incrementally. To prevent the feature extractor from
forgetting, eTag distills the embeddings of the network's intermediate blocks.
Additionally, eTag enables a generative network to produce suitable features,
fitting the needs of the top incremental classifier. Experimental results
confirmed that our proposed eTag considerably outperforms the state-of-the-art
methods on CIFAR-100 and ImageNet-sub\footnote{Our code is available in the
Supplementary Materials.
</p>

## transformer
### Title: DCN-T: Dual Context Network with Transformer for Hyperspectral Image Classification. (arXiv:2304.09915v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.09915](http://arxiv.org/abs/2304.09915)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.09915] DCN-T: Dual Context Network with Transformer for Hyperspectral Image Classification](http://arxiv.org/abs/2304.09915) #transformer`
* Summary: <p>Hyperspectral image (HSI) classification is challenging due to spatial
variability caused by complex imaging conditions. Prior methods suffer from
limited representation ability, as they train specially designed networks from
scratch on limited annotated data. We propose a tri-spectral image generation
pipeline that transforms HSI into high-quality tri-spectral images, enabling
the use of off-the-shelf ImageNet pretrained backbone networks for feature
extraction. Motivated by the observation that there are many homogeneous areas
with distinguished semantic and geometric properties in HSIs, which can be used
to extract useful contexts, we propose an end-to-end segmentation network named
DCN-T. It adopts transformers to effectively encode regional adaptation and
global aggregation spatial contexts within and between the homogeneous areas
discovered by similarity-based clustering. To fully exploit the rich spectrums
of the HSI, we adopt an ensemble approach where all segmentation results of the
tri-spectral images are integrated into the final prediction through a voting
scheme. Extensive experiments on three public benchmarks show that our proposed
method outperforms state-of-the-art methods for HSI classification.
</p>

### Title: Learning CLIP Guided Visual-Text Fusion Transformer for Video-based Pedestrian Attribute Recognition. (arXiv:2304.10091v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10091](http://arxiv.org/abs/2304.10091)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10091] Learning CLIP Guided Visual-Text Fusion Transformer for Video-based Pedestrian Attribute Recognition](http://arxiv.org/abs/2304.10091) #transformer`
* Summary: <p>Existing pedestrian attribute recognition (PAR) algorithms are mainly
developed based on a static image. However, the performance is not reliable for
images with challenging factors, such as heavy occlusion, motion blur, etc. In
this work, we propose to understand human attributes using video frames that
can make full use of temporal information. Specifically, we formulate the
video-based PAR as a vision-language fusion problem and adopt pre-trained big
models CLIP to extract the feature embeddings of given video frames. To better
utilize the semantic information, we take the attribute list as another input
and transform the attribute words/phrase into the corresponding sentence via
split, expand, and prompt. Then, the text encoder of CLIP is utilized for
language embedding. The averaged visual tokens and text tokens are concatenated
and fed into a fusion Transformer for multi-modal interactive learning. The
enhanced tokens will be fed into a classification head for pedestrian attribute
prediction. Extensive experiments on a large-scale video-based PAR dataset
fully validated the effectiveness of our proposed framework.
</p>

### Title: OOD-CV-v2: An extended Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images. (arXiv:2304.10266v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10266](http://arxiv.org/abs/2304.10266)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10266] OOD-CV-v2: An extended Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images](http://arxiv.org/abs/2304.10266) #transformer`
* Summary: <p>Enhancing the robustness of vision algorithms in real-world scenarios is
challenging. One reason is that existing robustness benchmarks are limited, as
they either rely on synthetic data or ignore the effects of individual nuisance
factors. We introduce OOD-CV-v2, a benchmark dataset that includes
out-of-distribution examples of 10 object categories in terms of pose, shape,
texture, context and the weather conditions, and enables benchmarking of models
for image classification, object detection, and 3D pose estimation. In addition
to this novel dataset, we contribute extensive experiments using popular
baseline methods, which reveal that: 1) Some nuisance factors have a much
stronger negative effect on the performance compared to others, also depending
on the vision task. 2) Current approaches to enhance robustness have only
marginal effects, and can even reduce robustness. 3) We do not observe
significant differences between convolutional and transformer architectures. We
believe our dataset provides a rich test bed to study robustness and will help
push forward research in this area.
</p>
<p>Our dataset can be accessed from <a href="http://www.ood-cv.org/challenge.html">this http URL</a>
</p>

### Title: DropDim: A Regularization Method for Transformer Networks. (arXiv:2304.10321v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.10321](http://arxiv.org/abs/2304.10321)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10321] DropDim: A Regularization Method for Transformer Networks](http://arxiv.org/abs/2304.10321) #transformer`
* Summary: <p>We introduceDropDim, a structured dropout method designed for regularizing
the self-attention mechanism, which is a key component of the transformer. In
contrast to the general dropout method, which randomly drops neurons, DropDim
drops part of the embedding dimensions. In this way, the semantic information
can be completely discarded. Thus, the excessive coadapting between different
embedding dimensions can be broken, and the self-attention is forced to encode
meaningful featureswith a certain number of embedding dimensions erased.
Experiments on a wide range of tasks executed on the MUST-C English-Germany
dataset show that DropDim can effectively improve model performance, reduce
over-fitting, and show complementary effects with other regularization methods.
When combined with label smoothing, the WER can be reduced from 19.1% to 15.1%
on the ASR task, and the BLEU value can be increased from26.90 to 28.38 on the
MT task. On the ST task, the model can reach a BLEU score of 22.99, an increase
by 1.86 BLEU points compared to the strong baseline.
</p>

### Title: Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts. (arXiv:2304.10505v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10505](http://arxiv.org/abs/2304.10505)
* Code URL: [https://github.com/KastanDay/video-pretrained-transformer](https://github.com/KastanDay/video-pretrained-transformer)
* Copy Paste: `<input type="checkbox">[[2304.10505] Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts](http://arxiv.org/abs/2304.10505) #transformer`
* Summary: <p>We present Video Pre-trained Transformer. VPT uses four SOTA encoder models
from prior work to convert a video into a sequence of compact embeddings. Our
backbone, based on a reference Flan-T5-11B architecture, learns a universal
representation of the video that is a non-linear sum of the encoder models. It
learns using an autoregressive causal language modeling loss by predicting the
words spoken in YouTube videos. Finally, we evaluate on standard downstream
benchmarks by training fully connected prediction heads for each task. To the
best of our knowledge, this is the first use of multiple frozen SOTA models as
encoders in an "embedding -&gt; backbone -&gt; prediction head" design pattern - all
others have trained their own joint encoder models. Additionally, we include
more modalities than the current SOTA, Merlot Reserve, by adding explicit Scene
Graph information. For these two reasons, we believe it could combine the
world's best open-source models to achieve SOTA performance. Initial
experiments demonstrate the model is learning appropriately, but more
experimentation and compute is necessary, and already in progress, to realize
our loftier goals. Alongside this work, we build on the YT-20M dataset,
reproducing it and adding 25,000 personally selected YouTube videos to its
corpus. All code and model checkpoints are open sourced under a standard MIT
license.
</p>

### Title: Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget. (arXiv:2304.10520v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10520](http://arxiv.org/abs/2304.10520)
* Code URL: [https://github.com/ml-jku/mae-ct](https://github.com/ml-jku/mae-ct)
* Copy Paste: `<input type="checkbox">[[2304.10520] Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget](http://arxiv.org/abs/2304.10520) #transformer`
* Summary: <p>Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE),
efficiently learn a rich representation of the input. However, for adapting to
downstream tasks, they require a sufficient amount of labeled data since their
rich features capture not only objects but also less relevant image background.
In contrast, Instance Discrimination (ID) methods focus on objects. In this
work, we study how to combine the efficiency and scalability of MIM with the
ability of ID to perform downstream classification in the absence of large
amounts of labeled data. To this end, we introduce Masked Autoencoder
Contrastive Tuning (MAE-CT), a sequential approach that applies Nearest
Neighbor Contrastive Learning (NNCLR) to a pre-trained MAE. MAE-CT tunes the
rich features such that they form semantic clusters of objects without using
any labels. Applied to large and huge Vision Transformer (ViT) models, MAE-CT
matches or excels previous self-supervised methods trained on ImageNet in
linear probing, k-NN and low-shot classification accuracy as well as in
unsupervised clustering accuracy. Notably, similar results can be achieved
without additional image augmentations. While ID methods generally rely on
hand-crafted augmentations to avoid shortcut learning, we find that nearest
neighbor lookup is sufficient and that this data-driven augmentation effect
improves with model size. MAE-CT is compute efficient. For instance, starting
from a MAE pre-trained ViT-L/16, MAE-CT increases the ImageNet 1% low-shot
accuracy from 67.7% to 72.6%, linear probing accuracy from 76.0% to 80.2% and
k-NN accuracy from 60.6% to 79.1% in just five hours using eight A100 GPUs.
</p>

### Title: Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers. (arXiv:2304.09948v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.09948](http://arxiv.org/abs/2304.09948)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.09948] Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers](http://arxiv.org/abs/2304.09948) #transformer`
* Summary: <p>The proliferation of fake reviews of doctors has potentially detrimental
consequences for patient well-being and has prompted concern among consumer
protection groups and regulatory bodies. Yet despite significant advancements
in the fields of machine learning and natural language processing, there
remains limited comprehension of the characteristics differentiating fraudulent
from authentic reviews. This study utilizes a novel pre-labeled dataset of
38048 physician reviews to establish the effectiveness of large language models
in classifying reviews. Specifically, we compare the performance of traditional
ML models, such as logistic regression and support vector machines, to
generative pre-trained transformer models. Furthermore, we use GPT4, the newest
model in the GPT family, to uncover the key dimensions along which fake and
genuine physician reviews differ. Our findings reveal significantly superior
performance of GPT-3 over traditional ML models in this context. Additionally,
our analysis suggests that GPT3 requires a smaller training sample than
traditional models, suggesting its appropriateness for tasks with scarce
training data. Moreover, the superiority of GPT3 performance increases in the
cold start context i.e., when there are no prior reviews of a doctor. Finally,
we employ GPT4 to reveal the crucial dimensions that distinguish fake physician
reviews. In sharp contrast to previous findings in the literature that were
obtained using simulated data, our findings from a real-world dataset show that
fake reviews are generally more clinically detailed, more reserved in
sentiment, and have better structure and grammar than authentic ones.
</p>

### Title: MasakhaNEWS: News Topic Classification for African languages. (arXiv:2304.09972v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.09972](http://arxiv.org/abs/2304.09972)
* Code URL: [https://github.com/masakhane-io/masakhane-news](https://github.com/masakhane-io/masakhane-news)
* Copy Paste: `<input type="checkbox">[[2304.09972] MasakhaNEWS: News Topic Classification for African languages](http://arxiv.org/abs/2304.09972) #transformer`
* Summary: <p>African languages are severely under-represented in NLP research due to lack
of datasets covering several NLP tasks. While there are individual language
specific datasets that are being expanded to different tasks, only a handful of
NLP tasks (e.g. named entity recognition and machine translation) have
standardized benchmark datasets covering several geographical and
typologically-diverse African languages. In this paper, we develop MasakhaNEWS
-- a new benchmark dataset for news topic classification covering 16 languages
widely spoken in Africa. We provide an evaluation of baseline models by
training classical machine learning models and fine-tuning several language
models. Furthermore, we explore several alternatives to full fine-tuning of
language models that are better suited for zero-shot and few-shot learning such
as cross-lingual parameter-efficient fine-tuning (like MAD-X), pattern
exploiting training (PET), prompting language models (like ChatGPT), and
prompt-free sentence transformer fine-tuning (SetFit and Cohere Embedding API).
Our evaluation in zero-shot setting shows the potential of prompting ChatGPT
for news topic classification in low-resource African languages, achieving an
average performance of 70 F1 points without leveraging additional supervision
like MAD-X. In few-shot setting, we show that with as little as 10 examples per
label, we achieved more than 90\% (i.e. 86.0 F1 points) of the performance of
full supervised training (92.6 F1 points) leveraging the PET approach.
</p>

### Title: Beyond Transformers for Function Learning. (arXiv:2304.09979v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.09979](http://arxiv.org/abs/2304.09979)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.09979] Beyond Transformers for Function Learning](http://arxiv.org/abs/2304.09979) #transformer`
* Summary: <p>The ability to learn and predict simple functions is a key aspect of human
intelligence. Recent works have started to explore this ability using
transformer architectures, however it remains unclear whether this is
sufficient to recapitulate the extrapolation abilities of people in this
domain. Here, we propose to address this gap by augmenting the transformer
architecture with two simple inductive learning biases, that are directly
adapted from recent models of abstract reasoning in cognitive science. The
results we report demonstrate that these biases are helpful in the context of
large neural network models, as well as shed light on the types of inductive
learning biases that may contribute to human abilities in extrapolation.
</p>

### Title: Recurrent Transformer for Dynamic Graph Representation Learning with Edge Temporal States. (arXiv:2304.10079v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.10079](http://arxiv.org/abs/2304.10079)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10079] Recurrent Transformer for Dynamic Graph Representation Learning with Edge Temporal States](http://arxiv.org/abs/2304.10079) #transformer`
* Summary: <p>Dynamic graph representation learning is growing as a trending yet
challenging research task owing to the widespread demand for graph data
analysis in real world applications. Despite the encouraging performance of
many recent works that build upon recurrent neural networks (RNNs) and graph
neural networks (GNNs), they fail to explicitly model the impact of edge
temporal states on node features over time slices. Additionally, they are
challenging to extract global structural features because of the inherent
over-smoothing disadvantage of GNNs, which further restricts the performance.
In this paper, we propose a recurrent difference graph transformer (RDGT)
framework, which firstly assigns the edges in each snapshot with various types
and weights to illustrate their specific temporal states explicitly, then a
structure-reinforced graph transformer is employed to capture the temporal node
representations by a recurrent learning paradigm. Experimental results on four
real-world datasets demonstrate the superiority of RDGT for discrete dynamic
graph representation learning, as it consistently outperforms competing methods
in dynamic link prediction tasks.
</p>

## generative
### Title: Not Only Generative Art: Stable Diffusion for Content-Style Disentanglement in Art Analysis. (arXiv:2304.10278v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10278](http://arxiv.org/abs/2304.10278)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10278] Not Only Generative Art: Stable Diffusion for Content-Style Disentanglement in Art Analysis](http://arxiv.org/abs/2304.10278) #generative`
* Summary: <p>The duality of content and style is inherent to the nature of art. For
humans, these two elements are clearly different: content refers to the objects
and concepts in the piece of art, and style to the way it is expressed. This
duality poses an important challenge for computer vision. The visual appearance
of objects and concepts is modulated by the style that may reflect the author's
emotions, social trends, artistic movement, etc., and their deep comprehension
undoubtfully requires to handle both. A promising step towards a general
paradigm for art analysis is to disentangle content and style, whereas relying
on human annotations to cull a single aspect of artworks has limitations in
learning semantic concepts and the visual appearance of paintings. We thus
present GOYA, a method that distills the artistic knowledge captured in a
recent generative model to disentangle content and style. Experiments show that
synthetically generated images sufficiently serve as a proxy of the real
distribution of artworks, allowing GOYA to separately represent the two
elements of art while keeping more information than existing methods.
</p>

### Title: FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits. (arXiv:2304.10306v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10306](http://arxiv.org/abs/2304.10306)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10306] FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits](http://arxiv.org/abs/2304.10306) #generative`
* Summary: <p>Generative DNNs are a powerful tool for image synthesis, but they are limited
by their computational load. On the other hand, given a trained model and a
task, e.g. faces generation within a range of characteristics, the output image
quality will be unevenly distributed among images with different
characteristics. It follows, that we might restrain the models complexity on
some instances, maintaining a high quality. We propose a method for diminishing
computations by adding so-called early exit branches to the original
architecture, and dynamically switching the computational path depending on how
difficult it will be to render the output. We apply our method on two different
SOTA models performing generative tasks: generation from a semantic map, and
cross-reenactment of face expressions; showing it is able to output images with
custom lower-quality thresholds. For a threshold of LPIPS &lt;=0.1, we diminish
their computations by up to a half. This is especially relevant for real-time
applications such as synthesis of faces, when quality loss needs to be
contained, but most of the inputs need fewer computations than the complex
instances.
</p>

### Title: Adaptive Consensus Optimization Method for GANs. (arXiv:2304.10317v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.10317](http://arxiv.org/abs/2304.10317)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10317] Adaptive Consensus Optimization Method for GANs](http://arxiv.org/abs/2304.10317) #generative`
* Summary: <p>We propose a second order gradient based method with ADAM and RMSprop for the
training of generative adversarial networks. The proposed method is fastest to
obtain similar accuracy when compared to prominent second order methods. Unlike
state-of-the-art recent methods, it does not require solving a linear system,
or it does not require additional mixed second derivative terms. We derive the
fixed point iteration corresponding to proposed method, and show that the
proposed method is convergent. The proposed method produces better or
comparable inception scores, and comparable quality of images compared to other
recently proposed state-of-the-art second order methods. Compared to first
order methods such as ADAM, it produces significantly better inception scores.
The proposed method is compared and validated on popular datasets such as FFHQ,
LSUN, CIFAR10, MNIST, and Fashion MNIST for image generation
tasks\footnote{Accepted in IJCNN 2023}. Codes:
\url{https://github.com/misterpawan/acom}
</p>

### Title: GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape Generative Models. (arXiv:2304.10523v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10523](http://arxiv.org/abs/2304.10523)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10523] GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape Generative Models](http://arxiv.org/abs/2304.10523) #generative`
* Summary: <p>This paper introduces GenCorres, a novel unsupervised joint shape matching
(JSM) approach. The basic idea of GenCorres is to learn a parametric mesh
generator to fit an unorganized deformable shape collection while constraining
deformations between adjacent synthetic shapes to preserve geometric structures
such as local rigidity and local conformality. GenCorres presents three
appealing advantages over existing JSM techniques. First, GenCorres performs
JSM among a synthetic shape collection whose size is much bigger than the input
shapes and fully leverages the data-driven power of JSM. Second, GenCorres
unifies consistent shape matching and pairwise matching (i.e., by enforcing
deformation priors between adjacent synthetic shapes). Third, the generator
provides a concise encoding of consistent shape correspondences. However,
learning a mesh generator from an unorganized shape collection is challenging.
It requires a good initial fitting to each shape and can easily get trapped by
local minimums. GenCorres addresses this issue by learning an implicit
generator from the input shapes, which provides intermediate shapes between two
arbitrary shapes. We introduce a novel approach for computing correspondences
between adjacent implicit surfaces and force the correspondences to preserve
geometric structures and be cycle-consistent. Synthetic shapes of the implicit
generator then guide initial fittings (i.e., via template-based deformation)
for learning the mesh generator. Experimental results show that GenCorres
considerably outperforms state-of-the-art JSM techniques on benchmark datasets.
The synthetic shapes of GenCorres preserve local geometric features and yield
competitive performance gains against state-of-the-art deformable shape
generators.
</p>

### Title: GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models. (arXiv:2304.09875v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.09875](http://arxiv.org/abs/2304.09875)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.09875] GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models](http://arxiv.org/abs/2304.09875) #generative`
* Summary: <p>Current studies on adversarial robustness mainly focus on aggregating local
robustness results from a set of data samples to evaluate and rank different
models. However, the local statistics may not well represent the true global
robustness of the underlying unknown data distribution. To address this
challenge, this paper makes the first attempt to present a new framework,
called GREAT Score , for global robustness evaluation of adversarial
perturbation using generative models. Formally, GREAT Score carries the
physical meaning of a global statistic capturing a mean certified attack-proof
perturbation level over all samples drawn from a generative model. For
finite-sample evaluation, we also derive a probabilistic guarantee on the
sample complexity and the difference between the sample mean and the true mean.
GREAT Score has several advantages: (1) Robustness evaluations using GREAT
Score are efficient and scalable to large models, by sparing the need of
running adversarial attacks. In particular, we show high correlation and
significantly reduced computation cost of GREAT Score when compared to the
attack-based model ranking on RobustBench (Croce,et. al. 2021). (2) The use of
generative models facilitates the approximation of the unknown data
distribution. In our ablation study with different generative adversarial
networks (GANs), we observe consistency between global robustness evaluation
and the quality of GANs. (3) GREAT Score can be used for remote auditing of
privacy-sensitive black-box models, as demonstrated by our robustness
evaluation on several online facial recognition services.
</p>

### Title: Learning Representative Trajectories of Dynamical Systems via Domain-Adaptive Imitation. (arXiv:2304.10260v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.10260](http://arxiv.org/abs/2304.10260)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10260] Learning Representative Trajectories of Dynamical Systems via Domain-Adaptive Imitation](http://arxiv.org/abs/2304.10260) #generative`
* Summary: <p>Domain-adaptive trajectory imitation is a skill that some predators learn for
survival, by mapping dynamic information from one domain (their speed and
steering direction) to a different domain (current position of the moving
prey). An intelligent agent with this skill could be exploited for a diversity
of tasks, including the recognition of abnormal motion in traffic once it has
learned to imitate representative trajectories. Towards this direction, we
propose DATI, a deep reinforcement learning agent designed for domain-adaptive
trajectory imitation using a cycle-consistent generative adversarial method.
Our experiments on a variety of synthetic families of reference trajectories
show that DATI outperforms baseline methods for imitation learning and optimal
control in this setting, keeping the same per-task hyperparameters. Its
generalization to a real-world scenario is shown through the discovery of
abnormal motion patterns in maritime traffic, opening the door for the use of
deep reinforcement learning methods for spatially-unconstrained trajectory data
mining.
</p>

### Title: SARF: Aliasing Relation Assisted Self-Supervised Learning for Few-shot Relation Reasoning. (arXiv:2304.10297v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.10297](http://arxiv.org/abs/2304.10297)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10297] SARF: Aliasing Relation Assisted Self-Supervised Learning for Few-shot Relation Reasoning](http://arxiv.org/abs/2304.10297) #generative`
* Summary: <p>Few-shot relation reasoning on knowledge graphs (FS-KGR) aims to infer
long-tail data-poor relations, which has drawn increasing attention these years
due to its practicalities. The pre-training of previous methods needs to
manually construct the meta-relation set, leading to numerous labor costs.
Self-supervised learning (SSL) is treated as a solution to tackle the issue,
but still at an early stage for FS-KGR task. Moreover, most of the existing
methods ignore leveraging the beneficial information from aliasing relations
(AR), i.e., data-rich relations with similar contextual semantics to the target
data-poor relation. Therefore, we proposed a novel Self-Supervised Learning
model by leveraging Aliasing Relations to assist FS-KGR, termed SARF.
Concretely, four main components are designed in our model, i.e., SSL reasoning
module, AR-assisted mechanism, fusion module, and scoring function. We first
generate the representation of the co-occurrence patterns in a generative
manner. Meanwhile, the representations of aliasing relations are learned to
enhance reasoning in the AR-assist mechanism. Besides, multiple strategies,
i.e., simple summation and learnable fusion, are offered for representation
fusion. Finally, the generated representation is used for scoring. Extensive
experiments on three few-shot benchmarks demonstrate that SARF achieves
state-of-the-art performance compared with other methods in most cases.
</p>

## label correction
## noise
### Title: Certified Adversarial Robustness Within Multiple Perturbation Bounds. (arXiv:2304.10446v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.10446](http://arxiv.org/abs/2304.10446)
* Code URL: [https://github.com/val-iisc/nu-certified-robustness](https://github.com/val-iisc/nu-certified-robustness)
* Copy Paste: `<input type="checkbox">[[2304.10446] Certified Adversarial Robustness Within Multiple Perturbation Bounds](http://arxiv.org/abs/2304.10446) #noise`
* Summary: <p>Randomized smoothing (RS) is a well known certified defense against
adversarial attacks, which creates a smoothed classifier by predicting the most
likely class under random noise perturbations of inputs during inference. While
initial work focused on robustness to $\ell_2$ norm perturbations using noise
sampled from a Gaussian distribution, subsequent works have shown that
different noise distributions can result in robustness to other $\ell_p$ norm
bounds as well. In general, a specific noise distribution is optimal for
defending against a given $\ell_p$ norm based attack. In this work, we aim to
improve the certified adversarial robustness against multiple perturbation
bounds simultaneously. Towards this, we firstly present a novel
\textit{certification scheme}, that effectively combines the certificates
obtained using different noise distributions to obtain optimal results against
multiple perturbation bounds. We further propose a novel \textit{training noise
distribution} along with a \textit{regularized training scheme} to improve the
certification within both $\ell_1$ and $\ell_2$ perturbation norms
simultaneously. Contrary to prior works, we compare the certified robustness of
different training algorithms across the same natural (clean) accuracy, rather
than across fixed noise levels used for training and certification. We also
empirically invalidate the argument that training and certifying the classifier
with the same amount of noise gives the best results. The proposed approach
achieves improvements on the ACR (Average Certified Radius) metric across both
$\ell_1$ and $\ell_2$ perturbation bounds.
</p>

### Title: Reconstructing Signing Avatars From Video Using Linguistic Priors. (arXiv:2304.10482v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10482](http://arxiv.org/abs/2304.10482)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10482] Reconstructing Signing Avatars From Video Using Linguistic Priors](http://arxiv.org/abs/2304.10482) #noise`
* Summary: <p>Sign language (SL) is the primary method of communication for the 70 million
Deaf people around the world. Video dictionaries of isolated signs are a core
SL learning tool. Replacing these with 3D avatars can aid learning and enable
AR/VR applications, improving access to technology and online media. However,
little work has attempted to estimate expressive 3D avatars from SL video;
occlusion, noise, and motion blur make this task difficult. We address this by
introducing novel linguistic priors that are universally applicable to SL and
provide constraints on 3D hand pose that help resolve ambiguities within
isolated signs. Our method, SGNify, captures fine-grained hand pose, facial
expression, and body movement fully automatically from in-the-wild monocular SL
videos. We evaluate SGNify quantitatively by using a commercial motion-capture
system to compute 3D avatars synchronized with monocular video. SGNify
outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL
videos. A perceptual study shows that SGNify's 3D reconstructions are
significantly more comprehensible and natural than those of previous methods
and are on par with the source videos. Code and data are available at
$\href{<a href="http://sgnify.is.tue.mpg.de">this http URL</a>}{\text{sgnify.is.tue.mpg.de}}$.
</p>

### Title: Censoring chemical data to mitigate dual use risk. (arXiv:2304.10510v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.10510](http://arxiv.org/abs/2304.10510)
* Code URL: [https://github.com/ur-whitelab/chem-dual-use](https://github.com/ur-whitelab/chem-dual-use)
* Copy Paste: `<input type="checkbox">[[2304.10510] Censoring chemical data to mitigate dual use risk](http://arxiv.org/abs/2304.10510) #noise`
* Summary: <p>The dual use of machine learning applications, where models can be used for
both beneficial and malicious purposes, presents a significant challenge. This
has recently become a particular concern in chemistry, where chemical datasets
containing sensitive labels (e.g. toxicological information) could be used to
develop predictive models that identify novel toxins or chemical warfare
agents. To mitigate dual use risks, we propose a model-agnostic method of
selectively noising datasets while preserving the utility of the data for
training deep neural networks in a beneficial region. We evaluate the
effectiveness of the proposed method across least squares, a multilayer
perceptron, and a graph neural network. Our findings show selectively noised
datasets can induce model variance and bias in predictions for sensitive labels
with control, suggesting the safe sharing of datasets containing sensitive
information is feasible. We also find omitting sensitive data often increases
model variance sufficiently to mitigate dual use. This work is proposed as a
foundation for future research on enabling more secure and collaborative data
sharing practices and safer machine learning applications in chemistry.
</p>

### Title: SREL: Severity Rating Ensemble Learning for Non-Destructive Fault Diagnosis of Cu Interconnects using S-parameter Patterns. (arXiv:2304.10207v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.10207](http://arxiv.org/abs/2304.10207)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10207] SREL: Severity Rating Ensemble Learning for Non-Destructive Fault Diagnosis of Cu Interconnects using S-parameter Patterns](http://arxiv.org/abs/2304.10207) #noise`
* Summary: <p>As operating frequencies and clock speeds in processors have increased over
the years, interconnects affect both the reliability and performance of entire
electronic systems. Fault detection and diagnosis of the interconnects are
crucial for prognostics and health management (PHM) of electronics. However,
existing research works utilizing electrical signals as prognostic factors have
limitations, such as the inability to distinguish the root cause of defects,
which eventually requires additional destructive evaluation, and vulnerability
to noise that results in a false alarm. Herein, we realize the non-destructive
detection and diagnosis of defects in Cu interconnects, achieving early
detection, high diagnostic accuracy, and noise robustness. To the best of our
knowledge, this study first simultaneously analyzes the root cause and severity
using electrical signal patterns. In this paper, we experimentally show that
S-parameter patterns have the ability for fault diagnosis and they are
effective input data for learning algorithms. Furthermore, we propose a novel
severity rating ensemble learning (SREL) approach to enhance diagnostic
accuracy and noise-robustness. Our method, with a maximum accuracy of 99.3%,
outperforms conventional machine learning and multi-class convolutional neural
networks (CNN) as additional noise levels increase.
</p>

## diffusion
### Title: A data augmentation perspective on diffusion models and retrieval. (arXiv:2304.10253v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10253](http://arxiv.org/abs/2304.10253)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10253] A data augmentation perspective on diffusion models and retrieval](http://arxiv.org/abs/2304.10253) #diffusion`
* Summary: <p>Diffusion models excel at generating photorealistic images from text-queries.
Naturally, many approaches have been proposed to use these generative abilities
to augment training datasets for downstream tasks, such as classification.
However, diffusion models are themselves trained on large noisily supervised,
but nonetheless, annotated datasets. It is an open question whether the
generalization capabilities of diffusion models beyond using the additional
data of the pre-training process for augmentation lead to improved downstream
performance. We perform a systematic evaluation of existing methods to generate
images from diffusion models and study new extensions to assess their benefit
for data augmentation. While we find that personalizing diffusion models
towards the target data outperforms simpler prompting strategies, we also show
that using the training data of the diffusion model alone, via a simple nearest
neighbor retrieval procedure, leads to even stronger downstream performance.
Overall, our study probes the limitations of diffusion models for data
augmentation but also highlights its potential in generating new training data
to improve performance on simple downstream vision tasks.
</p>

### Title: Anything-3D: Towards Single-view Anything Reconstruction in the Wild. (arXiv:2304.10261v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10261](http://arxiv.org/abs/2304.10261)
* Code URL: [https://github.com/anything-of-anything/anything-3d](https://github.com/anything-of-anything/anything-3d)
* Copy Paste: `<input type="checkbox">[[2304.10261] Anything-3D: Towards Single-view Anything Reconstruction in the Wild](http://arxiv.org/abs/2304.10261) #diffusion`
* Summary: <p>3D reconstruction from a single-RGB image in unconstrained real-world
scenarios presents numerous challenges due to the inherent diversity and
complexity of objects and environments. In this paper, we introduce
Anything-3D, a methodical framework that ingeniously combines a series of
visual-language models and the Segment-Anything object segmentation model to
elevate objects to 3D, yielding a reliable and versatile system for single-view
conditioned 3D reconstruction task. Our approach employs a BLIP model to
generate textural descriptions, utilizes the Segment-Anything model for the
effective extraction of objects of interest, and leverages a text-to-image
diffusion model to lift object into a neural radiance field. Demonstrating its
ability to produce accurate and detailed 3D reconstructions for a wide array of
objects, \emph{Anything-3D\footnotemark[2]} shows promise in addressing the
limitations of existing methodologies. Through comprehensive experiments and
evaluations on various datasets, we showcase the merits of our approach,
underscoring its potential to contribute meaningfully to the field of 3D
reconstruction. Demos and code will be available at
\href{https://github.com/Anything-of-anything/Anything-3D}{https://github.com/Anything-of-anything/Anything-3D}.
</p>

### Title: Collaborative Diffusion for Multi-Modal Face Generation and Editing. (arXiv:2304.10530v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10530](http://arxiv.org/abs/2304.10530)
* Code URL: [https://github.com/ziqihuangg/collaborative-diffusion](https://github.com/ziqihuangg/collaborative-diffusion)
* Copy Paste: `<input type="checkbox">[[2304.10530] Collaborative Diffusion for Multi-Modal Face Generation and Editing](http://arxiv.org/abs/2304.10530) #diffusion`
* Summary: <p>Diffusion models arise as a powerful generative tool recently. Despite the
great progress, existing diffusion models mainly focus on uni-modal control,
i.e., the diffusion process is driven by only one modality of condition. To
further unleash the users' creativity, it is desirable for the model to be
controllable by multiple modalities simultaneously, e.g., generating and
editing faces by describing the age (text-driven) while drawing the face shape
(mask-driven). In this work, we present Collaborative Diffusion, where
pre-trained uni-modal diffusion models collaborate to achieve multi-modal face
generation and editing without re-training. Our key insight is that diffusion
models driven by different modalities are inherently complementary regarding
the latent denoising steps, where bilateral connections can be established
upon. Specifically, we propose dynamic diffuser, a meta-network that adaptively
hallucinates multi-modal denoising steps by predicting the spatial-temporal
influence functions for each pre-trained uni-modal model. Collaborative
Diffusion not only collaborates generation capabilities from uni-modal
diffusion models, but also integrates multiple uni-modal manipulations to
perform multi-modal editing. Extensive qualitative and quantitative experiments
demonstrate the superiority of our framework in both image quality and
condition consistency.
</p>

### Title: Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs. (arXiv:2304.10532v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10532](http://arxiv.org/abs/2304.10532)
* Code URL: [https://github.com/ethanweber/nerfbusters](https://github.com/ethanweber/nerfbusters)
* Copy Paste: `<input type="checkbox">[[2304.10532] Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs](http://arxiv.org/abs/2304.10532) #diffusion`
* Summary: <p>Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts such
as floaters or flawed geometry when rendered outside the camera trajectory.
Existing evaluation protocols often do not capture these effects, since they
usually only assess image quality at every 8th frame of the training capture.
To push forward progress in novel-view synthesis, we propose a new dataset and
evaluation procedure, where two camera trajectories are recorded of the scene:
one used for training, and the other for evaluation. In this more challenging
in-the-wild setting, we find that existing hand-crafted regularizers do not
remove floaters nor improve scene geometry. Thus, we propose a 3D
diffusion-based method that leverages local 3D priors and a novel density-based
score distillation sampling loss to discourage artifacts during NeRF
optimization. We show that this data-driven prior removes floaters and improves
scene geometry for casual captures.
</p>

### Title: Farm3D: Learning Articulated 3D Animals by Distilling 2D Diffusion. (arXiv:2304.10535v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10535](http://arxiv.org/abs/2304.10535)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10535] Farm3D: Learning Articulated 3D Animals by Distilling 2D Diffusion](http://arxiv.org/abs/2304.10535) #diffusion`
* Summary: <p>We present Farm3D, a method to learn category-specific 3D reconstructors for
articulated objects entirely from "free" virtual supervision from a pre-trained
2D diffusion-based image generator. Recent approaches can learn, given a
collection of single-view images of an object category, a monocular network to
predict the 3D shape, albedo, illumination and viewpoint of any object
occurrence. We propose a framework using an image generator like Stable
Diffusion to generate virtual training data for learning such a reconstruction
network from scratch. Furthermore, we include the diffusion model as a score to
further improve learning. The idea is to randomise some aspects of the
reconstruction, such as viewpoint and illumination, generating synthetic views
of the reconstructed 3D object, and have the 2D network assess the quality of
the resulting image, providing feedback to the reconstructor. Different from
work based on distillation which produces a single 3D asset for each textual
prompt in hours, our approach produces a monocular reconstruction network that
can output a controllable 3D asset from a given image, real or generated, in
only seconds. Our network can be used for analysis, including monocular
reconstruction, or for synthesis, generating articulated assets for real-time
applications such as video games.
</p>

### Title: Prediction of the evolution of the nuclear reactor core parameters using artificial neural network. (arXiv:2304.10337v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.10337](http://arxiv.org/abs/2304.10337)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10337] Prediction of the evolution of the nuclear reactor core parameters using artificial neural network](http://arxiv.org/abs/2304.10337) #diffusion`
* Summary: <p>A nuclear reactor based on MIT BEAVRS benchmark was used as a typical power
generating Pressurized Water Reactor (PWR). The PARCS v3.2 nodal-diffusion core
simulator was used as a full-core reactor physics solver to emulate the
operation of a reactor and to generate training, and validation data for the
ANN. The ANN was implemented with dedicated Python 3.8 code with Google's
TensorFlow 2.0 library. The effort was based to a large extent on the process
of appropriate automatic transformation of data generated by PARCS simulator,
which was later used in the process of the ANN development. Various methods
that allow obtaining better accuracy of the ANN predicted results were studied,
such as trying different ANN architectures to find the optimal number of
neurons in the hidden layers of the network. Results were later compared with
the architectures proposed in the literature. For the selected best
architecture predictions were made for different core parameters and their
dependence on core loading patterns. In this study, a special focus was put on
the prediction of the fuel cycle length for a given core loading pattern, as it
can be considered one of the targets for plant economic operation. For
instance, the length of a single fuel cycle depending on the initial core
loading pattern was predicted with very good accuracy (&gt;99%). This work
contributes to the exploration of the usefulness of neural networks in solving
nuclear reactor design problems. Thanks to the application of ANN, designers
can avoid using an excessive amount of core simulator runs and more rapidly
explore the space of possible solutions before performing more detailed design
considerations.
</p>

## LLM
## segmentation
### Title: MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation. (arXiv:2304.09913v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.09913](http://arxiv.org/abs/2304.09913)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.09913] MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation](http://arxiv.org/abs/2304.09913) #segmentation`
* Summary: <p>Weakly-supervised semantic segmentation aims to reduce labeling costs by
training semantic segmentation models using weak supervision, such as
image-level class labels. However, most approaches struggle to produce accurate
localization maps and suffer from false predictions in class-related
backgrounds (i.e., biased objects), such as detecting a railroad with the train
class. Recent methods that remove biased objects require additional supervision
for manually identifying biased objects for each problematic class and
collecting their datasets by reviewing predictions, limiting their
applicability to the real-world dataset with multiple labels and complex
relationships for biasing. Following the first observation that biased features
can be separated and eliminated by matching biased objects with backgrounds in
the same dataset, we propose a fully-automatic/model-agnostic biased removal
framework called MARS (Model-Agnostic biased object Removal without additional
Supervision), which utilizes semantically consistent features of an
unsupervised technique to eliminate biased objects in pseudo labels.
Surprisingly, we show that MARS achieves new state-of-the-art results on two
popular benchmarks, PASCAL VOC 2012 (val: 77.7%, test: 77.2%) and MS COCO 2014
(val: 49.4%), by consistently improving the performance of various WSSS models
by at least 30% without additional supervision.
</p>

### Title: Learning Temporal Distribution and Spatial Correlation for Universal Moving Object Segmentation. (arXiv:2304.09949v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.09949](http://arxiv.org/abs/2304.09949)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.09949] Learning Temporal Distribution and Spatial Correlation for Universal Moving Object Segmentation](http://arxiv.org/abs/2304.09949) #segmentation`
* Summary: <p>Universal moving object segmentation aims to provide a general model for
videos from all types of natural scenes, as previous approaches are usually
effective for specific or similar scenes. In this paper, we propose a method
called Learning Temporal Distribution and Spatial Correlation (LTS) that has
the potential to be a general solution for universal moving object
segmentation. In the proposed approach, the distribution from temporal pixels
is first learned by our Defect Iterative Distribution Learning (DIDL) network
for a scene-independent segmentation. Then, the Stochastic Bayesian Refinement
(SBR) Network, which learns the spatial correlation, is proposed to improve the
binary mask generated by the DIDL network. Benefiting from the scene
independence of the temporal distribution and the accuracy improvement
resulting from the spatial correlation, the proposed approach performs well for
almost all videos from diverse and complex natural scenes with fixed
parameters. Comprehensive experiments on standard datasets including LASIESTA,
CDNet2014, BMC, SBMI2015 and 128 real world videos demonstrate the superiority
of proposed approach compared to state-of-the-art methods with or without the
use of deep learning networks. To the best of our knowledge, this work has high
potential to be a general solution for moving object segmentation in real world
environments.
</p>

### Title: SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery. (arXiv:2304.09974v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.09974](http://arxiv.org/abs/2304.09974)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.09974] SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery](http://arxiv.org/abs/2304.09974) #segmentation`
* Summary: <p>Advances in GPT-based large language models (LLMs) are revolutionizing
natural language processing, exponentially increasing its use across various
domains. Incorporating uni-directional attention, these autoregressive LLMs can
generate long and coherent paragraphs. However, for visual question answering
(VQA) tasks that require both vision and language processing, models with
bi-directional attention or models employing fusion techniques are often
employed to capture the context of multiple modalities all at once. As GPT does
not natively process vision tokens, to exploit the advancements in GPT models
for VQA in robotic surgery, we design an end-to-end trainable Language-Vision
GPT (LV-GPT) model that expands the GPT2 model to include vision input (image).
The proposed LV-GPT incorporates a feature extractor (vision tokenizer) and
vision token embedding (token type and pose). Given the limitations of
unidirectional attention in GPT models and their ability to generate coherent
long paragraphs, we carefully sequence the word tokens before vision tokens,
mimicking the human thought process of understanding the question to infer an
answer from an image. Quantitatively, we prove that the LV-GPT model
outperforms other state-of-the-art VQA models on two publically available
surgical-VQA datasets (based on endoscopic vision challenge robotic scene
segmentation 2018 and CholecTriplet2021) and on our newly annotated dataset
(based on the holistic surgical scene dataset). We further annotate all three
datasets to include question-type annotations to allow sub-type analysis.
Furthermore, we extensively study and present the effects of token sequencing,
token type and pose embedding for vision tokens in the LV-GPT model.
</p>

### Title: Analyzing the Domain Shift Immunity of Deep Homography Estimation. (arXiv:2304.09976v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.09976](http://arxiv.org/abs/2304.09976)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.09976] Analyzing the Domain Shift Immunity of Deep Homography Estimation](http://arxiv.org/abs/2304.09976) #segmentation`
* Summary: <p>Homography estimation is a basic image-alignment method in many applications.
Recently, with the development of convolutional neural networks (CNNs), some
learning based approaches have shown great success in this task. However, the
performance across different domains has never been researched. Unlike other
common tasks (\eg, classification, detection, segmentation), CNN based
homography estimation models show a domain shift immunity, which means a model
can be trained on one dataset and tested on another without any transfer
learning. To explain this unusual performance, we need to determine how CNNs
estimate homography. In this study, we first show the domain shift immunity of
different deep homography estimation models. We then use a shallow network with
a specially designed dataset to analyze the features used for estimation. The
results show that networks use low-level texture information to estimate
homography. We also design some experiments to compare the performance between
different texture densities and image features distorted on some common
datasets to demonstrate our findings. Based on these findings, we provide an
explanation of the domain shift immunity of deep homography estimation.
</p>

### Title: Clustered-patch Element Connection for Few-shot Learning. (arXiv:2304.10093v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10093](http://arxiv.org/abs/2304.10093)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10093] Clustered-patch Element Connection for Few-shot Learning](http://arxiv.org/abs/2304.10093) #segmentation`
* Summary: <p>Weak feature representation problem has influenced the performance of
few-shot classification task for a long time. To alleviate this problem, recent
researchers build connections between support and query instances through
embedding patch features to generate discriminative representations. However,
we observe that there exists semantic mismatches (foreground/ background) among
these local patches, because the location and size of the target object are not
fixed. What is worse, these mismatches result in unreliable similarity
confidences, and complex dense connection exacerbates the problem. According to
this, we propose a novel Clustered-patch Element Connection (CEC) layer to
correct the mismatch problem. The CEC layer leverages Patch Cluster and Element
Connection operations to collect and establish reliable connections with high
similarity patch features, respectively. Moreover, we propose a CECNet,
including CEC layer based attention module and distance metric. The former is
utilized to generate a more discriminative representation benefiting from the
global clustered-patch features, and the latter is introduced to reliably
measure the similarity between pair-features. Extensive experiments demonstrate
that our CECNet outperforms the state-of-the-art methods on classification
benchmark. Furthermore, our CEC approach can be extended into few-shot
segmentation and detection tasks, which achieves competitive performances.
</p>

### Title: Ensembling Instance and Semantic Segmentation for Panoptic Segmentation. (arXiv:2304.10326v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10326](http://arxiv.org/abs/2304.10326)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10326] Ensembling Instance and Semantic Segmentation for Panoptic Segmentation](http://arxiv.org/abs/2304.10326) #segmentation`
* Summary: <p>We demonstrate our solution for the 2019 COCO panoptic segmentation task. Our
method first performs instance segmentation and semantic segmentation
separately, then combines the two to generate panoptic segmentation results. To
enhance the performance, we add several expert models of Mask R-CNN in instance
segmentation to tackle the data imbalance problem in the training data; also
HTC model is adopted yielding our best instance segmentation results. In
semantic segmentation, we trained several models with various backbones and use
an ensemble strategy which further boosts the segmentation results. In the end,
we analyze various combinations of instance and semantic segmentation, and
report on their performance for the final panoptic segmentation results. Our
best model achieves $PQ$ 47.1 on 2019 COCO panoptic test-dev data.
</p>

### Title: Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review. (arXiv:2304.10410v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10410](http://arxiv.org/abs/2304.10410)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10410] Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review](http://arxiv.org/abs/2304.10410) #segmentation`
* Summary: <p>Driven by deep learning techniques, perception technology in autonomous
driving has developed rapidly in recent years. To achieve accurate and robust
perception capabilities, autonomous vehicles are often equipped with multiple
sensors, making sensor fusion a crucial part of the perception system. Among
these fused sensors, radars and cameras enable a complementary and
cost-effective perception of the surrounding environment regardless of lighting
and weather conditions. This review aims to provide a comprehensive guideline
for radar-camera fusion, particularly concentrating on perception tasks related
to object detection and semantic segmentation. Based on the principles of the
radar and camera sensors, we delve into the data processing process and
representations, followed by an in-depth analysis and summary of radar-camera
fusion datasets. In the review of methodologies in radar-camera fusion, we
address interrogative questions, including "why to fuse", "what to fuse",
"where to fuse", "when to fuse", and "how to fuse", subsequently discussing
various challenges and potential research directions within this domain. To
ease the retrieval and comparison of datasets and fusion methods, we also
provide an interactive website:
https://XJTLU-VEC.github.io/Radar-Camera-Fusion.
</p>

### Title: Road Genome: A Topology Reasoning Benchmark for Scene Understanding in Autonomous Driving. (arXiv:2304.10440v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10440](http://arxiv.org/abs/2304.10440)
* Code URL: [https://github.com/OpenDriveLab/OpenLane-V2](https://github.com/OpenDriveLab/OpenLane-V2)
* Copy Paste: `<input type="checkbox">[[2304.10440] Road Genome: A Topology Reasoning Benchmark for Scene Understanding in Autonomous Driving](http://arxiv.org/abs/2304.10440) #segmentation`
* Summary: <p>Understanding the complex traffic environment is crucial for self-driving
vehicles. Existing benchmarks in autonomous driving mainly cast scene
understanding as perception problems, e.g., perceiving lanelines with vanilla
detection or segmentation methods. As such, we argue that the perception
pipeline provides limited information for autonomous vehicles to drive in the
right way, especially without the aid of high-definition (HD) map. For
instance, following the wrong traffic signal at a complicated crossroad would
lead to a catastrophic incident. By introducing Road Genome (OpenLane-V2), we
intend to shift the community's attention and take a step further beyond
perception - to the task of topology reasoning for scene structure. The goal of
Road Genome is to understand the scene structure by investigating the
relationship of perceived entities among traffic elements and lanes. Built on
top of prevailing datasets, the newly minted benchmark comprises 2,000
sequences of multi-view images captured from diverse real-world scenarios. We
annotate data with high-quality manual checks in the loop. Three subtasks
compromise the gist of Road Genome, including the 3D lane detection inherited
from OpenLane. We have/will host Challenges in the upcoming future at
top-tiered venues.
</p>

### Title: Securing Neural Networks with Knapsack Optimization. (arXiv:2304.10442v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10442](http://arxiv.org/abs/2304.10442)
* Code URL: [https://github.com/yg320/secure_inference](https://github.com/yg320/secure_inference)
* Copy Paste: `<input type="checkbox">[[2304.10442] Securing Neural Networks with Knapsack Optimization](http://arxiv.org/abs/2304.10442) #segmentation`
* Summary: <p>Deep learning inference brings together the data and the Convolutional Neural
Network (CNN). This is problematic in case the user wants to preserve the
privacy of the data and the service provider does not want to reveal the
weights of his CNN. Secure Inference allows the two parties to engage in a
protocol that preserves their respective privacy concerns, while revealing only
the inference result to the user. This is known as Multi-Party Computation
(MPC). A major bottleneck of MPC algorithms is communication, as the parties
must send data back and forth. The linear component of a CNN (i.e.
convolutions) can be done efficiently with minimal communication, but the
non-linear part (i.e., ReLU) requires the bulk of communication bandwidth. We
propose two ways to accelerate Secure Inference. The first is based on the
observation that the ReLU outcome of many convolutions is highly correlated.
Therefore, we replace the per pixel ReLU operation by a ReLU operation per
patch. Each layer in the network will benefit from a patch of a different size
and we devise an algorithm to choose the optimal set of patch sizes through a
novel reduction of the problem to a knapsack problem. The second way to
accelerate Secure Inference is based on cutting the number of bit comparisons
required for a secure ReLU operation. We demonstrate the cumulative effect of
these tools in the semi-honest secure 3-party setting for four problems:
Classifying ImageNet using ResNet50 backbone, classifying CIFAR100 using
ResNet18 backbone, semantic segmentation of ADE20K using MobileNetV2 backbone
and semantic segmentation of Pascal VOC 2012 using ResNet50 backbone. Our
source code is publicly available:
$\href{https://github.com/yg320/secure_inference}{\text{https://github.com/yg320/secure_inference}}$
</p>

### Title: Segment Anything Model for Medical Image Analysis: an Experimental Study. (arXiv:2304.10517v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.10517](http://arxiv.org/abs/2304.10517)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.10517] Segment Anything Model for Medical Image Analysis: an Experimental Study](http://arxiv.org/abs/2304.10517) #segmentation`
* Summary: <p>Training segmentation models for medical images continues to be challenging
due to the limited availability and acquisition expense of data annotations.
Segment Anything Model (SAM) is a foundation model trained on over 1 billion
annotations, predominantly for natural images, that is intended to be able to
segment the user-defined object of interest in an interactive manner. Despite
its impressive performance on natural images, it is unclear how the model is
affected when shifting to medical image domains. Here, we perform an extensive
evaluation of SAM's ability to segment medical images on a collection of 11
medical imaging datasets from various modalities and anatomies. In our
experiments, we generated point prompts using a standard method that simulates
interactive segmentation. Experimental results show that SAM's performance
based on single prompts highly varies depending on the task and the dataset,
i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset,
evaluated by IoU. Performance appears to be high for tasks including
well-circumscribed objects with unambiguous prompts and poorer in many other
scenarios such as segmentation of tumors. When multiple prompts are provided,
performance improves only slightly overall, but more so for datasets where the
object is not contiguous. An additional comparison to RITM showed a much better
performance of SAM for one prompt but a similar performance of the two methods
for a larger number of prompts. We conclude that SAM shows impressive
performance for some datasets given the zero-shot learning setup but poor to
moderate performance for multiple other datasets. While SAM as a model and as a
learning paradigm might be impactful in the medical imaging domain, extensive
research is needed to identify the proper ways of adapting it in this domain.
</p>

## object detection
