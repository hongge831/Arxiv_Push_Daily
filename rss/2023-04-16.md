## data-free
## transformer
### Title: PATMAT: Person Aware Tuning of Mask-Aware Transformer for Face Inpainting. (arXiv:2304.06107v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06107](http://arxiv.org/abs/2304.06107)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06107] PATMAT: Person Aware Tuning of Mask-Aware Transformer for Face Inpainting](http://arxiv.org/abs/2304.06107) #transformer`
* Summary: <p>Generative models such as StyleGAN2 and Stable Diffusion have achieved
state-of-the-art performance in computer vision tasks such as image synthesis,
inpainting, and de-noising. However, current generative models for face
inpainting often fail to preserve fine facial details and the identity of the
person, despite creating aesthetically convincing image structures and
textures. In this work, we propose Person Aware Tuning (PAT) of Mask-Aware
Transformer (MAT) for face inpainting, which addresses this issue. Our proposed
method, PATMAT, effectively preserves identity by incorporating reference
images of a subject and fine-tuning a MAT architecture trained on faces. By
using ~40 reference images, PATMAT creates anchor points in MAT's style module,
and tunes the model using the fixed anchors to adapt the model to a new face
identity. Moreover, PATMAT's use of multiple images per anchor during training
allows the model to use fewer reference images than competing methods. We
demonstrate that PATMAT outperforms state-of-the-art models in terms of image
quality, the preservation of person-specific details, and the identity of the
subject. Our results suggest that PATMAT can be a promising approach for
improving the quality of personalized face inpainting.
</p>

### Title: AutoShot: A Short Video Dataset and State-of-the-Art Shot Boundary Detection. (arXiv:2304.06116v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06116](http://arxiv.org/abs/2304.06116)
* Code URL: [https://github.com/wentaozhu/AutoShot](https://github.com/wentaozhu/AutoShot)
* Copy Paste: `<input type="checkbox">[[2304.06116] AutoShot: A Short Video Dataset and State-of-the-Art Shot Boundary Detection](http://arxiv.org/abs/2304.06116) #transformer`
* Summary: <p>The short-form videos have explosive popularity and have dominated the new
social media trends. Prevailing short-video platforms,~\textit{e.g.}, Kuaishou
(Kwai), TikTok, Instagram Reels, and YouTube Shorts, have changed the way we
consume and create content. For video content creation and understanding, the
shot boundary detection (SBD) is one of the most essential components in
various scenarios. In this work, we release a new public Short video sHot
bOundary deTection dataset, named SHOT, consisting of 853 complete short videos
and 11,606 shot annotations, with 2,716 high quality shot boundary annotations
in 200 test videos. Leveraging this new data wealth, we propose to optimize the
model design for video SBD, by conducting neural architecture search in a
search space encapsulating various advanced 3D ConvNets and Transformers. Our
proposed approach, named AutoShot, achieves higher F1 scores than previous
state-of-the-art approaches, e.g., outperforming TransNetV2 by 4.2%, when being
derived and evaluated on our newly constructed SHOT dataset. Moreover, to
validate the generalizability of the AutoShot architecture, we directly
evaluate it on another three public datasets: ClipShots, BBC and RAI, and the
F1 scores of AutoShot outperform previous state-of-the-art approaches by 1.1%,
0.9% and 1.2%, respectively. The SHOT dataset and code can be found in
https://github.com/wentaozhu/AutoShot.git .
</p>

### Title: Towards Evaluating Explanations of Vision Transformers for Medical Imaging. (arXiv:2304.06133v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06133](http://arxiv.org/abs/2304.06133)
* Code URL: [https://github.com/piotr-komorowski/towards-evaluating-explanations-of-vit](https://github.com/piotr-komorowski/towards-evaluating-explanations-of-vit)
* Copy Paste: `<input type="checkbox">[[2304.06133] Towards Evaluating Explanations of Vision Transformers for Medical Imaging](http://arxiv.org/abs/2304.06133) #transformer`
* Summary: <p>As deep learning models increasingly find applications in critical domains
such as medical imaging, the need for transparent and trustworthy
decision-making becomes paramount. Many explainability methods provide insights
into how these models make predictions by attributing importance to input
features. As Vision Transformer (ViT) becomes a promising alternative to
convolutional neural networks for image classification, its interpretability
remains an open research question. This paper investigates the performance of
various interpretation methods on a ViT applied to classify chest X-ray images.
We introduce the notion of evaluating faithfulness, sensitivity, and complexity
of ViT explanations. The obtained results indicate that Layerwise relevance
propagation for transformers outperforms Local interpretable model-agnostic
explanations and Attention visualization, providing a more accurate and
reliable representation of what a ViT has actually learned. Our findings
provide insights into the applicability of ViT explanations in medical imaging
and highlight the importance of using appropriate evaluation criteria for
comparing them.
</p>

### Title: RSIR Transformer: Hierarchical Vision Transformer using Random Sampling Windows and Important Region Windows. (arXiv:2304.06250v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06250](http://arxiv.org/abs/2304.06250)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06250] RSIR Transformer: Hierarchical Vision Transformer using Random Sampling Windows and Important Region Windows](http://arxiv.org/abs/2304.06250) #transformer`
* Summary: <p>Recently, Transformers have shown promising performance in various vision
tasks. However, the high costs of global self-attention remain challenging for
Transformers, especially for high-resolution vision tasks. Local self-attention
runs attention computation within a limited region for the sake of efficiency,
resulting in insufficient context modeling as their receptive fields are small.
In this work, we introduce two new attention modules to enhance the global
modeling capability of the hierarchical vision transformer, namely, random
sampling windows (RS-Win) and important region windows (IR-Win). Specifically,
RS-Win sample random image patches to compose the window, following a uniform
distribution, i.e., the patches in RS-Win can come from any position in the
image. IR-Win composes the window according to the weights of the image patches
in the attention map. Notably, RS-Win is able to capture global information
throughout the entire model, even in earlier, high-resolution stages. IR-Win
enables the self-attention module to focus on important regions of the image
and capture more informative features. Incorporated with these designs,
RSIR-Win Transformer demonstrates competitive performance on common vision
tasks.
</p>

### Title: EWT: Efficient Wavelet-Transformer for Single Image Denoising. (arXiv:2304.06274v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06274](http://arxiv.org/abs/2304.06274)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06274] EWT: Efficient Wavelet-Transformer for Single Image Denoising](http://arxiv.org/abs/2304.06274) #transformer`
* Summary: <p>Transformer-based image denoising methods have achieved encouraging results
in the past year. However, it must uses linear operations to model long-range
dependencies, which greatly increases model inference time and consumes GPU
storage space. Compared with convolutional neural network-based methods,
current Transformer-based image denoising methods cannot achieve a balance
between performance improvement and resource consumption. In this paper, we
propose an Efficient Wavelet Transformer (EWT) for image denoising.
Specifically, we use Discrete Wavelet Transform (DWT) and Inverse Wavelet
Transform (IWT) for downsampling and upsampling, respectively. This method can
fully preserve the image features while reducing the image resolution, thereby
greatly reducing the device resource consumption of the Transformer model.
Furthermore, we propose a novel Dual-stream Feature Extraction Block (DFEB) to
extract image features at different levels, which can further reduce model
inference time and GPU memory usage. Experiments show that our method speeds up
the original Transformer by more than 80%, reduces GPU memory usage by more
than 60%, and achieves excellent denoising results. All code will be public.
</p>

### Title: Efficient Multimodal Fusion via Interactive Prompting. (arXiv:2304.06306v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06306](http://arxiv.org/abs/2304.06306)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06306] Efficient Multimodal Fusion via Interactive Prompting](http://arxiv.org/abs/2304.06306) #transformer`
* Summary: <p>Large-scale pre-training has brought unimodal fields such as computer vision
and natural language processing to a new era. Following this trend, the size of
multi-modal learning models constantly increases, leading to an urgent need to
reduce the massive computational cost of finetuning these models for downstream
tasks. In this paper, we propose an efficient and flexible multimodal fusion
method, namely PMF, tailored for fusing unimodally pre-trained transformers.
Specifically, we first present a modular multimodal fusion framework that
exhibits high flexibility and facilitates mutual interactions among different
modalities. In addition, we disentangle vanilla prompts into three types in
order to learn different optimizing objectives for multimodal learning. It is
also worth noting that we propose to add prompt vectors only on the deep layers
of the unimodal transformers, thus significantly reducing the training memory
usage. Experiment results show that our proposed method achieves comparable
performance to several other multimodal finetuning methods with less than 3%
trainable parameters and up to 66% saving of training memory usage.
</p>

### Title: DDT: Dual-branch Deformable Transformer for Image Denoising. (arXiv:2304.06346v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06346](http://arxiv.org/abs/2304.06346)
* Code URL: [https://github.com/merenguelkl/ddt](https://github.com/merenguelkl/ddt)
* Copy Paste: `<input type="checkbox">[[2304.06346] DDT: Dual-branch Deformable Transformer for Image Denoising](http://arxiv.org/abs/2304.06346) #transformer`
* Summary: <p>Transformer is beneficial for image denoising tasks since it can model
long-range dependencies to overcome the limitations presented by inductive
convolutional biases. However, directly applying the transformer structure to
remove noise is challenging because its complexity grows quadratically with the
spatial resolution. In this paper, we propose an efficient Dual-branch
Deformable Transformer (DDT) denoising network which captures both local and
global interactions in parallel. We divide features with a fixed patch size and
a fixed number of patches in local and global branches, respectively. In
addition, we apply deformable attention operation in both branches, which helps
the network focus on more important regions and further reduces computational
complexity. We conduct extensive experiments on real-world and synthetic
denoising tasks, and the proposed DDT achieves state-of-the-art performance
with significantly fewer computational costs.
</p>

### Title: Sign Language Translation from Instructional Videos. (arXiv:2304.06371v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.06371](http://arxiv.org/abs/2304.06371)
* Code URL: [https://github.com/imatge-upc/slt_how2sign_wicv2023](https://github.com/imatge-upc/slt_how2sign_wicv2023)
* Copy Paste: `<input type="checkbox">[[2304.06371] Sign Language Translation from Instructional Videos](http://arxiv.org/abs/2304.06371) #transformer`
* Summary: <p>The advances in automatic sign language translation (SLT) to spoken languages
have been mostly benchmarked with datasets of limited size and restricted
domains. Our work advances the state of the art by providing the first baseline
results on How2Sign, a large and broad dataset.
</p>
<p>We train a Transformer over I3D video features, using the reduced BLEU as a
reference metric for validation, instead of the widely used BLEU score. We
report a result of 8.03 on the BLEU score, and publish the first open-source
implementation of its kind to promote further advances.
</p>

### Title: TransHP: Image Classification with Hierarchical Prompting. (arXiv:2304.06385v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06385](http://arxiv.org/abs/2304.06385)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06385] TransHP: Image Classification with Hierarchical Prompting](http://arxiv.org/abs/2304.06385) #transformer`
* Summary: <p>This paper explores a hierarchical prompting mechanism for the hierarchical
image classification (HIC) task. Different from prior HIC methods, our
hierarchical prompting is the first to explicitly inject ancestor-class
information as a tokenized hint that benefits the descendant-class
discrimination. We think it well imitates human visual recognition, i.e.,
humans may use the ancestor class as a prompt to draw focus on the subtle
differences among descendant classes. We model this prompting mechanism into a
Transformer with Hierarchical Prompting (TransHP). TransHP consists of three
steps: 1) learning a set of prompt tokens to represent the coarse (ancestor)
classes, 2) on-the-fly predicting the coarse class of the input image at an
intermediate block, and 3) injecting the prompt token of the predicted coarse
class into the intermediate feature. Though the parameters of TransHP maintain
the same for all input images, the injected coarse-class prompt conditions
(modifies) the subsequent feature extraction and encourages a dynamic focus on
relatively subtle differences among the descendant classes. Extensive
experiments show that TransHP improves image classification on accuracy (e.g.,
improving ViT-B/16 by +2.83% ImageNet classification accuracy), training data
efficiency (e.g., +12.69% improvement under 10% ImageNet training data), and
model explainability. Moreover, TransHP also performs favorably against prior
HIC methods, showing that TransHP well exploits the hierarchical information.
</p>

### Title: VISION DIFFMASK: Faithful Interpretation of Vision Transformers with Differentiable Patch Masking. (arXiv:2304.06391v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06391](http://arxiv.org/abs/2304.06391)
* Code URL: [https://github.com/angelosnal/vision-diffmask](https://github.com/angelosnal/vision-diffmask)
* Copy Paste: `<input type="checkbox">[[2304.06391] VISION DIFFMASK: Faithful Interpretation of Vision Transformers with Differentiable Patch Masking](http://arxiv.org/abs/2304.06391) #transformer`
* Summary: <p>The lack of interpretability of the Vision Transformer may hinder its use in
critical real-world applications despite its effectiveness. To overcome this
issue, we propose a post-hoc interpretability method called VISION DIFFMASK,
which uses the activations of the model's hidden layers to predict the relevant
parts of the input that contribute to its final predictions. Our approach uses
a gating mechanism to identify the minimal subset of the original input that
preserves the predicted distribution over classes. We demonstrate the
faithfulness of our method, by introducing a faithfulness task, and comparing
it to other state-of-the-art attribution methods on CIFAR-10 and ImageNet-1K,
achieving compelling results. To aid reproducibility and further extension of
our work, we open source our implementation:
https://github.com/AngelosNal/Vision-DiffMask
</p>

### Title: SpectFormer: Frequency and Attention is what you need in a Vision Transformer. (arXiv:2304.06446v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06446](http://arxiv.org/abs/2304.06446)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06446] SpectFormer: Frequency and Attention is what you need in a Vision Transformer](http://arxiv.org/abs/2304.06446) #transformer`
* Summary: <p>Vision transformers have been applied successfully for image recognition
tasks. There have been either multi-headed self-attention based (ViT
\cite{dosovitskiy2020image}, DeIT, \cite{touvron2021training}) similar to the
original work in textual models or more recently based on spectral layers
(Fnet\cite{lee2021fnet}, GFNet\cite{rao2021global},
AFNO\cite{guibas2021efficient}). We hypothesize that both spectral and
multi-headed attention plays a major role. We investigate this hypothesis
through this work and observe that indeed combining spectral and multi-headed
attention layers provides a better transformer architecture. We thus propose
the novel Spectformer architecture for transformers that combines spectral and
multi-headed attention layers. We believe that the resulting representation
allows the transformer to capture the feature representation appropriately and
it yields improved performance over other transformer representations. For
instance, it improves the top-1 accuracy by 2\% on ImageNet compared to both
GFNet-H and LiT. SpectFormer-S reaches 84.25\% top-1 accuracy on ImageNet-1K
(state of the art for small version). Further, Spectformer-L achieves 85.7\%
that is the state of the art for the comparable base version of the
transformers. We further ensure that we obtain reasonable results in other
scenarios such as transfer learning on standard datasets such as CIFAR-10,
CIFAR-100, Oxford-IIIT-flower, and Standford Car datasets. We then investigate
its use in downstream tasks such of object detection and instance segmentation
on the MS-COCO dataset and observe that Spectformer shows consistent
performance that is comparable to the best backbones and can be further
optimized and improved. Hence, we believe that combined spectral and attention
layers are what are needed for vision transformers.
</p>

### Title: NeRD: Neural field-based Demosaicking. (arXiv:2304.06566v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06566](http://arxiv.org/abs/2304.06566)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06566] NeRD: Neural field-based Demosaicking](http://arxiv.org/abs/2304.06566) #transformer`
* Summary: <p>We introduce NeRD, a new demosaicking method for generating full-color images
from Bayer patterns. Our approach leverages advancements in neural fields to
perform demosaicking by representing an image as a coordinate-based neural
network with sine activation functions. The inputs to the network are spatial
coordinates and a low-resolution Bayer pattern, while the outputs are the
corresponding RGB values. An encoder network, which is a blend of ResNet and
U-net, enhances the implicit neural representation of the image to improve its
quality and ensure spatial consistency through prior learning. Our experimental
results demonstrate that NeRD outperforms traditional and state-of-the-art
CNN-based methods and significantly closes the gap to transformer-based
methods.
</p>

### Title: DynaMITe: Dynamic Query Bootstrapping for Multi-object Interactive Segmentation Transformer. (arXiv:2304.06668v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06668](http://arxiv.org/abs/2304.06668)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06668] DynaMITe: Dynamic Query Bootstrapping for Multi-object Interactive Segmentation Transformer](http://arxiv.org/abs/2304.06668) #transformer`
* Summary: <p>Most state-of-the-art instance segmentation methods rely on large amounts of
pixel-precise ground-truth annotations for training, which are expensive to
create. Interactive segmentation networks help generate such annotations based
on an image and the corresponding user interactions such as clicks. Existing
methods for this task can only process a single instance at a time and each
user interaction requires a full forward pass through the entire deep network.
We introduce a more efficient approach, called DynaMITe, in which we represent
user interactions as spatio-temporal queries to a Transformer decoder with a
potential to segment multiple object instances in a single iteration. Our
architecture also alleviates any need to re-compute image features during
refinement, and requires fewer interactions for segmenting multiple instances
in a single image when compared to other methods. DynaMITe achieves
state-of-the-art results on multiple existing interactive segmentation
benchmarks, and also on the new multi-instance benchmark that we propose in
this paper.
</p>

### Title: Remote Sensing Change Detection With Transformers Trained from Scratch. (arXiv:2304.06710v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06710](http://arxiv.org/abs/2304.06710)
* Code URL: [https://github.com/mustansarfiaz/scratchformer](https://github.com/mustansarfiaz/scratchformer)
* Copy Paste: `<input type="checkbox">[[2304.06710] Remote Sensing Change Detection With Transformers Trained from Scratch](http://arxiv.org/abs/2304.06710) #transformer`
* Summary: <p>Current transformer-based change detection (CD) approaches either employ a
pre-trained model trained on large-scale image classification ImageNet dataset
or rely on first pre-training on another CD dataset and then fine-tuning on the
target benchmark. This current strategy is driven by the fact that transformers
typically require a large amount of training data to learn inductive biases,
which is insufficient in standard CD datasets due to their small size. We
develop an end-to-end CD approach with transformers that is trained from
scratch and yet achieves state-of-the-art performance on four public
benchmarks. Instead of using conventional self-attention that struggles to
capture inductive biases when trained from scratch, our architecture utilizes a
shuffled sparse-attention operation that focuses on selected sparse informative
regions to capture the inherent characteristics of the CD data. Moreover, we
introduce a change-enhanced feature fusion (CEFF) module to fuse the features
from input image pairs by performing a per-channel re-weighting. Our CEFF
module aids in enhancing the relevant semantic changes while suppressing the
noisy ones. Extensive experiments on four CD datasets reveal the merits of the
proposed contributions, achieving gains as high as 14.27\% in
intersection-over-union (IoU) score, compared to the best-published results in
the literature. Code is available at
\url{https://github.com/mustansarfiaz/ScratchFormer}.
</p>

## generative
### Title: ALR-GAN: Adaptive Layout Refinement for Text-to-Image Synthesis. (arXiv:2304.06297v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06297](http://arxiv.org/abs/2304.06297)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06297] ALR-GAN: Adaptive Layout Refinement for Text-to-Image Synthesis](http://arxiv.org/abs/2304.06297) #generative`
* Summary: <p>We propose a novel Text-to-Image Generation Network, Adaptive Layout
Refinement Generative Adversarial Network (ALR-GAN), to adaptively refine the
layout of synthesized images without any auxiliary information. The ALR-GAN
includes an Adaptive Layout Refinement (ALR) module and a Layout Visual
Refinement (LVR) loss. The ALR module aligns the layout structure (which refers
to locations of objects and background) of a synthesized image with that of its
corresponding real image. In ALR module, we proposed an Adaptive Layout
Refinement (ALR) loss to balance the matching of hard and easy features, for
more efficient layout structure matching. Based on the refined layout
structure, the LVR loss further refines the visual representation within the
layout area. Experimental results on two widely-used datasets show that ALR-GAN
performs competitively at the Text-to-Image generation task.
</p>

### Title: Intriguing properties of synthetic images: from generative adversarial networks to diffusion models. (arXiv:2304.06408v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06408](http://arxiv.org/abs/2304.06408)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06408] Intriguing properties of synthetic images: from generative adversarial networks to diffusion models](http://arxiv.org/abs/2304.06408) #generative`
* Summary: <p>Detecting fake images is becoming a major goal of computer vision. This need
is becoming more and more pressing with the continuous improvement of synthesis
methods based on Generative Adversarial Networks (GAN), and even more with the
appearance of powerful methods based on Diffusion Models (DM). Towards this
end, it is important to gain insight into which image features better
discriminate fake images from real ones. In this paper we report on our
systematic study of a large number of image generators of different families,
aimed at discovering the most forensically relevant characteristics of real and
generated images. Our experiments provide a number of interesting observations
and shed light on some intriguing properties of synthetic images: (1) not only
the GAN models but also the DM and VQ-GAN (Vector Quantized Generative
Adversarial Networks) models give rise to visible artifacts in the Fourier
domain and exhibit anomalous regular patterns in the autocorrelation; (2) when
the dataset used to train the model lacks sufficient variety, its biases can be
transferred to the generated images; (3) synthetic and real images exhibit
significant differences in the mid-high frequency signal content, observable in
their radial and angular spectral power distributions.
</p>

### Title: LasUIE: Unifying Information Extraction with Latent Adaptive Structure-aware Generative Language Model. (arXiv:2304.06248v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.06248](http://arxiv.org/abs/2304.06248)
* Code URL: [https://github.com/chocowu/lasuie](https://github.com/chocowu/lasuie)
* Copy Paste: `<input type="checkbox">[[2304.06248] LasUIE: Unifying Information Extraction with Latent Adaptive Structure-aware Generative Language Model](http://arxiv.org/abs/2304.06248) #generative`
* Summary: <p>Universally modeling all typical information extraction tasks (UIE) with one
generative language model (GLM) has revealed great potential by the latest
study, where various IE predictions are unified into a linearized hierarchical
expression under a GLM. Syntactic structure information, a type of effective
feature which has been extensively utilized in IE community, should also be
beneficial to UIE. In this work, we propose a novel structure-aware GLM, fully
unleashing the power of syntactic knowledge for UIE. A heterogeneous structure
inductor is explored to unsupervisedly induce rich heterogeneous structural
representations by post-training an existing GLM. In particular, a structural
broadcaster is devised to compact various latent trees into explicit high-order
forests, helping to guide a better generation during decoding. We finally
introduce a task-oriented structure fine-tuning mechanism, further adjusting
the learned structures to most coincide with the end-task's need. Over 12 IE
benchmarks across 7 tasks our system shows significant improvements over the
baseline UIE system. Further in-depth analyses show that our GLM learns rich
task-adaptive structural bias that greatly resolves the UIE crux, the
long-range dependence issue and boundary identifying. Source codes are open at
https://github.com/ChocoWu/LasUIE.
</p>

### Title: G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection. (arXiv:2304.06653v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.06653](http://arxiv.org/abs/2304.06653)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06653] G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection](http://arxiv.org/abs/2304.06653) #generative`
* Summary: <p>It has been reported that clustering-based topic models, which cluster
high-quality sentence embeddings with an appropriate word selection method, can
generate better topics than generative probabilistic topic models. However,
these approaches suffer from the inability to select appropriate parameters and
incomplete models that overlook the quantitative relation between words with
topics and topics with text. To solve these issues, we propose graph to topic
(G2T), a simple but effective framework for topic modelling. The framework is
composed of four modules. First, document representation is acquired using
pretrained language models. Second, a semantic graph is constructed according
to the similarity between document representations. Third, communities in
document semantic graphs are identified, and the relationship between topics
and documents is quantified accordingly. Fourth, the word--topic distribution
is computed based on a variant of TFIDF. Automatic evaluation suggests that G2T
achieved state-of-the-art performance on both English and Chinese documents
with different lengths. Human judgements demonstrate that G2T can produce
topics with better interpretability and coverage than baselines. In addition,
G2T can not only determine the topic number automatically but also give the
probabilistic distribution of words in topics and topics in documents. Finally,
G2T is publicly available, and the distillation experiments provide instruction
on how it works.
</p>

### Title: Energy-guided Entropic Neural Optimal Transport. (arXiv:2304.06094v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.06094](http://arxiv.org/abs/2304.06094)
* Code URL: [https://github.com/rosinality/igebm-pytorch](https://github.com/rosinality/igebm-pytorch)
* Copy Paste: `<input type="checkbox">[[2304.06094] Energy-guided Entropic Neural Optimal Transport](http://arxiv.org/abs/2304.06094) #generative`
* Summary: <p>Energy-Based Models (EBMs) are known in the Machine Learning community for
the decades. Since the seminal works devoted to EBMs dating back to the
noughties there have been appearing a lot of efficient methods which solve the
generative modelling problem by means of energy potentials (unnormalized
likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in
particular, neural OT solvers is much less explored and limited by few recent
works (excluding WGAN based approaches which utilize OT as a loss function and
do not model OT maps themselves). In our work, we bridge the gap between EBMs
and Entropy-regularized OT. We present the novel methodology which allows
utilizing the recent developments and technical improvements of the former in
order to enrich the latter. We validate the applicability of our method on toy
2D scenarios as well as standard unpaired image-to-image translation problems.
For the sake of simplicity, we choose simple short- and long- run EBMs as a
backbone of our Energy-guided Entropic OT method, leaving the application of
more sophisticated EBMs for future research.
</p>

### Title: Improving novelty detection with generative adversarial networks on hand gesture data. (arXiv:2304.06696v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.06696](http://arxiv.org/abs/2304.06696)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06696] Improving novelty detection with generative adversarial networks on hand gesture data](http://arxiv.org/abs/2304.06696) #generative`
* Summary: <p>We propose a novel way of solving the issue of classification of
out-of-vocabulary gestures using Artificial Neural Networks (ANNs) trained in
the Generative Adversarial Network (GAN) framework. A generative model augments
the data set in an online fashion with new samples and stochastic target
vectors, while a discriminative model determines the class of the samples. The
approach was evaluated on the UC2017 SG and UC2018 DualMyo data sets. The
generative models performance was measured with a distance metric between
generated and real samples. The discriminative models were evaluated by their
accuracy on trained and novel classes. In terms of sample generation quality,
the GAN is significantly better than a random distribution (noise) in mean
distance, for all classes. In the classification tests, the baseline neural
network was not capable of identifying untrained gestures. When the proposed
methodology was implemented, we found that there is a trade-off between the
detection of trained and untrained gestures, with some trained samples being
mistaken as novelty. Nevertheless, a novelty detection accuracy of 95.4% or
90.2% (depending on the data set) was achieved with just 5% loss of accuracy on
trained classes.
</p>

## label correction
## noise
### Title: An Edit Friendly DDPM Noise Space: Inversion and Manipulations. (arXiv:2304.06140v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06140](http://arxiv.org/abs/2304.06140)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06140] An Edit Friendly DDPM Noise Space: Inversion and Manipulations](http://arxiv.org/abs/2304.06140) #noise`
* Summary: <p>Denoising diffusion probabilistic models (DDPMs) employ a sequence of white
Gaussian noise samples to generate an image. In analogy with GANs, those noise
maps could be considered as the latent code associated with the generated
image. However, this native noise space does not possess a convenient
structure, and is thus challenging to work with in editing tasks. Here, we
propose an alternative latent noise space for DDPM that enables a wide range of
editing operations via simple means, and present an inversion method for
extracting these edit-friendly noise maps for any given image (real or
synthetically generated). As opposed to the native DDPM noise space, the
edit-friendly noise maps do not have a standard normal distribution and are not
statistically independent across timesteps. However, they allow perfect
reconstruction of any desired image, and simple transformations on them
translate into meaningful manipulations of the output image (e.g., shifting,
color edits). Moreover, in text-conditional models, fixing those noise maps
while changing the text prompt, modifies semantics while retaining structure.
We illustrate how this property enables text-based editing of real images via
the diverse DDPM sampling scheme (in contrast to the popular non-diverse DDIM
inversion). We also show how it can be used within existing diffusion-based
editing methods to improve their quality and diversity.
</p>

### Title: Noisy Correspondence Learning with Meta Similarity Correction. (arXiv:2304.06275v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06275](http://arxiv.org/abs/2304.06275)
* Code URL: [https://github.com/hhc1997/mscn](https://github.com/hhc1997/mscn)
* Copy Paste: `<input type="checkbox">[[2304.06275] Noisy Correspondence Learning with Meta Similarity Correction](http://arxiv.org/abs/2304.06275) #noise`
* Summary: <p>Despite the success of multimodal learning in cross-modal retrieval task, the
remarkable progress relies on the correct correspondence among multimedia data.
However, collecting such ideal data is expensive and time-consuming. In
practice, most widely used datasets are harvested from the Internet and
inevitably contain mismatched pairs. Training on such noisy correspondence
datasets causes performance degradation because the cross-modal retrieval
methods can wrongly enforce the mismatched data to be similar. To tackle this
problem, we propose a Meta Similarity Correction Network (MSCN) to provide
reliable similarity scores. We view a binary classification task as the
meta-process that encourages the MSCN to learn discrimination from positive and
negative meta-data. To further alleviate the influence of noise, we design an
effective data purification strategy using meta-data as prior knowledge to
remove the noisy samples. Extensive experiments are conducted to demonstrate
the strengths of our method in both synthetic and real-world noises, including
Flickr30K, MS-COCO, and Conceptual Captions.
</p>

### Title: Certified Zeroth-order Black-Box Defense with Robust UNet Denoiser. (arXiv:2304.06430v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06430](http://arxiv.org/abs/2304.06430)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06430] Certified Zeroth-order Black-Box Defense with Robust UNet Denoiser](http://arxiv.org/abs/2304.06430) #noise`
* Summary: <p>Certified defense methods against adversarial perturbations have been
recently investigated in the black-box setting with a zeroth-order (ZO)
perspective. However, these methods suffer from high model variance with low
performance on high-dimensional datasets due to the ineffective design of the
denoiser and are limited in their utilization of ZO techniques. To this end, we
propose a certified ZO preprocessing technique for removing adversarial
perturbations from the attacked image in the black-box setting using only model
queries. We propose a robust UNet denoiser (RDUNet) that ensures the robustness
of black-box models trained on high-dimensional datasets. We propose a novel
black-box denoised smoothing (DS) defense mechanism, ZO-RUDS, by prepending our
RDUNet to the black-box model, ensuring black-box defense. We further propose
ZO-AE-RUDS in which RDUNet followed by autoencoder (AE) is prepended to the
black-box model. We perform extensive experiments on four classification
datasets, CIFAR-10, CIFAR-10, Tiny Imagenet, STL-10, and the MNIST dataset for
image reconstruction tasks. Our proposed defense methods ZO-RUDS and ZO-AE-RUDS
beat SOTA with a huge margin of $35\%$ and $9\%$, for low dimensional
(CIFAR-10) and with a margin of $20.61\%$ and $23.51\%$ for high-dimensional
(STL-10) datasets, respectively.
</p>

### Title: Event-based tracking of human hands. (arXiv:2304.06534v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06534](http://arxiv.org/abs/2304.06534)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06534] Event-based tracking of human hands](http://arxiv.org/abs/2304.06534) #noise`
* Summary: <p>This paper proposes a novel method for human hands tracking using data from
an event camera. The event camera detects changes in brightness, measuring
motion, with low latency, no motion blur, low power consumption and high
dynamic range. Captured frames are analysed using lightweight algorithms
reporting 3D hand position data. The chosen pick-and-place scenario serves as
an example input for collaborative human-robot interactions and in obstacle
avoidance for human-robot safety applications. Events data are pre-processed
into intensity frames. The regions of interest (ROI) are defined through object
edge event activity, reducing noise. ROI features are extracted for use
in-depth perception. Event-based tracking of human hand demonstrated feasible,
in real time and at a low computational cost. The proposed ROI-finding method
reduces noise from intensity images, achieving up to 89% of data reduction in
relation to the original, while preserving the features. The depth estimation
error in relation to ground truth (measured with wearables), measured using
dynamic time warping and using a single event camera, is from 15 to 30
millimetres, depending on the plane it is measured. Tracking of human hands in
3D space using a single event camera data and lightweight algorithms to define
ROI features (hands tracking in space).
</p>

### Title: Sequential Monte Carlo applied to virtual flow meter calibration. (arXiv:2304.06310v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.06310](http://arxiv.org/abs/2304.06310)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06310] Sequential Monte Carlo applied to virtual flow meter calibration](http://arxiv.org/abs/2304.06310) #noise`
* Summary: <p>Soft-sensors are gaining popularity due to their ability to provide estimates
of key process variables with little intervention required on the asset and at
a low cost. In oil and gas production, virtual flow metering (VFM) is a popular
soft-sensor that attempts to estimate multiphase flow rates in real time. VFMs
are based on models, and these models require calibration. The calibration is
highly dependent on the application, both due to the great diversity of the
models, and in the available measurements. The most accurate calibration is
achieved by careful tuning of the VFM parameters to well tests, but this can be
work intensive, and not all wells have frequent well test data available. This
paper presents a calibration method based on the measurement provided by the
production separator, and the assumption that the observed flow should be equal
to the sum of flow rates from each individual well. This allows us to jointly
calibrate the VFMs continuously. The method applies Sequential Monte Carlo
(SMC) to infer a tuning factor and the flow composition for each well. The
method is tested on a case with ten wells, using both synthetic and real data.
The results are promising and the method is able to provide reasonable
estimates of the parameters without relying on well tests. However, some
challenges are identified and discussed, particularly related to the process
noise and how to manage varying data quality.
</p>

## diffusion
### Title: DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning. (arXiv:2304.06648v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06648](http://arxiv.org/abs/2304.06648)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06648] DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning](http://arxiv.org/abs/2304.06648) #diffusion`
* Summary: <p>Diffusion models have proven to be highly effective in generating
high-quality images. However, adapting large pre-trained diffusion models to
new domains remains an open challenge, which is critical for real-world
applications. This paper proposes DiffFit, a parameter-efficient strategy to
fine-tune large pre-trained diffusion models that enable fast adaptation to new
domains. DiffFit is embarrassingly simple that only fine-tunes the bias term
and newly-added scaling factors in specific layers, yet resulting in
significant training speed-up and reduced model storage costs. Compared with
full fine-tuning, DiffFit achieves 2$\times$ training speed-up and only needs
to store approximately 0.12\% of the total model parameters. Intuitive
theoretical analysis has been provided to justify the efficacy of scaling
factors on fast adaptation. On 8 downstream datasets, DiffFit achieves superior
or competitive performances compared to the full fine-tuning while being more
efficient. Remarkably, we show that DiffFit can adapt a pre-trained
low-resolution generative model to a high-resolution one by adding minimal
cost. Among diffusion-based methods, DiffFit sets a new state-of-the-art FID of
3.02 on ImageNet 512$\times$512 benchmark by fine-tuning only 25 epochs from a
public pre-trained ImageNet 256$\times$256 checkpoint while being 30$\times$
more training efficient than the closest competitor.
</p>

### Title: Learning Controllable 3D Diffusion Models from Single-view Images. (arXiv:2304.06700v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06700](http://arxiv.org/abs/2304.06700)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06700] Learning Controllable 3D Diffusion Models from Single-view Images](http://arxiv.org/abs/2304.06700) #diffusion`
* Summary: <p>Diffusion models have recently become the de-facto approach for generative
modeling in the 2D domain. However, extending diffusion models to 3D is
challenging due to the difficulties in acquiring 3D ground truth data for
training. On the other hand, 3D GANs that integrate implicit 3D representations
into GANs have shown remarkable 3D-aware generation when trained only on
single-view image datasets. However, 3D GANs do not provide straightforward
ways to precisely control image synthesis. To address these challenges, We
present Control3Diff, a 3D diffusion model that combines the strengths of
diffusion models and 3D GANs for versatile, controllable 3D-aware image
synthesis for single-view datasets. Control3Diff explicitly models the
underlying latent distribution (optionally conditioned on external inputs),
thus enabling direct control during the diffusion process. Moreover, our
approach is general and applicable to any type of controlling input, allowing
us to train it with the same diffusion objective without any auxiliary
supervision. We validate the efficacy of Control3Diff on standard image
generation benchmarks, including FFHQ, AFHQ, and ShapeNet, using various
conditioning inputs such as images, sketches, and text prompts. Please see the
project website (\url{https://jiataogu.me/control3diff}) for video comparisons.
</p>

### Title: DiffusionRig: Learning Personalized Priors for Facial Appearance Editing. (arXiv:2304.06711v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06711](http://arxiv.org/abs/2304.06711)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06711] DiffusionRig: Learning Personalized Priors for Facial Appearance Editing](http://arxiv.org/abs/2304.06711) #diffusion`
* Summary: <p>We address the problem of learning person-specific facial priors from a small
number (e.g., 20) of portrait photos of the same person. This enables us to
edit this specific person's facial appearance, such as expression and lighting,
while preserving their identity and high-frequency facial details. Key to our
approach, which we dub DiffusionRig, is a diffusion model conditioned on, or
"rigged by," crude 3D face models estimated from single in-the-wild images by
an off-the-shelf estimator. On a high level, DiffusionRig learns to map
simplistic renderings of 3D face models to realistic photos of a given person.
Specifically, DiffusionRig is trained in two stages: It first learns generic
facial priors from a large-scale face dataset and then person-specific priors
from a small portrait photo collection of the person of interest. By learning
the CGI-to-photo mapping with such personalized priors, DiffusionRig can "rig"
the lighting, facial expression, head pose, etc. of a portrait photo,
conditioned only on coarse 3D models while preserving this person's identity
and other high-frequency characteristics. Qualitative and quantitative
experiments show that DiffusionRig outperforms existing approaches in both
identity preservation and photorealism. Please see the project website:
https://diffusionrig.github.io for the supplemental material, video, code, and
data.
</p>

### Title: Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction. (arXiv:2304.06714v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06714](http://arxiv.org/abs/2304.06714)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06714] Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction](http://arxiv.org/abs/2304.06714) #diffusion`
* Summary: <p>3D-aware image synthesis encompasses a variety of tasks, such as scene
generation and novel view synthesis from images. Despite numerous task-specific
methods, developing a comprehensive model remains challenging. In this paper,
we present SSDNeRF, a unified approach that employs an expressive diffusion
model to learn a generalizable prior of neural radiance fields (NeRF) from
multi-view images of diverse objects. Previous studies have used two-stage
approaches that rely on pretrained NeRFs as real data to train diffusion
models. In contrast, we propose a new single-stage training paradigm with an
end-to-end objective that jointly optimizes a NeRF auto-decoder and a latent
diffusion model, enabling simultaneous 3D reconstruction and prior learning,
even from sparsely available views. At test time, we can directly sample the
diffusion prior for unconditional generation, or combine it with arbitrary
observations of unseen objects for NeRF reconstruction. SSDNeRF demonstrates
robust results comparable to or better than leading task-specific methods in
unconditional generation and single/sparse-view 3D reconstruction.
</p>

### Title: Expressive Text-to-Image Generation with Rich Text. (arXiv:2304.06720v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06720](http://arxiv.org/abs/2304.06720)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06720] Expressive Text-to-Image Generation with Rich Text](http://arxiv.org/abs/2304.06720) #diffusion`
* Summary: <p>Plain text has become a prevalent interface for text-to-image synthesis.
However, its limited customization options hinder users from accurately
describing desired outputs. For example, plain text makes it hard to specify
continuous quantities, such as the precise RGB color value or importance of
each word. Furthermore, creating detailed text prompts for complex scenes is
tedious for humans to write and challenging for text encoders to interpret. To
address these challenges, we propose using a rich-text editor supporting
formats such as font style, size, color, and footnote. We extract each word's
attributes from rich text to enable local style control, explicit token
reweighting, precise color rendering, and detailed region synthesis. We achieve
these capabilities through a region-based diffusion process. We first obtain
each word's region based on cross-attention maps of a vanilla diffusion process
using plain text. For each region, we enforce its text attributes by creating
region-specific detailed prompts and applying region-specific guidance. We
present various examples of image generation from rich text and demonstrate
that our method outperforms strong baselines with quantitative evaluations.
</p>

## LLM
## segmentation
### Title: Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation. (arXiv:2304.06051v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06051](http://arxiv.org/abs/2304.06051)
* Code URL: [https://github.com/Traffic-X/Open-TransMind](https://github.com/Traffic-X/Open-TransMind)
* Copy Paste: `<input type="checkbox">[[2304.06051] Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation](http://arxiv.org/abs/2304.06051) #segmentation`
* Summary: <p>With the continuous improvement of computing power and deep learning
algorithms in recent years, the foundation model has grown in popularity.
Because of its powerful capabilities and excellent performance, this technology
is being adopted and applied by an increasing number of industries. In the
intelligent transportation industry, artificial intelligence faces the
following typical challenges: few shots, poor generalization, and a lack of
multi-modal techniques. Foundation model technology can significantly alleviate
the aforementioned issues. To address these, we designed the 1st Foundation
Model Challenge, with the goal of increasing the popularity of foundation model
technology in traffic scenarios and promoting the rapid development of the
intelligent transportation industry. The challenge is divided into two tracks:
all-in-one and cross-modal image retrieval. Furthermore, we provide a new
baseline and benchmark for the two tracks, called Open-TransMind. According to
our knowledge, Open-TransMind is the first open-source transportation
foundation model with multi-task and multi-modal capabilities. Simultaneously,
Open-TransMind can achieve state-of-the-art performance on detection,
classification, and segmentation datasets of traffic scenarios. Our source code
is available at https://github.com/Traffic-X/Open-TransMind.
</p>

### Title: UniverSeg: Universal Medical Image Segmentation. (arXiv:2304.06131v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06131](http://arxiv.org/abs/2304.06131)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06131] UniverSeg: Universal Medical Image Segmentation](http://arxiv.org/abs/2304.06131) #segmentation`
* Summary: <p>While deep learning models have become the predominant method for medical
image segmentation, they are typically not capable of generalizing to unseen
segmentation tasks involving new anatomies, image modalities, or labels. Given
a new segmentation task, researchers generally have to train or fine-tune
models, which is time-consuming and poses a substantial barrier for clinical
researchers, who often lack the resources and expertise to train neural
networks. We present UniverSeg, a method for solving unseen medical
segmentation tasks without additional training. Given a query image and example
set of image-label pairs that define a new segmentation task, UniverSeg employs
a new Cross-Block mechanism to produce accurate segmentation maps without the
need for additional training. To achieve generalization to new tasks, we have
gathered and standardized a collection of 53 open-access medical segmentation
datasets with over 22,000 scans, which we refer to as MegaMedical. We used this
collection to train UniverSeg on a diverse set of anatomies and imaging
modalities. We demonstrate that UniverSeg substantially outperforms several
related methods on unseen tasks, and thoroughly analyze and draw insights about
important aspects of the proposed system. The UniverSeg source code and model
weights are freely available at https://universeg.csail.mit.edu
</p>

### Title: Boosting Video Object Segmentation via Space-time Correspondence Learning. (arXiv:2304.06211v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06211](http://arxiv.org/abs/2304.06211)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06211] Boosting Video Object Segmentation via Space-time Correspondence Learning](http://arxiv.org/abs/2304.06211) #segmentation`
* Summary: <p>Current top-leading solutions for video object segmentation (VOS) typically
follow a matching-based regime: for each query frame, the segmentation mask is
inferred according to its correspondence to previously processed and the first
annotated frames. They simply exploit the supervisory signals from the
groundtruth masks for learning mask prediction only, without posing any
constraint on the space-time correspondence matching, which, however, is the
fundamental building block of such regime. To alleviate this crucial yet
commonly ignored issue, we devise a correspondence-aware training framework,
which boosts matching-based VOS solutions by explicitly encouraging robust
correspondence matching during network learning. Through comprehensively
exploring the intrinsic coherence in videos on pixel and object levels, our
algorithm reinforces the standard, fully supervised training of mask
segmentation with label-free, contrastive correspondence learning. Without
neither requiring extra annotation cost during training, nor causing speed
delay during deployment, nor incurring architectural modification, our
algorithm provides solid performance gains on four widely used benchmarks,
i.e., DAVIS2016&amp;2017, and YouTube-VOS2018&amp;2019, on the top of famous
matching-based VOS solutions.
</p>

### Title: [CLS] Token is All You Need for Zero-Shot Semantic Segmentation. (arXiv:2304.06212v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06212](http://arxiv.org/abs/2304.06212)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06212] [CLS] Token is All You Need for Zero-Shot Semantic Segmentation](http://arxiv.org/abs/2304.06212) #segmentation`
* Summary: <p>In this paper, we propose an embarrassingly simple yet highly effective
zero-shot semantic segmentation (ZS3) method, based on the pre-trained
vision-language model CLIP. First, our study provides a couple of key
discoveries: (i) the global tokens (a.k.a [CLS] tokens in Transformer) of the
text branch in CLIP provide a powerful representation of semantic information
and (ii) these text-side [CLS] tokens can be regarded as category priors to
guide CLIP visual encoder pay more attention on the corresponding region of
interest. Based on that, we build upon the CLIP model as a backbone which we
extend with a One-Way [CLS] token navigation from text to the visual branch
that enables zero-shot dense prediction, dubbed \textbf{ClsCLIP}. Specifically,
we use the [CLS] token output from the text branch, as an auxiliary semantic
prompt, to replace the [CLS] token in shallow layers of the ViT-based visual
encoder. This one-way navigation embeds such global category prior earlier and
thus promotes semantic segmentation. Furthermore, to better segment tiny
objects in ZS3, we further enhance ClsCLIP with a local zoom-in strategy, which
employs a region proposal pre-processing and we get ClsCLIP+. Extensive
experiments demonstrate that our proposed ZS3 method achieves a SOTA
performance, and it is even comparable with those few-shot semantic
segmentation methods.
</p>

### Title: SPColor: Semantic Prior Guided Exemplar-based Image Colorization. (arXiv:2304.06255v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06255](http://arxiv.org/abs/2304.06255)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06255] SPColor: Semantic Prior Guided Exemplar-based Image Colorization](http://arxiv.org/abs/2304.06255) #segmentation`
* Summary: <p>Exemplar-based image colorization aims to colorize a target grayscale image
based on a color reference image, and the key is to establish accurate
pixel-level semantic correspondence between these two images. Previous methods
search for correspondence across the entire reference image, and this type of
global matching is easy to get mismatch. We summarize the difficulties in two
aspects: (1) When the reference image only contains a part of objects related
to target image, improper correspondence will be established in unrelated
regions. (2) It is prone to get mismatch in regions where the shape or texture
of the object is easily confused. To overcome these issues, we propose SPColor,
a semantic prior guided exemplar-based image colorization framework. Different
from previous methods, SPColor first coarsely classifies pixels of the
reference and target images to several pseudo-classes under the guidance of
semantic prior, then the correspondences are only established locally between
the pixels in the same class via the newly designed semantic prior guided
correspondence network. In this way, improper correspondence between different
semantic classes is explicitly excluded, and the mismatch is obviously
alleviated. Besides, to better reserve the color from reference, a similarity
masked perceptual loss is designed. Noting that the carefully designed SPColor
utilizes the semantic prior provided by an unsupervised segmentation model,
which is free for additional manual semantic annotations. Experiments
demonstrate that our model outperforms recent state-of-the-art methods both
quantitatively and qualitatively on public dataset.
</p>

### Title: Leveraging triplet loss for unsupervised action segmentation. (arXiv:2304.06403v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06403](http://arxiv.org/abs/2304.06403)
* Code URL: [https://github.com/elenabbbuenob/tsa-actionseg](https://github.com/elenabbbuenob/tsa-actionseg)
* Copy Paste: `<input type="checkbox">[[2304.06403] Leveraging triplet loss for unsupervised action segmentation](http://arxiv.org/abs/2304.06403) #segmentation`
* Summary: <p>In this paper, we propose a novel fully unsupervised framework that learns
action representations suitable for the action segmentation task from the
single input video itself, without requiring any training data. Our method is a
deep metric learning approach rooted in a shallow network with a triplet loss
operating on similarity distributions and a novel triplet selection strategy
that effectively models temporal and semantic priors to discover actions in the
new representational space. Under these circumstances, we successfully recover
temporal boundaries in the learned action representations with higher quality
compared with existing unsupervised approaches. The proposed method is
evaluated on two widely used benchmark datasets for the action segmentation
task and it achieves competitive performance by applying a generic clustering
algorithm on the learned representations.
</p>

### Title: Tracking by 3D Model Estimation of Unknown Objects in Videos. (arXiv:2304.06419v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06419](http://arxiv.org/abs/2304.06419)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06419] Tracking by 3D Model Estimation of Unknown Objects in Videos](http://arxiv.org/abs/2304.06419) #segmentation`
* Summary: <p>Most model-free visual object tracking methods formulate the tracking task as
object location estimation given by a 2D segmentation or a bounding box in each
video frame. We argue that this representation is limited and instead propose
to guide and improve 2D tracking with an explicit object representation, namely
the textured 3D shape and 6DoF pose in each video frame. Our representation
tackles a complex long-term dense correspondence problem between all 3D points
on the object for all video frames, including frames where some points are
invisible. To achieve that, the estimation is driven by re-rendering the input
video frames as well as possible through differentiable rendering, which has
not been used for tracking before. The proposed optimization minimizes a novel
loss function to estimate the best 3D shape, texture, and 6DoF pose. We improve
the state-of-the-art in 2D segmentation tracking on three different datasets
with mostly rigid objects.
</p>

### Title: RadarGNN: Transformation Invariant Graph Neural Network for Radar-based Perception. (arXiv:2304.06547v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06547](http://arxiv.org/abs/2304.06547)
* Code URL: [https://github.com/tumftm/radargnn](https://github.com/tumftm/radargnn)
* Copy Paste: `<input type="checkbox">[[2304.06547] RadarGNN: Transformation Invariant Graph Neural Network for Radar-based Perception](http://arxiv.org/abs/2304.06547) #segmentation`
* Summary: <p>A reliable perception has to be robust against challenging environmental
conditions. Therefore, recent efforts focused on the use of radar sensors in
addition to camera and lidar sensors for perception applications. However, the
sparsity of radar point clouds and the poor data availability remain
challenging for current perception methods. To address these challenges, a
novel graph neural network is proposed that does not just use the information
of the points themselves but also the relationships between the points. The
model is designed to consider both point features and point-pair features,
embedded in the edges of the graph. Furthermore, a general approach for
achieving transformation invariance is proposed which is robust against unseen
scenarios and also counteracts the limited data availability. The
transformation invariance is achieved by an invariant data representation
rather than an invariant model architecture, making it applicable to other
methods. The proposed RadarGNN model outperforms all previous methods on the
RadarScenes dataset. In addition, the effects of different invariances on the
object detection and semantic segmentation quality are investigated. The code
is made available as open-source software under
https://github.com/TUMFTM/RadarGNN.
</p>

### Title: Brain Structure Ages -- A new biomarker for multi-disease classification. (arXiv:2304.06591v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06591](http://arxiv.org/abs/2304.06591)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06591] Brain Structure Ages -- A new biomarker for multi-disease classification](http://arxiv.org/abs/2304.06591) #segmentation`
* Summary: <p>Age is an important variable to describe the expected brain's anatomy status
across the normal aging trajectory. The deviation from that normative aging
trajectory may provide some insights into neurological diseases. In
neuroimaging, predicted brain age is widely used to analyze different diseases.
However, using only the brain age gap information (\ie the difference between
the chronological age and the estimated age) can be not enough informative for
disease classification problems. In this paper, we propose to extend the notion
of global brain age by estimating brain structure ages using structural
magnetic resonance imaging. To this end, an ensemble of deep learning models is
first used to estimate a 3D aging map (\ie voxel-wise age estimation). Then, a
3D segmentation mask is used to obtain the final brain structure ages. This
biomarker can be used in several situations. First, it enables to accurately
estimate the brain age for the purpose of anomaly detection at the population
level. In this situation, our approach outperforms several state-of-the-art
methods. Second, brain structure ages can be used to compute the deviation from
the normal aging process of each brain structure. This feature can be used in a
multi-disease classification task for an accurate differential diagnosis at the
subject level. Finally, the brain structure age deviations of individuals can
be visualized, providing some insights about brain abnormality and helping
clinicians in real medical contexts.
</p>

### Title: STU-Net: Scalable and Transferable Medical Image Segmentation Models Empowered by Large-Scale Supervised Pre-training. (arXiv:2304.06716v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06716](http://arxiv.org/abs/2304.06716)
* Code URL: [https://github.com/ziyan-huang/stu-net](https://github.com/ziyan-huang/stu-net)
* Copy Paste: `<input type="checkbox">[[2304.06716] STU-Net: Scalable and Transferable Medical Image Segmentation Models Empowered by Large-Scale Supervised Pre-training](http://arxiv.org/abs/2304.06716) #segmentation`
* Summary: <p>Large-scale models pre-trained on large-scale datasets have profoundly
advanced the development of deep learning. However, the state-of-the-art models
for medical image segmentation are still small-scale, with their parameters
only in the tens of millions. Further scaling them up to higher orders of
magnitude is rarely explored. An overarching goal of exploring large-scale
models is to train them on large-scale medical segmentation datasets for better
transfer capacities. In this work, we design a series of Scalable and
Transferable U-Net (STU-Net) models, with parameter sizes ranging from 14
million to 1.4 billion. Notably, the 1.4B STU-Net is the largest medical image
segmentation model to date. Our STU-Net is based on nnU-Net framework due to
its popularity and impressive performance. We first refine the default
convolutional blocks in nnU-Net to make them scalable. Then, we empirically
evaluate different scaling combinations of network depth and width, discovering
that it is optimal to scale model depth and width together. We train our
scalable STU-Net models on a large-scale TotalSegmentator dataset and find that
increasing model size brings a stronger performance gain. This observation
reveals that a large model is promising in medical image segmentation.
Furthermore, we evaluate the transferability of our model on 14 downstream
datasets for direct inference and 3 datasets for further fine-tuning, covering
various modalities and segmentation targets. We observe good performance of our
pre-trained model in both direct inference and fine-tuning. The code and
pre-trained models are available at https://github.com/Ziyan-Huang/STU-Net.
</p>

### Title: Segment Everything Everywhere All at Once. (arXiv:2304.06718v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06718](http://arxiv.org/abs/2304.06718)
* Code URL: [https://github.com/ux-decoder/segment-everything-everywhere-all-at-once](https://github.com/ux-decoder/segment-everything-everywhere-all-at-once)
* Copy Paste: `<input type="checkbox">[[2304.06718] Segment Everything Everywhere All at Once](http://arxiv.org/abs/2304.06718) #segmentation`
* Summary: <p>Despite the growing demand for interactive AI systems, there have been few
comprehensive studies on human-AI interaction in visual understanding e.g.
segmentation. Inspired by the development of prompt-based universal interfaces
for LLMs, this paper presents SEEM, a promptable, interactive model for
Segmenting Everything Everywhere all at once in an image. SEEM has four
desiderata: i) Versatility: by introducing a versatile prompting engine for
different types of prompts, including points, boxes, scribbles, masks, texts,
and referred regions of another image; ii) Compositionality: by learning a
joint visual-semantic space for visual and textual prompts to compose queries
on the fly for inference as shown in Fig 1; iii)Interactivity: by incorporating
learnable memory prompts to retain dialog history information via mask-guided
cross-attention; and iv) Semantic-awareness: by using a text encoder to encode
text queries and mask labels for open-vocabulary segmentation.
</p>

### Title: An Arrhythmia Classification-Guided Segmentation Model for Electrocardiogram Delineation. (arXiv:2304.06237v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.06237](http://arxiv.org/abs/2304.06237)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06237] An Arrhythmia Classification-Guided Segmentation Model for Electrocardiogram Delineation](http://arxiv.org/abs/2304.06237) #segmentation`
* Summary: <p>Accurate delineation of key waveforms in an ECG is a critical initial step in
extracting relevant features to support the diagnosis and treatment of heart
conditions. Although deep learning based methods using a segmentation model to
locate P, QRS and T waves have shown promising results, their ability to handle
signals exhibiting arrhythmia remains unclear. In this study, we propose a
novel approach that leverages a deep learning model to accurately delineate
signals with a wide range of arrhythmia. Our approach involves training a
segmentation model using a hybrid loss function that combines segmentation with
the task of arrhythmia classification. In addition, we use a diverse training
set containing various arrhythmia types, enabling our model to handle a wide
range of challenging cases. Experimental results show that our model accurately
delineates signals with a broad range of abnormal rhythm types, and the
combined training with classification guidance can effectively reduce false
positive P wave predictions, particularly during atrial fibrillation and atrial
flutter. Furthermore, our proposed method shows competitive performance with
previous delineation algorithms on the Lobachevsky University Database (LUDB).
</p>

## object detection
### Title: Gamifying Math Education using Object Detection. (arXiv:2304.06270v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06270](http://arxiv.org/abs/2304.06270)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06270] Gamifying Math Education using Object Detection](http://arxiv.org/abs/2304.06270) #object detection`
* Summary: <p>Manipulatives used in the right way help improve mathematical concepts
leading to better learning outcomes. In this paper, we present a phygital
(physical + digital) curriculum inspired teaching system for kids aged 5-8 to
learn geometry using shape tile manipulatives. Combining smaller shapes to form
larger ones is an important skill kids learn early on which requires shape
tiles to be placed close to each other in the play area. This introduces a
challenge of oriented object detection for densely packed objects with
arbitrary orientations. Leveraging simulated data for neural network training
and light-weight mobile architectures, we enable our system to understand user
interactions and provide real-time audiovisual feedback. Experimental results
show that our network runs real-time with high precision/recall on consumer
devices, thereby providing a consistent and enjoyable learning experience.
</p>

### Title: Boosting Convolutional Neural Networks with Middle Spectrum Grouped Convolution. (arXiv:2304.06305v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06305](http://arxiv.org/abs/2304.06305)
* Code URL: [https://github.com/hellozhuo/msgc](https://github.com/hellozhuo/msgc)
* Copy Paste: `<input type="checkbox">[[2304.06305] Boosting Convolutional Neural Networks with Middle Spectrum Grouped Convolution](http://arxiv.org/abs/2304.06305) #object detection`
* Summary: <p>This paper proposes a novel module called middle spectrum grouped convolution
(MSGC) for efficient deep convolutional neural networks (DCNNs) with the
mechanism of grouped convolution. It explores the broad "middle spectrum" area
between channel pruning and conventional grouped convolution. Compared with
channel pruning, MSGC can retain most of the information from the input feature
maps due to the group mechanism; compared with grouped convolution, MSGC
benefits from the learnability, the core of channel pruning, for constructing
its group topology, leading to better channel division. The middle spectrum
area is unfolded along four dimensions: group-wise, layer-wise, sample-wise,
and attention-wise, making it possible to reveal more powerful and
interpretable structures. As a result, the proposed module acts as a booster
that can reduce the computational cost of the host backbones for general image
recognition with even improved predictive accuracy. For example, in the
experiments on ImageNet dataset for image classification, MSGC can reduce the
multiply-accumulates (MACs) of ResNet-18 and ResNet-50 by half but still
increase the Top-1 accuracy by more than 1%. With 35% reduction of MACs, MSGC
can also increase the Top-1 accuracy of the MobileNetV2 backbone. Results on MS
COCO dataset for object detection show similar observations. Our code and
trained models are available at https://github.com/hellozhuo/msgc.
</p>

### Title: ODAM: Gradient-based instance-specific visual explanations for object detection. (arXiv:2304.06354v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06354](http://arxiv.org/abs/2304.06354)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06354] ODAM: Gradient-based instance-specific visual explanations for object detection](http://arxiv.org/abs/2304.06354) #object detection`
* Summary: <p>We propose the gradient-weighted Object Detector Activation Maps (ODAM), a
visualized explanation technique for interpreting the predictions of object
detectors. Utilizing the gradients of detector targets flowing into the
intermediate feature maps, ODAM produces heat maps that show the influence of
regions on the detector's decision for each predicted attribute. Compared to
previous works classification activation maps (CAM), ODAM generates
instance-specific explanations rather than class-specific ones. We show that
ODAM is applicable to both one-stage detectors and two-stage detectors with
different types of detector backbones and heads, and produces higher-quality
visual explanations than the state-of-the-art both effectively and efficiently.
We next propose a training scheme, Odam-Train, to improve the explanation
ability on object discrimination of the detector through encouraging
consistency between explanations for detections on the same object, and
distinct explanations for detections on different objects. Based on the heat
maps produced by ODAM with Odam-Train, we propose Odam-NMS, which considers the
information of the model's explanation for each prediction to distinguish the
duplicate detected objects. We present a detailed analysis of the visualized
explanations of detectors and carry out extensive experiments to validate the
effectiveness of the proposed ODAM.
</p>

### Title: You are here! Finding position and orientation on a 2D map from a single image: The Flatlandia localization problem and dataset. (arXiv:2304.06373v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06373](http://arxiv.org/abs/2304.06373)
* Code URL: [https://github.com/IIT-PAVIS/Flatlandia](https://github.com/IIT-PAVIS/Flatlandia)
* Copy Paste: `<input type="checkbox">[[2304.06373] You are here! Finding position and orientation on a 2D map from a single image: The Flatlandia localization problem and dataset](http://arxiv.org/abs/2304.06373) #object detection`
* Summary: <p>We introduce Flatlandia, a novel problem for visual localization of an image
from object detections composed of two specific tasks: i) Coarse Map
Localization: localizing a single image observing a set of objects in respect
to a 2D map of object landmarks; ii) Fine-grained 3DoF Localization: estimating
latitude, longitude, and orientation of the image within a 2D map. Solutions
for these new tasks exploit the wide availability of open urban maps annotated
with GPS locations of common objects (\eg via surveying or crowd-sourced). Such
maps are also more storage-friendly than standard large-scale 3D models often
used in visual localization while additionally being privacy-preserving. As
existing datasets are unsuited for the proposed problem, we provide the
Flatlandia dataset, designed for 3DoF visual localization in multiple urban
settings and based on crowd-sourced data from five European cities. We use the
Flatlandia dataset to validate the complexity of the proposed tasks.
</p>

### Title: Learning Accurate Performance Predictors for Ultrafast Automated Model Compression. (arXiv:2304.06393v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06393](http://arxiv.org/abs/2304.06393)
* Code URL: [https://github.com/ziweiwangthu/seernet](https://github.com/ziweiwangthu/seernet)
* Copy Paste: `<input type="checkbox">[[2304.06393] Learning Accurate Performance Predictors for Ultrafast Automated Model Compression](http://arxiv.org/abs/2304.06393) #object detection`
* Summary: <p>In this paper, we propose an ultrafast automated model compression framework
called SeerNet for flexible network deployment. Conventional
non-differen-tiable methods discretely search the desirable compression policy
based on the accuracy from exhaustively trained lightweight models, and
existing differentiable methods optimize an extremely large supernet to obtain
the required compressed model for deployment. They both cause heavy
computational cost due to the complex compression policy search and evaluation
process. On the contrary, we obtain the optimal efficient networks by directly
optimizing the compression policy with an accurate performance predictor, where
the ultrafast automated model compression for various computational cost
constraint is achieved without complex compression policy search and
evaluation. Specifically, we first train the performance predictor based on the
accuracy from uncertain compression policies actively selected by efficient
evolutionary search, so that informative supervision is provided to learn the
accurate performance predictor with acceptable cost. Then we leverage the
gradient that maximizes the predicted performance under the barrier complexity
constraint for ultrafast acquisition of the desirable compression policy, where
adaptive update stepsizes with momentum are employed to enhance optimality of
the acquired pruning and quantization strategy. Compared with the
state-of-the-art automated model compression methods, experimental results on
image classification and object detection show that our method achieves
competitive accuracy-complexity trade-offs with significant reduction of the
search cost.
</p>

### Title: Class-Incremental Learning of Plant and Disease Detection: Growing Branches with Knowledge Distillation. (arXiv:2304.06619v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.06619](http://arxiv.org/abs/2304.06619)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06619] Class-Incremental Learning of Plant and Disease Detection: Growing Branches with Knowledge Distillation](http://arxiv.org/abs/2304.06619) #object detection`
* Summary: <p>This paper investigates the problem of class-incremental object detection for
agricultural applications where a model needs to learn new plant species and
diseases incrementally without forgetting the previously learned ones. We adapt
two public datasets to include new categories over time, simulating a more
realistic and dynamic scenario. We then compare three class-incremental
learning methods that leverage different forms of knowledge distillation to
mitigate catastrophic forgetting. Our experiments show that all three methods
suffer from catastrophic forgetting, but the recent Dynamic Y-KD approach,
which additionally uses a dynamic architecture that grows new branches to learn
new tasks, outperforms ILOD and Faster-ILOD in most scenarios both on new and
old classes.
</p>
<p>These results highlight the challenges and opportunities of continual object
detection for agricultural applications. In particular, the large intra-class
and small inter-class variability that is typical of plant images exacerbate
the difficulty of learning new categories without interfering with previous
knowledge. We publicly release our code to encourage future work.
</p>

### Title: Confident Object Detection via Conformal Prediction and Conformal Risk Control: an Application to Railway Signaling. (arXiv:2304.06052v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.06052](http://arxiv.org/abs/2304.06052)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.06052] Confident Object Detection via Conformal Prediction and Conformal Risk Control: an Application to Railway Signaling](http://arxiv.org/abs/2304.06052) #object detection`
* Summary: <p>Deploying deep learning models in real-world certified systems requires the
ability to provide confidence estimates that accurately reflect their
uncertainty. In this paper, we demonstrate the use of the conformal prediction
framework to construct reliable and trustworthy predictors for detecting
railway signals. Our approach is based on a novel dataset that includes images
taken from the perspective of a train operator and state-of-the-art object
detectors. We test several conformal approaches and introduce a new method
based on conformal risk control. Our findings demonstrate the potential of the
conformal prediction framework to evaluate model performance and provide
practical guidance for achieving formally guaranteed uncertainty bounds.
</p>

