## data-free
## transformer
### Title: SSTM: Spatiotemporal Recurrent Transformers for Multi-frame Optical Flow Estimation. (arXiv:2304.14418v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14418](http://arxiv.org/abs/2304.14418)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14418] SSTM: Spatiotemporal Recurrent Transformers for Multi-frame Optical Flow Estimation](http://arxiv.org/abs/2304.14418) #transformer`
* Summary: <p>Inaccurate optical flow estimates in and near occluded regions, and
out-of-boundary regions are two of the current significant limitations of
optical flow estimation algorithms. Recent state-of-the-art optical flow
estimation algorithms are two-frame based methods where optical flow is
estimated sequentially for each consecutive image pair in a sequence. While
this approach gives good flow estimates, it fails to generalize optical flows
in occluded regions mainly due to limited local evidence regarding moving
elements in a scene. In this work, we propose a learning-based multi-frame
optical flow estimation method that estimates two or more consecutive optical
flows in parallel from multi-frame image sequences. Our underlying hypothesis
is that by understanding temporal scene dynamics from longer sequences with
more than two frames, we can characterize pixel-wise dependencies in a larger
spatiotemporal domain, generalize complex motion patterns and thereby improve
the accuracy of optical flow estimates in occluded regions. We present
learning-based spatiotemporal recurrent transformers for multi-frame based
optical flow estimation (SSTMs). Our method utilizes 3D Convolutional Gated
Recurrent Units (3D-ConvGRUs) and spatiotemporal transformers to learn
recurrent space-time motion dynamics and global dependencies in the scene and
provide a generalized optical flow estimation. When compared with recent
state-of-the-art two-frame and multi-frame methods on real world and synthetic
datasets, performance of the SSTMs were significantly higher in occluded and
out-of-boundary regions. Among all published state-of-the-art multi-frame
methods, SSTM achieved state-of the-art results on the Sintel Final and
KITTI2015 benchmark datasets.
</p>

### Title: Local-Global Transformer Enhanced Unfolding Network for Pan-sharpening. (arXiv:2304.14612v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14612](http://arxiv.org/abs/2304.14612)
* Code URL: [https://github.com/lms-07/lgteun](https://github.com/lms-07/lgteun)
* Copy Paste: `<input type="checkbox">[[2304.14612] Local-Global Transformer Enhanced Unfolding Network for Pan-sharpening](http://arxiv.org/abs/2304.14612) #transformer`
* Summary: <p>Pan-sharpening aims to increase the spatial resolution of the low-resolution
multispectral (LrMS) image with the guidance of the corresponding panchromatic
(PAN) image. Although deep learning (DL)-based pan-sharpening methods have
achieved promising performance, most of them have a two-fold deficiency. For
one thing, the universally adopted black box principle limits the model
interpretability. For another thing, existing DL-based methods fail to
efficiently capture local and global dependencies at the same time, inevitably
limiting the overall performance. To address these mentioned issues, we first
formulate the degradation process of the high-resolution multispectral (HrMS)
image as a unified variational optimization problem, and alternately solve its
data and prior subproblems by the designed iterative proximal gradient descent
(PGD) algorithm. Moreover, we customize a Local-Global Transformer (LGT) to
simultaneously model local and global dependencies, and further formulate an
LGT-based prior module for image denoising. Besides the prior module, we also
design a lightweight data module. Finally, by serially integrating the data and
prior modules in each iterative stage, we unfold the iterative algorithm into a
stage-wise unfolding network, Local-Global Transformer Enhanced Unfolding
Network (LGTEUN), for the interpretable MS pan-sharpening. Comprehensive
experimental results on three satellite data sets demonstrate the effectiveness
and efficiency of LGTEUN compared with state-of-the-art (SOTA) methods. The
source code is available at https://github.com/lms-07/LGTEUN.
</p>

### Title: LostPaw: Finding Lost Pets using a Contrastive Learning-based Transformer with Visual Input. (arXiv:2304.14765v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14765](http://arxiv.org/abs/2304.14765)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14765] LostPaw: Finding Lost Pets using a Contrastive Learning-based Transformer with Visual Input](http://arxiv.org/abs/2304.14765) #transformer`
* Summary: <p>Losing pets can be highly distressing for pet owners, and finding a lost pet
is often challenging and time-consuming. An artificial intelligence-based
application can significantly improve the speed and accuracy of finding lost
pets. In order to facilitate such an application, this study introduces a
contrastive neural network model capable of accurately distinguishing between
images of pets. The model was trained on a large dataset of dog images and
evaluated through 3-fold cross-validation. Following 350 epochs of training,
the model achieved a test accuracy of 90%. Furthermore, overfitting was
avoided, as the test accuracy closely matched the training accuracy. Our
findings suggest that contrastive neural network models hold promise as a tool
for locating lost pets. This paper provides the foundation for a potential web
application that allows users to upload images of their missing pets, receiving
notifications when matching images are found in the application's image
database. This would enable pet owners to quickly and accurately locate lost
pets and reunite them with their families.
</p>

### Title: IMP: Iterative Matching and Pose Estimation with Adaptive Pooling. (arXiv:2304.14837v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14837](http://arxiv.org/abs/2304.14837)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14837] IMP: Iterative Matching and Pose Estimation with Adaptive Pooling](http://arxiv.org/abs/2304.14837) #transformer`
* Summary: <p>Previous methods solve feature matching and pose estimation using a two-stage
process by first finding matches and then estimating the pose. As they ignore
the geometric relationships between the two tasks, they focus on either
improving the quality of matches or filtering potential outliers, leading to
limited efficiency or accuracy. In contrast, we propose an iterative matching
and pose estimation framework (IMP) leveraging the geometric connections
between the two tasks: a few good matches are enough for a roughly accurate
pose estimation; a roughly accurate pose can be used to guide the matching by
providing geometric constraints. To this end, we implement a geometry-aware
recurrent attention-based module which jointly outputs sparse matches and
camera poses. Specifically, for each iteration, we first implicitly embed
geometric information into the module via a pose-consistency loss, allowing it
to predict geometry-aware matches progressively. Second, we introduce an
\textbf{e}fficient IMP, called EIMP, to dynamically discard keypoints without
potential matches, avoiding redundant updating and significantly reducing the
quadratic time complexity of attention computation in transformers. Experiments
on YFCC100m, Scannet, and Aachen Day-Night datasets demonstrate that the
proposed method outperforms previous approaches in terms of accuracy and
efficiency.
</p>

### Title: MASK-CNN-Transformer For Real-Time Multi-Label Weather Recognition. (arXiv:2304.14857v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14857](http://arxiv.org/abs/2304.14857)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14857] MASK-CNN-Transformer For Real-Time Multi-Label Weather Recognition](http://arxiv.org/abs/2304.14857) #transformer`
* Summary: <p>Weather recognition is an essential support for many practical life
applications, including traffic safety, environment, and meteorology. However,
many existing related works cannot comprehensively describe weather conditions
due to their complex co-occurrence dependencies. This paper proposes a novel
multi-label weather recognition model considering these dependencies. The
proposed model called MASK-Convolutional Neural Network-Transformer (MASK-CT)
is based on the Transformer, the convolutional process, and the MASK mechanism.
The model employs multiple convolutional layers to extract features from
weather images and a Transformer encoder to calculate the probability of each
weather condition based on the extracted features. To improve the
generalization ability of MASK-CT, a MASK mechanism is used during the training
phase. The effect of the MASK mechanism is explored and discussed. The Mask
mechanism randomly withholds some information from one-pair training instances
(one image and its corresponding label). There are two types of MASK methods.
Specifically, MASK-I is designed and deployed on the image before feeding it
into the weather feature extractor and MASK-II is applied to the image label.
The Transformer encoder is then utilized on the randomly masked image features
and labels. The experimental results from various real-world weather
recognition datasets demonstrate that the proposed MASK-CT model outperforms
state-of-the-art methods. Furthermore, the high-speed dynamic real-time weather
recognition capability of the MASK-CT is evaluated.
</p>

### Title: An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14933](http://arxiv.org/abs/2304.14933)
* Code URL: [https://github.com/ylsung/vl-merging](https://github.com/ylsung/vl-merging)
* Copy Paste: `<input type="checkbox">[[2304.14933] An Empirical Study of Multimodal Model Merging](http://arxiv.org/abs/2304.14933) #transformer`
* Summary: <p>Model merging (e.g., via interpolation or task arithmetic) fuses multiple
models trained on different tasks to generate a multi-task solution. The
technique has been proven successful in previous studies, where the models are
trained on similar tasks and with the same initialization. In this paper, we
expand on this concept to a multimodal setup by merging transformers trained on
different modalities. Furthermore, we conduct our study for a novel goal where
we can merge vision, language, and cross-modal transformers of a
modality-specific architecture to create a parameter-efficient
modality-agnostic architecture. Through comprehensive experiments, we
systematically investigate the key factors impacting model performance after
merging, including initialization, merging mechanisms, and model architectures.
Our analysis leads to an effective training recipe for matching the performance
of the modality-agnostic baseline (i.e. pre-trained from scratch) via model
merging. Our code is available at: https://github.com/ylsung/vl-merging
</p>

### Title: FlowTransformer: A Transformer Framework for Flow-based Network Intrusion Detection Systems. (arXiv:2304.14746v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2304.14746](http://arxiv.org/abs/2304.14746)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14746] FlowTransformer: A Transformer Framework for Flow-based Network Intrusion Detection Systems](http://arxiv.org/abs/2304.14746) #transformer`
* Summary: <p>This paper presents the FlowTransformer framework, a novel approach for
implementing transformer-based Network Intrusion Detection Systems (NIDSs).
FlowTransformer leverages the strengths of transformer models in identifying
the long-term behaviour and characteristics of networks, which are often
overlooked by most existing NIDSs. By capturing these complex patterns in
network traffic, FlowTransformer offers a flexible and efficient tool for
researchers and practitioners in the cybersecurity community who are seeking to
implement NIDSs using transformer-based models. FlowTransformer allows the
direct substitution of various transformer components, including the input
encoding, transformer, classification head, and the evaluation of these across
any flow-based network dataset. To demonstrate the effectiveness and efficiency
of the FlowTransformer framework, we utilise it to provide an extensive
evaluation of various common transformer architectures, such as GPT 2.0 and
BERT, on three commonly used public NIDS benchmark datasets. We provide results
for accuracy, model size and speed. A key finding of our evaluation is that the
choice of classification head has the most significant impact on the model
performance. Surprisingly, Global Average Pooling, which is commonly used in
text classification, performs very poorly in the context of NIDS. In addition,
we show that model size can be reduced by over 50\%, and inference and training
times improved, with no loss of accuracy, by making specific choices of input
encoding and classification head instead of other commonly used alternatives.
</p>

### Title: Dissecting Recall of Factual Associations in Auto-Regressive Language Models. (arXiv:2304.14767v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.14767](http://arxiv.org/abs/2304.14767)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14767] Dissecting Recall of Factual Associations in Auto-Regressive Language Models](http://arxiv.org/abs/2304.14767) #transformer`
* Summary: <p>Transformer-based language models (LMs) are known to capture factual
knowledge in their parameters. While previous work looked into where factual
associations are stored, only little is known about how they are retrieved
internally during inference. We investigate this question through the lens of
information flow. Given a subject-relation query, we study how the model
aggregates information about the subject and relation to predict the correct
attribute. With interventions on attention edges, we first identify two
critical points where information propagates to the prediction: one from the
relation positions followed by another from the subject positions. Next, by
analyzing the information at these points, we unveil a three-step internal
mechanism for attribute extraction. First, the representation at the
last-subject position goes through an enrichment process, driven by the early
MLP sublayers, to encode many subject-related attributes. Second, information
from the relation propagates to the prediction. Third, the prediction
representation "queries" the enriched subject to extract the attribute. Perhaps
surprisingly, this extraction is typically done via attention heads, which
often encode subject-attribute mappings in their parameters. Overall, our
findings introduce a comprehensive view of how factual associations are stored
and extracted internally in LMs, facilitating future research on knowledge
localization and editing.
</p>

### Title: ResiDual: Transformer with Dual Residual Connections. (arXiv:2304.14802v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.14802](http://arxiv.org/abs/2304.14802)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14802] ResiDual: Transformer with Dual Residual Connections](http://arxiv.org/abs/2304.14802) #transformer`
* Summary: <p>Transformer networks have become the preferred architecture for many tasks
due to their state-of-the-art performance. However, the optimal way to
implement residual connections in Transformer, which are essential for
effective training, is still debated. Two widely used variants are the
Post-Layer-Normalization (Post-LN) and Pre-Layer-Normalization (Pre-LN)
Transformers, which apply layer normalization after each residual block's
output or before each residual block's input, respectively. While both variants
enjoy their advantages, they also suffer from severe limitations: Post-LN
causes gradient vanishing issue that hinders training deep Transformers, and
Pre-LN causes representation collapse issue that limits model capacity. In this
paper, we propose ResiDual, a novel Transformer architecture with Pre-Post-LN
(PPLN), which fuses the connections in Post-LN and Pre-LN together and inherits
their advantages while avoids their limitations. We conduct both theoretical
analyses and empirical experiments to verify the effectiveness of ResiDual.
Theoretically, we prove that ResiDual has a lower bound on the gradient to
avoid the vanishing issue due to the residual connection from Pre-LN. Moreover,
ResiDual also has diverse model representations to avoid the collapse issue due
to the residual connection from Post-LN. Empirically, ResiDual outperforms both
Post-LN and Pre-LN on several machine translation benchmarks across different
network depths and data sizes. Thanks to the good theoretical and empirical
performance, ResiDual Transformer can serve as a foundation architecture for
different AI models (e.g., large language models). Our code is available at
https://github.com/microsoft/ResiDual.
</p>

### Title: Information Redundancy and Biases in Public Document Information Extraction Benchmarks. (arXiv:2304.14936v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.14936](http://arxiv.org/abs/2304.14936)
* Code URL: [https://github.com/seif-lat/bias-study-funsd-sroie](https://github.com/seif-lat/bias-study-funsd-sroie)
* Copy Paste: `<input type="checkbox">[[2304.14936] Information Redundancy and Biases in Public Document Information Extraction Benchmarks](http://arxiv.org/abs/2304.14936) #transformer`
* Summary: <p>Advances in the Visually-rich Document Understanding (VrDU) field and
particularly the Key-Information Extraction (KIE) task are marked with the
emergence of efficient Transformer-based approaches such as the LayoutLM
models. Despite the good performance of KIE models when fine-tuned on public
benchmarks, they still struggle to generalize on complex real-life use-cases
lacking sufficient document annotations. Our research highlighted that KIE
standard benchmarks such as SROIE and FUNSD contain significant similarity
between training and testing documents and can be adjusted to better evaluate
the generalization of models. In this work, we designed experiments to quantify
the information redundancy in public benchmarks, revealing a 75% template
replication in SROIE official test set and 16% in FUNSD. We also proposed
resampling strategies to provide benchmarks more representative of the
generalization ability of models. We showed that models not suited for document
analysis struggle on the adjusted splits dropping on average 10,5% F1 score on
SROIE and 3.5% on FUNSD compared to multi-modal models dropping only 7,5% F1 on
SROIE and 0.5% F1 on FUNSD.
</p>

### Title: X-RLflow: Graph Reinforcement Learning for Neural Network Subgraphs Transformation. (arXiv:2304.14698v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.14698](http://arxiv.org/abs/2304.14698)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14698] X-RLflow: Graph Reinforcement Learning for Neural Network Subgraphs Transformation](http://arxiv.org/abs/2304.14698) #transformer`
* Summary: <p>Tensor graph superoptimisation systems perform a sequence of subgraph
substitution to neural networks, to find the optimal computation graph
structure. Such a graph transformation process naturally falls into the
framework of sequential decision-making, and existing systems typically employ
a greedy search approach, which cannot explore the whole search space as it
cannot tolerate a temporary loss of performance. In this paper, we address the
tensor graph superoptimisation problem by exploring an alternative search
approach, reinforcement learning (RL). Our proposed approach, X-RLflow, can
learn to perform neural network dataflow graph rewriting, which substitutes a
subgraph one at a time. X-RLflow is based on a model-free RL agent that uses a
graph neural network (GNN) to encode the target computation graph and outputs a
transformed computation graph iteratively. We show that our approach can
outperform state-of-the-art superoptimisation systems over a range of deep
learning models and achieve by up to 40% on those that are based on
transformer-style architectures.
</p>

### Title: Towards Automated Circuit Discovery for Mechanistic Interpretability. (arXiv:2304.14997v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.14997](http://arxiv.org/abs/2304.14997)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14997] Towards Automated Circuit Discovery for Mechanistic Interpretability](http://arxiv.org/abs/2304.14997) #transformer`
* Summary: <p>Recent work in mechanistic interpretability has reverse-engineered nontrivial
behaviors of transformer models. These contributions required considerable
effort and researcher intuition, which makes it difficult to apply the same
methods to understand the complex behavior that current models display. At
their core however, the workflow for these discoveries is surprisingly similar.
Researchers create a data set and metric that elicit the desired model
behavior, subdivide the network into appropriate abstract units, replace
activations of those units to identify which are involved in the behavior, and
then interpret the functions that these units implement. By varying the data
set, metric, and units under investigation, researchers can understand the
functionality of each neural network region and the circuits they compose. This
work proposes a novel algorithm, Automatic Circuit DisCovery (ACDC), to
automate the identification of the important units in the network. Given a
model's computational graph, ACDC finds subgraphs that explain a behavior of
the model. ACDC was able to reproduce a previously identified circuit for
Python docstrings in a small transformer, identifying 6/7 important attention
heads that compose up to 3 layers deep, while including 91% fewer the
connections.
</p>

## generative
### Title: Symmetry and Complexity in Object-Centric Deep Active Inference Models. (arXiv:2304.14493v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14493](http://arxiv.org/abs/2304.14493)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14493] Symmetry and Complexity in Object-Centric Deep Active Inference Models](http://arxiv.org/abs/2304.14493) #generative`
* Summary: <p>Humans perceive and interact with hundreds of objects every day. In doing so,
they need to employ mental models of these objects and often exploit symmetries
in the object's shape and appearance in order to learn generalizable and
transferable skills. Active inference is a first principles approach to
understanding and modeling sentient agents. It states that agents entertain a
generative model of their environment, and learn and act by minimizing an upper
bound on their surprisal, i.e. their Free Energy. The Free Energy decomposes
into an accuracy and complexity term, meaning that agents favor the least
complex model, that can accurately explain their sensory observations. In this
paper, we investigate how inherent symmetries of particular objects also emerge
as symmetries in the latent state space of the generative model learnt under
deep active inference. In particular, we focus on object-centric
representations, which are trained from pixels to predict novel object views as
the agent moves its viewpoint. First, we investigate the relation between model
complexity and symmetry exploitation in the state space. Second, we do a
principal component analysis to demonstrate how the model encodes the principal
axis of symmetry of the object in the latent space. Finally, we also
demonstrate how more symmetrical representations can be exploited for better
generalization in the context of manipulation.
</p>

### Title: Interpreting Vision and Language Generative Models with Semantic Visual Priors. (arXiv:2304.14986v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14986](http://arxiv.org/abs/2304.14986)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14986] Interpreting Vision and Language Generative Models with Semantic Visual Priors](http://arxiv.org/abs/2304.14986) #generative`
* Summary: <p>When applied to Image-to-text models, interpretability methods often provide
token-by-token explanations namely, they compute a visual explanation for each
token of the generated sequence. Those explanations are expensive to compute
and unable to comprehensively explain the model's output. Therefore, these
models often require some sort of approximation that eventually leads to
misleading explanations. We develop a framework based on SHAP, that allows for
generating comprehensive, meaningful explanations leveraging the meaning
representation of the output sequence as a whole. Moreover, by exploiting
semantic priors in the visual backbone, we extract an arbitrary number of
features that allows the efficient computation of Shapley values on large-scale
models, generating at the same time highly meaningful visual explanations. We
demonstrate that our method generates semantically more expressive explanations
than traditional methods at a lower compute cost and that it can be generalized
over other explainability methods.
</p>

### Title: ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger. (arXiv:2304.14475v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2304.14475](http://arxiv.org/abs/2304.14475)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14475] ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger](http://arxiv.org/abs/2304.14475) #generative`
* Summary: <p>Textual backdoor attacks pose a practical threat to existing systems, as they
can compromise the model by inserting imperceptible triggers into inputs and
manipulating labels in the training dataset. With cutting-edge generative
models such as GPT-4 pushing rewriting to extraordinary levels, such attacks
are becoming even harder to detect. We conduct a comprehensive investigation of
the role of black-box generative models as a backdoor attack tool, highlighting
the importance of researching relative defense strategies. In this paper, we
reveal that the proposed generative model-based attack, BGMAttack, could
effectively deceive textual classifiers. Compared with the traditional attack
methods, BGMAttack makes the backdoor trigger less conspicuous by leveraging
state-of-the-art generative models. Our extensive evaluation of attack
effectiveness across five datasets, complemented by three distinct human
cognition assessments, reveals that Figure 4 achieves comparable attack
performance while maintaining superior stealthiness relative to baseline
methods.
</p>

### Title: Adversary Aware Continual Learning. (arXiv:2304.14483v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.14483](http://arxiv.org/abs/2304.14483)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14483] Adversary Aware Continual Learning](http://arxiv.org/abs/2304.14483) #generative`
* Summary: <p>Class incremental learning approaches are useful as they help the model to
learn new information (classes) sequentially, while also retaining the
previously acquired information (classes). However, it has been shown that such
approaches are extremely vulnerable to the adversarial backdoor attacks, where
an intelligent adversary can introduce small amount of misinformation to the
model in the form of imperceptible backdoor pattern during training to cause
deliberate forgetting of a specific task or class at test time. In this work,
we propose a novel defensive framework to counter such an insidious attack
where, we use the attacker's primary strength-hiding the backdoor pattern by
making it imperceptible to humans-against it, and propose to learn a
perceptible (stronger) pattern (also during the training) that can overpower
the attacker's imperceptible (weaker) pattern. We demonstrate the effectiveness
of the proposed defensive mechanism through various commonly used Replay-based
(both generative and exact replay-based) class incremental learning algorithms
using continual learning benchmark variants of CIFAR-10, CIFAR-100, and MNIST
datasets. Most noteworthy, our proposed defensive framework does not assume
that the attacker's target task and target class is known to the defender. The
defender is also unaware of the shape, size, and location of the attacker's
pattern. We show that our proposed defensive framework considerably improves
the performance of class incremental learning algorithms with no knowledge of
the attacker's target task, attacker's target class, and attacker's
imperceptible pattern. We term our defensive framework as Adversary Aware
Continual Learning (AACL).
</p>

### Title: Multisample Flow Matching: Straightening Flows with Minibatch Couplings. (arXiv:2304.14772v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.14772](http://arxiv.org/abs/2304.14772)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14772] Multisample Flow Matching: Straightening Flows with Minibatch Couplings](http://arxiv.org/abs/2304.14772) #generative`
* Summary: <p>Simulation-free methods for training continuous-time generative models
construct probability paths that go between noise distributions and individual
data samples. Recent works, such as Flow Matching, derived paths that are
optimal for each data sample. However, these algorithms rely on independent
data and noise samples, and do not exploit underlying structure in the data
distribution for constructing probability paths. We propose Multisample Flow
Matching, a more general framework that uses non-trivial couplings between data
and noise samples while satisfying the correct marginal constraints. At very
small overhead costs, this generalization allows us to (i) reduce gradient
variance during training, (ii) obtain straighter flows for the learned vector
field, which allows us to generate high-quality samples using fewer function
evaluations, and (iii) obtain transport maps with lower cost in high
dimensions, which has applications beyond generative modeling. Importantly, we
do so in a completely simulation-free manner with a simple minimization
objective. We show that our proposed methods improve sample consistency on
downsampled ImageNet data sets, and lead to better low-cost sample generation.
</p>

## label correction
## noise
### Title: Unsupervised Learning of Robust Spectral Shape Matching. (arXiv:2304.14419v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14419](http://arxiv.org/abs/2304.14419)
* Code URL: [https://github.com/dongliangcao/unsupervised-learning-of-robust-spectral-shape-matching](https://github.com/dongliangcao/unsupervised-learning-of-robust-spectral-shape-matching)
* Copy Paste: `<input type="checkbox">[[2304.14419] Unsupervised Learning of Robust Spectral Shape Matching](http://arxiv.org/abs/2304.14419) #noise`
* Summary: <p>We propose a novel learning-based approach for robust 3D shape matching. Our
method builds upon deep functional maps and can be trained in a fully
unsupervised manner. Previous deep functional map methods mainly focus on
predicting optimised functional maps alone, and then rely on off-the-shelf
post-processing to obtain accurate point-wise maps during inference. However,
this two-stage procedure for obtaining point-wise maps often yields sub-optimal
performance. In contrast, building upon recent insights about the relation
between functional maps and point-wise maps, we propose a novel unsupervised
loss to couple the functional maps and point-wise maps, and thereby directly
obtain point-wise maps without any post-processing. Our approach obtains
accurate correspondences not only for near-isometric shapes, but also for more
challenging non-isometric shapes and partial shapes, as well as shapes with
different discretisation or topological noise. Using a total of nine diverse
datasets, we extensively evaluate the performance and demonstrate that our
method substantially outperforms previous state-of-the-art methods, even
compared to recent supervised methods. Our code is available at
https://github.com/dongliangcao/Unsupervised-Learning-of-Robust-Spectral-Shape-Matching.
</p>

### Title: It is all about where you start: Text-to-image generation with seed selection. (arXiv:2304.14530v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14530](http://arxiv.org/abs/2304.14530)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14530] It is all about where you start: Text-to-image generation with seed selection](http://arxiv.org/abs/2304.14530) #noise`
* Summary: <p>Text-to-image diffusion models can synthesize a large variety of concepts in
new compositions and scenarios. However, they still struggle with generating
uncommon concepts, rare unusual combinations, or structured concepts like hand
palms. Their limitation is partly due to the long-tail nature of their training
data: web-crawled data sets are strongly unbalanced, causing models to
under-represent concepts from the tail of the distribution. Here we
characterize the effect of unbalanced training data on text-to-image models and
offer a remedy. We show that rare concepts can be correctly generated by
carefully selecting suitable generation seeds in the noise space, a technique
that we call SeedSelect. SeedSelect is efficient and does not require
retraining the diffusion model. We evaluate the benefit of SeedSelect on a
series of problems. First, in few-shot semantic data augmentation, where we
generate semantically correct images for few-shot and long-tail benchmarks. We
show classification improvement on all classes, both from the head and tail of
the training data of diffusion models. We further evaluate SeedSelect on
correcting images of hands, a well-known pitfall of current diffusion models,
and show that it improves hand generation substantially.
</p>

### Title: Quality-agnostic Image Captioning to Safely Assist People with Vision Impairment. (arXiv:2304.14623v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14623](http://arxiv.org/abs/2304.14623)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14623] Quality-agnostic Image Captioning to Safely Assist People with Vision Impairment](http://arxiv.org/abs/2304.14623) #noise`
* Summary: <p>Automated image captioning has the potential to be a useful tool for people
with vision impairments. Images taken by this user group are often noisy, which
leads to incorrect and even unsafe model predictions. In this paper, we propose
a quality-agnostic framework to improve the performance and robustness of image
captioning models for visually impaired people. We address this problem from
three angles: data, model, and evaluation. First, we show how data augmentation
techniques for generating synthetic noise can address data sparsity in this
domain. Second, we enhance the robustness of the model by expanding a
state-of-the-art model to a dual network architecture, using the augmented data
and leveraging different consistency losses. Our results demonstrate increased
performance, e.g. an absolute improvement of 2.15 on CIDEr, compared to
state-of-the-art image captioning networks, as well as increased robustness to
noise with up to 3 points improvement on CIDEr in more noisy settings. Finally,
we evaluate the prediction reliability using confidence calibration on images
with different difficulty/noise levels, showing that our models perform more
reliably in safety-critical situations. The improved model is part of an
assisted living application, which we develop in partnership with the Royal
National Institute of Blind People.
</p>

### Title: CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction. (arXiv:2304.14633v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14633](http://arxiv.org/abs/2304.14633)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14633] CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction](http://arxiv.org/abs/2304.14633) #noise`
* Summary: <p>Recent advances in neural reconstruction using posed image sequences have
made remarkable progress. However, due to the lack of depth information,
existing volumetric-based techniques simply duplicate 2D image features of the
object surface along the entire camera ray. We contend this duplication
introduces noise in empty and occluded spaces, posing challenges for producing
high-quality 3D geometry. Drawing inspiration from traditional multi-view
stereo methods, we propose an end-to-end 3D neural reconstruction framework
CVRecon, designed to exploit the rich geometric embedding in the cost volumes
to facilitate 3D geometric feature learning. Furthermore, we present
Ray-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature
representation that encodes view-dependent information with improved integrity
and robustness. Through comprehensive experiments, we demonstrate that our
approach significantly improves the reconstruction quality in various metrics
and recovers clear fine details of the 3D geometries. Our extensive ablation
studies provide insights into the development of effective 3D geometric feature
learning schemes. Project page: https://cvrecon.ziyue.cool/
</p>

### Title: Non-Contact Heart Rate Measurement from Deteriorated Videos. (arXiv:2304.14789v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14789](http://arxiv.org/abs/2304.14789)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14789] Non-Contact Heart Rate Measurement from Deteriorated Videos](http://arxiv.org/abs/2304.14789) #noise`
* Summary: <p>Remote photoplethysmography (rPPG) offers a state-of-the-art, non-contact
methodology for estimating human pulse by analyzing facial videos. Despite its
potential, rPPG methods can be susceptible to various artifacts, such as noise,
occlusions, and other obstructions caused by sunglasses, masks, or even
involuntary facial contact, such as individuals inadvertently touching their
faces. In this study, we apply image processing transformations to
intentionally degrade video quality, mimicking these challenging conditions,
and subsequently evaluate the performance of both non-learning and
learning-based rPPG methods on the deteriorated data. Our results reveal a
significant decrease in accuracy in the presence of these artifacts, prompting
us to propose the application of restoration techniques, such as denoising and
inpainting, to improve heart-rate estimation outcomes. By addressing these
challenging conditions and occlusion artifacts, our approach aims to make rPPG
methods more robust and adaptable to real-world situations. To assess the
effectiveness of our proposed methods, we undertake comprehensive experiments
on three publicly available datasets, encompassing a wide range of scenarios
and artifact types. Our findings underscore the potential to construct a robust
rPPG system by employing an optimal combination of restoration algorithms and
rPPG techniques. Moreover, our study contributes to the advancement of
privacy-conscious rPPG methodologies, thereby bolstering the overall utility
and impact of this innovative technology in the field of remote heart-rate
estimation under realistic and diverse conditions.
</p>

### Title: A noise-robust acoustic method for recognition of foraging activities of grazing cattle. (arXiv:2304.14824v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.14824](http://arxiv.org/abs/2304.14824)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14824] A noise-robust acoustic method for recognition of foraging activities of grazing cattle](http://arxiv.org/abs/2304.14824) #noise`
* Summary: <p>To stay competitive in the growing dairy market, farmers must continuously
improve their livestock production systems. Precision livestock farming
technologies provide individualised monitoring of animals on commercial farms,
optimising livestock production. Continuous acoustic monitoring is a widely
accepted sensing technique used to estimate the daily rumination and grazing
time budget of free-ranging cattle. However, typical environmental and natural
noises on pasture noticeably affect the performance and generalisation of
current acoustic methods. In this study, we present an acoustic method called
Noise-Robust Foraging Activity Recognizer (NRFAR). The proposed method
determines foraging activity bouts by analysing fixed-length segments of
identified jaw movement events associated with grazing and rumination. The
additive noise robustness of NRFAR was evaluated for several signal-to-noise
ratios, using stationary Gaussian white noise and four different non-stationary
natural noise sources. In noiseless conditions, NRFAR reaches an average
balanced accuracy of 89%, outperforming two previous acoustic methods by more
than 7%. Additionally, NRFAR presents better performance than previous acoustic
methods in 66 out of 80 evaluated noisy scenarios (p&lt;0.01). NRFAR operates
online with a similar computational cost to previous acoustic methods. The
combination of these properties and the high performance in harsh free-ranging
environments render NRFAR an excellent choice for real-time implementation in a
low-power embedded device. The instrumentation and computational algorithms
presented within this publication are protected by a pending patent
application: AR P20220100910. Web demo available at:
https://sinc.unl.edu.ar/web-demo/nrfar
</p>

## diffusion
### Title: Learning a Diffusion Prior for NeRFs. (arXiv:2304.14473v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14473](http://arxiv.org/abs/2304.14473)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14473] Learning a Diffusion Prior for NeRFs](http://arxiv.org/abs/2304.14473) #diffusion`
* Summary: <p>Neural Radiance Fields (NeRFs) have emerged as a powerful neural 3D
representation for objects and scenes derived from 2D data. Generating NeRFs,
however, remains difficult in many scenarios. For instance, training a NeRF
with only a small number of views as supervision remains challenging since it
is an under-constrained problem. In such settings, it calls for some inductive
prior to filter out bad local minima. One way to introduce such inductive
priors is to learn a generative model for NeRFs modeling a certain class of
scenes. In this paper, we propose to use a diffusion model to generate NeRFs
encoded on a regularized grid. We show that our model can sample realistic
NeRFs, while at the same time allowing conditional generations, given a certain
observation as guidance.
</p>

### Title: SceneGenie: Scene Graph Guided Diffusion Models for Image Synthesis. (arXiv:2304.14573v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14573](http://arxiv.org/abs/2304.14573)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14573] SceneGenie: Scene Graph Guided Diffusion Models for Image Synthesis](http://arxiv.org/abs/2304.14573) #diffusion`
* Summary: <p>Text-conditioned image generation has made significant progress in recent
years with generative adversarial networks and more recently, diffusion models.
While diffusion models conditioned on text prompts have produced impressive and
high-quality images, accurately representing complex text prompts such as the
number of instances of a specific object remains challenging.
</p>
<p>To address this limitation, we propose a novel guidance approach for the
sampling process in the diffusion model that leverages bounding box and
segmentation map information at inference time without additional training
data. Through a novel loss in the sampling process, our approach guides the
model with semantic features from CLIP embeddings and enforces geometric
constraints, leading to high-resolution images that accurately represent the
scene. To obtain bounding box and segmentation map information, we structure
the text prompt as a scene graph and enrich the nodes with CLIP embeddings. Our
proposed model achieves state-of-the-art performance on two public benchmarks
for image generation from scene graphs, surpassing both scene graph to image
and text-based diffusion models in various metrics. Our results demonstrate the
effectiveness of incorporating bounding box and segmentation map guidance in
the diffusion model sampling process for more accurate text-to-image
generation.
</p>

### Title: MUDiff: Unified Diffusion for Complete Molecule Generation. (arXiv:2304.14621v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.14621](http://arxiv.org/abs/2304.14621)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14621] MUDiff: Unified Diffusion for Complete Molecule Generation](http://arxiv.org/abs/2304.14621) #diffusion`
* Summary: <p>We present a new model for generating molecular data by combining discrete
and continuous diffusion processes. Our model generates a comprehensive
representation of molecules, including atom features, 2D discrete molecule
structures, and 3D continuous molecule coordinates. The use of diffusion
processes allows for capturing the probabilistic nature of molecular processes
and the ability to explore the effect of different factors on molecular
structures and properties. Additionally, we propose a novel graph transformer
architecture to denoise the diffusion process. The transformer is equivariant
to Euclidean transformations, allowing it to learn invariant atom and edge
representations while preserving the equivariance of atom coordinates. This
transformer can be used to learn molecular representations robust to geometric
transformations. We evaluate the performance of our model through experiments
and comparisons with existing methods, showing its ability to generate more
stable and valid molecules with good properties. Our model is a promising
approach for designing molecules with desired properties and can be applied to
a wide range of tasks in molecular modeling.
</p>

## LLM
## segmentation
### Title: SRCNet: Seminal Representation Collaborative Network for Marine Oil Spill Segmentation. (arXiv:2304.14500v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14500](http://arxiv.org/abs/2304.14500)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14500] SRCNet: Seminal Representation Collaborative Network for Marine Oil Spill Segmentation](http://arxiv.org/abs/2304.14500) #segmentation`
* Summary: <p>Effective oil spill segmentation in Synthetic Aperture Radar (SAR) images is
critical for marine oil pollution cleanup, and proper image representation is
helpful for accurate image segmentation. In this paper, we propose an effective
oil spill image segmentation network named SRCNet by leveraging SAR image
representation and the training for oil spill segmentation simultaneously.
Specifically, our proposed segmentation network is constructed with a pair of
deep neural nets with the collaboration of the seminal representation that
describes SAR images, where one deep neural net is the generative net which
strives to produce oil spill segmentation maps, and the other is the
discriminative net which trys its best to distinguish between the produced and
the true segmentations, and they thus built a two-player game. Particularly,
the seminal representation exploited in our proposed SRCNet originates from SAR
imagery, modelling with the internal characteristics of SAR images. Thus, in
the training process, the collaborated seminal representation empowers the
mapped generative net to produce accurate oil spill segmentation maps
efficiently with small amount of training data, promoting the discriminative
net reaching its optimal solution at a fast speed. Therefore, our proposed
SRCNet operates effective oil spill segmentation in an economical and efficient
manner. Additionally, to increase the segmentation capability of the proposed
segmentation network in terms of accurately delineating oil spill details in
SAR images, a regularisation term that penalises the segmentation loss is
devised. This encourages our proposed SRCNet for accurately segmenting oil
spill areas from SAR images. Empirical experimental evaluations from different
metrics validate the effectiveness of our proposed SRCNet for oil spill image
segmentation.
</p>

### Title: DIAMANT: Dual Image-Attention Map Encoders For Medical Image Segmentation. (arXiv:2304.14571v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14571](http://arxiv.org/abs/2304.14571)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14571] DIAMANT: Dual Image-Attention Map Encoders For Medical Image Segmentation](http://arxiv.org/abs/2304.14571) #segmentation`
* Summary: <p>Although purely transformer-based architectures showed promising performance
in many computer vision tasks, many hybrid models consisting of CNN and
transformer blocks are introduced to fit more specialized tasks. Nevertheless,
despite the performance gain of both pure and hybrid transformer-based
architectures compared to CNNs in medical imaging segmentation, their high
training cost and complexity make it challenging to use them in real scenarios.
In this work, we propose simple architectures based on purely convolutional
layers, and show that by just taking advantage of the attention map
visualizations obtained from a self-supervised pretrained vision transformer
network (e.g., DINO) one can outperform complex transformer-based networks with
much less computation costs. The proposed architecture is composed of two
encoder branches with the original image as input in one branch and the
attention map visualizations of the same image from multiple self-attention
heads from a pre-trained DINO model (as multiple channels) in the other branch.
The results of our experiments on two publicly available medical imaging
datasets show that the proposed pipeline outperforms U-Net and the
state-of-the-art medical image segmentation models.
</p>

### Title: SCOPE: Structural Continuity Preservation for Medical Image Segmentation. (arXiv:2304.14572v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14572](http://arxiv.org/abs/2304.14572)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14572] SCOPE: Structural Continuity Preservation for Medical Image Segmentation](http://arxiv.org/abs/2304.14572) #segmentation`
* Summary: <p>Although the preservation of shape continuity and physiological anatomy is a
natural assumption in the segmentation of medical images, it is often neglected
by deep learning methods that mostly aim for the statistical modeling of input
data as pixels rather than interconnected structures. In biological structures,
however, organs are not separate entities; for example, in reality, a severed
vessel is an indication of an underlying problem, but traditional segmentation
models are not designed to strictly enforce the continuity of anatomy,
potentially leading to inaccurate medical diagnoses. To address this issue, we
propose a graph-based approach that enforces the continuity and connectivity of
anatomical topology in medical images. Our method encodes the continuity of
shapes as a graph constraint, ensuring that the network's predictions maintain
this continuity. We evaluate our method on two public benchmarks on retinal
vessel segmentation, showing significant improvements in connectivity metrics
compared to traditional methods while getting better or on-par performance on
segmentation metrics.
</p>

### Title: Pre-processing training data improves accuracy and generalisability of convolutional neural network based landscape semantic segmentation. (arXiv:2304.14625v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14625](http://arxiv.org/abs/2304.14625)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14625] Pre-processing training data improves accuracy and generalisability of convolutional neural network based landscape semantic segmentation](http://arxiv.org/abs/2304.14625) #segmentation`
* Summary: <p>In this paper, we trialled different methods of data preparation for
Convolutional Neural Network (CNN) training and semantic segmentation of land
use land cover (LULC) features within aerial photography over the Wet Tropics
and Atherton Tablelands, Queensland, Australia. This was conducted through
trialling and ranking various training patch selection sampling strategies,
patch and batch sizes and data augmentations and scaling. We also compared
model accuracy through producing the LULC classification using a single pass of
a grid of patches and averaging multiple grid passes and three rotated version
of each patch. Our results showed: a stratified random sampling approach for
producing training patches improved the accuracy of classes with a smaller area
while having minimal effect on larger classes; a smaller number of larger
patches compared to a larger number of smaller patches improves model accuracy;
applying data augmentations and scaling are imperative in creating a
generalised model able to accurately classify LULC features in imagery from a
different date and sensor; and producing the output classification by averaging
multiple grids of patches and three rotated versions of each patch produced and
more accurate and aesthetic result. Combining the findings from the trials, we
fully trained five models on the 2018 training image and applied the model to
the 2015 test image with the output LULC classifications achieving an average
kappa of 0.84 user accuracy of 0.81 and producer accuracy of 0.87. This study
has demonstrated the importance of data pre-processing for developing a
generalised deep-learning model for LULC classification which can be applied to
a different date and sensor. Future research using CNN and earth observation
data should implement the findings of this study to increase LULC model
accuracy and transferability.
</p>

### Title: Differentiable Sensor Layouts for End-to-End Learning of Task-Specific Camera Parameters. (arXiv:2304.14736v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14736](http://arxiv.org/abs/2304.14736)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14736] Differentiable Sensor Layouts for End-to-End Learning of Task-Specific Camera Parameters](http://arxiv.org/abs/2304.14736) #segmentation`
* Summary: <p>The success of deep learning is frequently described as the ability to train
all parameters of a network on a specific application in an end-to-end fashion.
Yet, several design choices on the camera level, including the pixel layout of
the sensor, are considered as pre-defined and fixed, and high resolution,
regular pixel layouts are considered to be the most generic ones in computer
vision and graphics, treating all regions of an image as equally important.
While several works have considered non-uniform, \eg, hexagonal or foveated,
pixel layouts in hardware and image processing, the layout has not been
integrated into the end-to-end learning paradigm so far. In this work, we
present the first truly end-to-end trained imaging pipeline that optimizes the
size and distribution of pixels on the imaging sensor jointly with the
parameters of a given neural network on a specific task. We derive an analytic,
differentiable approach for the sensor layout parameterization that allows for
task-specific, local varying pixel resolutions. We present two pixel layout
parameterization functions: rectangular and curvilinear grid shapes that retain
a regular topology. We provide a drop-in module that approximates sensor
simulation given existing high-resolution images to directly connect our method
with existing deep learning models. We show that network predictions benefit
from learnable pixel layouts for two different downstream tasks, classification
and semantic segmentation.
</p>

### Title: Multi-to-Single Knowledge Distillation for Point Cloud Semantic Segmentation. (arXiv:2304.14800v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14800](http://arxiv.org/abs/2304.14800)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14800] Multi-to-Single Knowledge Distillation for Point Cloud Semantic Segmentation](http://arxiv.org/abs/2304.14800) #segmentation`
* Summary: <p>3D point cloud semantic segmentation is one of the fundamental tasks for
environmental understanding. Although significant progress has been made in
recent years, the performance of classes with few examples or few points is
still far from satisfactory. In this paper, we propose a novel multi-to-single
knowledge distillation framework for the 3D point cloud semantic segmentation
task to boost the performance of those hard classes. Instead of fusing all the
points of multi-scans directly, only the instances that belong to the
previously defined hard classes are fused. To effectively and sufficiently
distill valuable knowledge from multi-scans, we leverage a multilevel
distillation framework, i.e., feature representation distillation, logit
distillation, and affinity distillation. We further develop a novel
instance-aware affinity distillation algorithm for capturing high-level
structural knowledge to enhance the distillation efficacy for hard classes.
Finally, we conduct experiments on the SemanticKITTI dataset, and the results
on both the validation and test sets demonstrate that our method yields
substantial improvements compared with the baseline method. The code is
available at \Url{https://github.com/skyshoumeng/M2SKD}.
</p>

### Title: NeRF-LiDAR: Generating Realistic LiDAR Point Clouds with Neural Radiance Fields. (arXiv:2304.14811v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14811](http://arxiv.org/abs/2304.14811)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14811] NeRF-LiDAR: Generating Realistic LiDAR Point Clouds with Neural Radiance Fields](http://arxiv.org/abs/2304.14811) #segmentation`
* Summary: <p>Labeling LiDAR point clouds for training autonomous driving is extremely
expensive and difficult. LiDAR simulation aims at generating realistic LiDAR
data with labels for training and verifying self-driving algorithms more
efficiently. Recently, Neural Radiance Fields (NeRF) have been proposed for
novel view synthesis using implicit reconstruction of 3D scenes. Inspired by
this, we present NeRF-LIDAR, a novel LiDAR simulation method that leverages
real-world information to generate realistic LIDAR point clouds. Different from
existing LiDAR simulators, we use real images and point cloud data collected by
self-driving cars to learn the 3D scene representation, point cloud generation
and label rendering. We verify the effectiveness of our NeRF-LiDAR by training
different 3D segmentation models on the generated LiDAR point clouds. It
reveals that the trained models are able to achieve similar accuracy when
compared with the same model trained on the real LiDAR data. Besides, the
generated data is capable of boosting the accuracy through pre-training which
helps reduce the requirements of the real labeled data.
</p>

### Title: SFD2: Semantic-guided Feature Detection and Description. (arXiv:2304.14845v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14845](http://arxiv.org/abs/2304.14845)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14845] SFD2: Semantic-guided Feature Detection and Description](http://arxiv.org/abs/2304.14845) #segmentation`
* Summary: <p>Visual localization is a fundamental task for various applications including
autonomous driving and robotics. Prior methods focus on extracting large
amounts of often redundant locally reliable features, resulting in limited
efficiency and accuracy, especially in large-scale environments under
challenging conditions. Instead, we propose to extract globally reliable
features by implicitly embedding high-level semantics into both the detection
and description processes. Specifically, our semantic-aware detector is able to
detect keypoints from reliable regions (e.g. building, traffic lane) and
suppress unreliable areas (e.g. sky, car) implicitly instead of relying on
explicit semantic labels. This boosts the accuracy of keypoint matching by
reducing the number of features sensitive to appearance changes and avoiding
the need of additional segmentation networks at test time. Moreover, our
descriptors are augmented with semantics and have stronger discriminative
ability, providing more inliers at test time. Particularly, experiments on
long-term large-scale visual localization Aachen Day-Night and RobotCar-Seasons
datasets demonstrate that our model outperforms previous local features and
gives competitive accuracy to advanced matchers but is about 2 and 3 times
faster when using 2k and 4k keypoints, respectively.
</p>

### Title: Quality-Adaptive Split-Federated Learning for Segmenting Medical Images with Inaccurate Annotations. (arXiv:2304.14976v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14976](http://arxiv.org/abs/2304.14976)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14976] Quality-Adaptive Split-Federated Learning for Segmenting Medical Images with Inaccurate Annotations](http://arxiv.org/abs/2304.14976) #segmentation`
* Summary: <p>SplitFed Learning, a combination of Federated and Split Learning (FL and SL),
is one of the most recent developments in the decentralized machine learning
domain. In SplitFed learning, a model is trained by clients and a server
collaboratively. For image segmentation, labels are created at each client
independently and, therefore, are subject to clients' bias, inaccuracies, and
inconsistencies. In this paper, we propose a data quality-based adaptive
averaging strategy for SplitFed learning, called QA-SplitFed, to cope with the
variation of annotated ground truth (GT) quality over multiple clients. The
proposed method is compared against five state-of-the-art model averaging
methods on the task of learning human embryo image segmentation. Our
experiments show that all five baseline methods fail to maintain accuracy as
the number of corrupted clients increases. QA-SplitFed, however, copes
effectively with corruption as long as there is at least one uncorrupted
client.
</p>

## object detection
### Title: HyperMODEST: Self-Supervised 3D Object Detection with Confidence Score Filtering. (arXiv:2304.14446v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14446](http://arxiv.org/abs/2304.14446)
* Code URL: [https://github.com/trailab/hypermodest](https://github.com/trailab/hypermodest)
* Copy Paste: `<input type="checkbox">[[2304.14446] HyperMODEST: Self-Supervised 3D Object Detection with Confidence Score Filtering](http://arxiv.org/abs/2304.14446) #object detection`
* Summary: <p>Current LiDAR-based 3D object detectors for autonomous driving are almost
entirely trained on human-annotated data collected in specific geographical
domains with specific sensor setups, making it difficult to adapt to a
different domain. MODEST is the first work to train 3D object detectors without
any labels. Our work, HyperMODEST, proposes a universal method implemented on
top of MODEST that can largely accelerate the self-training process and does
not require tuning on a specific dataset. We filter intermediate pseudo-labels
used for data augmentation with low confidence scores. On the nuScenes dataset,
we observe a significant improvement of 1.6% in AP BEV in 0-80m range at
IoU=0.25 and an improvement of 1.7% in AP BEV in 0-80m range at IoU=0.5 while
only using one-fifth of the training time in the original approach by MODEST.
On the Lyft dataset, we also observe an improvement over the baseline during
the first round of iterative self-training. We explore the trade-off between
high precision and high recall in the early stage of the self-training process
by comparing our proposed method with two other score filtering methods:
confidence score filtering for pseudo-labels with and without static label
retention. The code and models of this work are available at
https://github.com/TRAILab/HyperMODEST
</p>

### Title: Gradient-based Maximally Interfered Retrieval for Domain Incremental 3D Object Detection. (arXiv:2304.14460v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14460](http://arxiv.org/abs/2304.14460)
* Code URL: [https://github.com/trailab/gmir](https://github.com/trailab/gmir)
* Copy Paste: `<input type="checkbox">[[2304.14460] Gradient-based Maximally Interfered Retrieval for Domain Incremental 3D Object Detection](http://arxiv.org/abs/2304.14460) #object detection`
* Summary: <p>Accurate 3D object detection in all weather conditions remains a key
challenge to enable the widespread deployment of autonomous vehicles, as most
work to date has been performed on clear weather data. In order to generalize
to adverse weather conditions, supervised methods perform best if trained from
scratch on all weather data instead of finetuning a model pretrained on clear
weather data. Training from scratch on all data will eventually become
computationally infeasible and expensive as datasets continue to grow and
encompass the full extent of possible weather conditions. On the other hand,
naive finetuning on data from a different weather domain can result in
catastrophic forgetting of the previously learned domain. Inspired by the
success of replay-based continual learning methods, we propose Gradient-based
Maximally Interfered Retrieval (GMIR), a gradient based sampling strategy for
replay. During finetuning, GMIR periodically retrieves samples from the
previous domain dataset whose gradient vectors show maximal interference with
the gradient vector of the current update. Our 3D object detection experiments
on the SeeingThroughFog (STF) dataset show that GMIR not only overcomes
forgetting but also offers competitive performance compared to scratch training
on all data with a 46.25% reduction in total training time.
</p>

### Title: Nordic Vehicle Dataset (NVD): Performance of vehicle detectors using newly captured NVD from UAV in different snowy weather conditions. (arXiv:2304.14466v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14466](http://arxiv.org/abs/2304.14466)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14466] Nordic Vehicle Dataset (NVD): Performance of vehicle detectors using newly captured NVD from UAV in different snowy weather conditions](http://arxiv.org/abs/2304.14466) #object detection`
* Summary: <p>Vehicle detection and recognition in drone images is a complex problem that
has been used for different safety purposes. The main challenge of these images
is captured at oblique angles and poses several challenges like non-uniform
illumination effect, degradations, blur, occlusion, loss of visibility, etc.
Additionally, weather conditions play a crucial role in causing safety concerns
and add another high level of challenge to the collected data. Over the past
few decades, various techniques have been employed to detect and track vehicles
in different weather conditions. However, detecting vehicles in heavy snow is
still in the early stages because of a lack of available data. Furthermore,
there has been no research on detecting vehicles in snowy weather using real
images captured by unmanned aerial vehicles (UAVs). This study aims to address
this gap by providing the scientific community with data on vehicles captured
by UAVs in different settings and under various snow cover conditions in the
Nordic region. The data covers different adverse weather conditions like
overcast with snowfall, low light and low contrast conditions with patchy snow
cover, high brightness, sunlight, fresh snow, and the temperature reaching far
below -0 degrees Celsius. The study also evaluates the performance of commonly
used object detection methods such as Yolo v8, Yolo v5, and fast RCNN.
Additionally, data augmentation techniques are explored, and those that enhance
the detectors' performance in such scenarios are proposed. The code and the
dataset will be available at https://nvd.ltu-ai.dev
</p>

### Title: OriCon3D: Effective 3D Object Detection using Orientation and Confidence. (arXiv:2304.14484v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14484](http://arxiv.org/abs/2304.14484)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14484] OriCon3D: Effective 3D Object Detection using Orientation and Confidence](http://arxiv.org/abs/2304.14484) #object detection`
* Summary: <p>We introduce a technique for detecting 3D objects and estimating their
position from a single image. Our method is built on top of a similar
state-of-the-art technique [1], but with improved accuracy. The approach
followed in this research first estimates common 3D properties of an object
using a Deep Convolutional Neural Network (DCNN), contrary to other frameworks
that only leverage centre-point predictions. We then combine these estimates
with geometric constraints provided by a 2D bounding box to produce a complete
3D bounding box. The first output of our network estimates the 3D object
orientation using a discrete-continuous loss [1]. The second output predicts
the 3D object dimensions with minimal variance. Here we also present our
extensions by augmenting light-weight feature extractors and a customized
multibin architecture. By combining these estimates with the geometric
constraints of the 2D bounding box, we can accurately (or comparatively)
determine the 3D object pose better than our baseline [1] on the KITTI 3D
detection benchmark [2].
</p>

### Title: Vehicle Safety Management System. (arXiv:2304.14497v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14497](http://arxiv.org/abs/2304.14497)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14497] Vehicle Safety Management System](http://arxiv.org/abs/2304.14497) #object detection`
* Summary: <p>Overtaking is a critical maneuver in driving that requires accurate
information about the location and distance of other vehicles on the road. This
study suggests a real-time overtaking assistance system that uses a combination
of the You Only Look Once (YOLO) object detection algorithm and stereo vision
techniques to accurately identify and locate vehicles in front of the driver,
and estimate their distance. The system then signals the vehicles behind the
driver using colored lights to inform them of the safe overtaking distance. The
proposed system has been implemented using Stereo vision for distance analysis
and You Only Look Once (YOLO) for object identification. The results
demonstrate its effectiveness in providing vehicle type and the distance
between the camera module and the vehicle accurately with an approximate error
of 4.107%. Our system has the potential to reduce the risk of accidents and
improve the safety of overtaking maneuvers, especially on busy highways and
roads.
</p>

### Title: Fusion is Not Enough: Single-Modal Attacks to Compromise Fusion Models in Autonomous Driving. (arXiv:2304.14614v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14614](http://arxiv.org/abs/2304.14614)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14614] Fusion is Not Enough: Single-Modal Attacks to Compromise Fusion Models in Autonomous Driving](http://arxiv.org/abs/2304.14614) #object detection`
* Summary: <p>Multi-sensor fusion (MSF) is widely adopted for perception in autonomous
vehicles (AVs), particularly for the task of 3D object detection with camera
and LiDAR sensors. The rationale behind fusion is to capitalize on the
strengths of each modality while mitigating their limitations. The exceptional
and leading performance of fusion models has been demonstrated by advanced deep
neural network (DNN)-based fusion techniques. Fusion models are also perceived
as more robust to attacks compared to single-modal ones due to the redundant
information in multiple modalities. In this work, we challenge this perspective
with single-modal attacks that targets the camera modality, which is considered
less significant in fusion but more affordable for attackers. We argue that the
weakest link of fusion models depends on their most vulnerable modality, and
propose an attack framework that targets advanced camera-LiDAR fusion models
with adversarial patches. Our approach employs a two-stage optimization-based
strategy that first comprehensively assesses vulnerable image areas under
adversarial attacks, and then applies customized attack strategies to different
fusion models, generating deployable patches. Evaluations with five
state-of-the-art camera-LiDAR fusion models on a real-world dataset show that
our attacks successfully compromise all models. Our approach can either reduce
the mean average precision (mAP) of detection performance from 0.824 to 0.353
or degrade the detection score of the target object from 0.727 to 0.151 on
average, demonstrating the effectiveness and practicality of our proposed
attack framework.
</p>

### Title: A positive feedback method based on F-measure value for Salient Object Detection. (arXiv:2304.14619v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.14619](http://arxiv.org/abs/2304.14619)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.14619] A positive feedback method based on F-measure value for Salient Object Detection](http://arxiv.org/abs/2304.14619) #object detection`
* Summary: <p>The majority of current salient object detection (SOD) models are focused on
designing a series of decoders based on fully convolutional networks (FCNs) or
Transformer architectures and integrating them in a skillful manner. These
models have achieved remarkable high performance and made significant
contributions to the development of SOD. Their primary research objective is to
develop novel algorithms that can outperform state-of-the-art models, a task
that is extremely difficult and time-consuming. In contrast, this paper
proposes a positive feedback method based on F-measure value for SOD, aiming to
improve the accuracy of saliency prediction using existing methods.
Specifically, our proposed method takes an image to be detected and inputs it
into several existing models to obtain their respective prediction maps. These
prediction maps are then fed into our positive feedback method to generate the
final prediction result, without the need for careful decoder design or model
training. Moreover, our method is adaptive and can be implemented based on
existing models without any restrictions. Experimental results on five publicly
available datasets show that our proposed positive feedback method outperforms
the latest 12 methods in five evaluation metrics for saliency map prediction.
Additionally, we conducted a robustness experiment, which shows that when at
least one good prediction result exists in the selected existing model, our
proposed approach can ensure that the prediction result is not worse. Our
approach achieves a prediction speed of 20 frames per second (FPS) when
evaluated on a low configuration host and after removing the prediction time
overhead of inserted models. These results highlight the effectiveness,
efficiency, and robustness of our proposed approach for salient object
detection.
</p>

