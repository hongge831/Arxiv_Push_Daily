## data-free
## transformer
### Title: Mitigating Undisciplined Over-Smoothing in Transformer for Weakly Supervised Semantic Segmentation. (arXiv:2305.03112v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03112](http://arxiv.org/abs/2305.03112)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03112] Mitigating Undisciplined Over-Smoothing in Transformer for Weakly Supervised Semantic Segmentation](http://arxiv.org/abs/2305.03112) #transformer`
* Summary: <p>A surge of interest has emerged in weakly supervised semantic segmentation
due to its remarkable efficiency in recent years. Existing approaches based on
transformers mainly focus on exploring the affinity matrix to boost CAMs with
global relationships. While in this work, we first perform a scrupulous
examination towards the impact of successive affinity matrices and discover
that they possess an inclination toward sparsification as the network
approaches convergence, hence disclosing a manifestation of over-smoothing.
Besides, it has been observed that enhanced attention maps tend to evince a
substantial amount of extraneous background noise in deeper layers. Drawing
upon this, we posit a daring conjecture that the undisciplined over-smoothing
phenomenon introduces a noteworthy quantity of semantically irrelevant
background noise, causing performance degradation. To alleviate this issue, we
propose a novel perspective that highlights the objects of interest by
investigating the regions of the trait, thereby fostering an extensive
comprehension of the successive affinity matrix. Consequently, we suggest an
adaptive re-activation mechanism (AReAM) that alleviates the issue of
incomplete attention within the object and the unbounded background noise.
AReAM accomplishes this by supervising high-level attention with shallow
affinity matrices, yielding promising results. Exhaustive experiments conducted
on the commonly used dataset manifest that segmentation results can be greatly
improved through our proposed AReAM, which imposes restrictions on each
affinity matrix in deep layers to make it attentive to semantic regions.
</p>

### Title: Semantic Segmentation using Vision Transformers: A survey. (arXiv:2305.03273v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03273](http://arxiv.org/abs/2305.03273)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03273] Semantic Segmentation using Vision Transformers: A survey](http://arxiv.org/abs/2305.03273) #transformer`
* Summary: <p>Semantic segmentation has a broad range of applications in a variety of
domains including land coverage analysis, autonomous driving, and medical image
analysis. Convolutional neural networks (CNN) and Vision Transformers (ViTs)
provide the architecture models for semantic segmentation. Even though ViTs
have proven success in image classification, they cannot be directly applied to
dense prediction tasks such as image segmentation and object detection since
ViT is not a general purpose backbone due to its patch partitioning scheme. In
this survey, we discuss some of the different ViT architectures that can be
used for semantic segmentation and how their evolution managed the above-stated
challenge. The rise of ViT and its performance with a high success rate
motivated the community to slowly replace the traditional convolutional neural
networks in various computer vision tasks. This survey aims to review and
compare the performances of ViT architectures designed for semantic
segmentation using benchmarking datasets. This will be worthwhile for the
community to yield knowledge regarding the implementations carried out in
semantic segmentation and to discover more efficient methodologies using ViTs.
</p>

### Title: FM-ViT: Flexible Modal Vision Transformers for Face Anti-Spoofing. (arXiv:2305.03277v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03277](http://arxiv.org/abs/2305.03277)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03277] FM-ViT: Flexible Modal Vision Transformers for Face Anti-Spoofing](http://arxiv.org/abs/2305.03277) #transformer`
* Summary: <p>The availability of handy multi-modal (i.e., RGB-D) sensors has brought about
a surge of face anti-spoofing research. However, the current multi-modal face
presentation attack detection (PAD) has two defects: (1) The framework based on
multi-modal fusion requires providing modalities consistent with the training
input, which seriously limits the deployment scenario. (2) The performance of
ConvNet-based model on high fidelity datasets is increasingly limited. In this
work, we present a pure transformer-based framework, dubbed the Flexible Modal
Vision Transformer (FM-ViT), for face anti-spoofing to flexibly target any
single-modal (i.e., RGB) attack scenarios with the help of available
multi-modal data. Specifically, FM-ViT retains a specific branch for each
modality to capture different modal information and introduces the Cross-Modal
Transformer Block (CMTB), which consists of two cascaded attentions named
Multi-headed Mutual-Attention (MMA) and Fusion-Attention (MFA) to guide each
modal branch to mine potential features from informative patch tokens, and to
learn modality-agnostic liveness features by enriching the modal information of
own CLS token, respectively. Experiments demonstrate that the single model
trained based on FM-ViT can not only flexibly evaluate different modal samples,
but also outperforms existing single-modal frameworks by a large margin, and
approaches the multi-modal frameworks introduced with smaller FLOPs and model
parameters.
</p>

### Title: LOGO-Former: Local-Global Spatio-Temporal Transformer for Dynamic Facial Expression Recognition. (arXiv:2305.03343v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03343](http://arxiv.org/abs/2305.03343)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03343] LOGO-Former: Local-Global Spatio-Temporal Transformer for Dynamic Facial Expression Recognition](http://arxiv.org/abs/2305.03343) #transformer`
* Summary: <p>Previous methods for dynamic facial expression recognition (DFER) in the wild
are mainly based on Convolutional Neural Networks (CNNs), whose local
operations ignore the long-range dependencies in videos. Transformer-based
methods for DFER can achieve better performances but result in higher FLOPs and
computational costs. To solve these problems, the local-global spatio-temporal
Transformer (LOGO-Former) is proposed to capture discriminative features within
each frame and model contextual relationships among frames while balancing the
complexity. Based on the priors that facial muscles move locally and facial
expressions gradually change, we first restrict both the space attention and
the time attention to a local window to capture local interactions among
feature tokens. Furthermore, we perform the global attention by querying a
token with features from each local window iteratively to obtain long-range
information of the whole video sequence. In addition, we propose the compact
loss regularization term to further encourage the learned features have the
minimum intra-class distance and the maximum inter-class distance. Experiments
on two in-the-wild dynamic facial expression datasets (i.e., DFEW and FERV39K)
indicate that our method provides an effective way to make use of the spatial
and temporal dependencies for DFER.
</p>

### Title: Optimized Table Tokenization for Table Structure Recognition. (arXiv:2305.03393v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03393](http://arxiv.org/abs/2305.03393)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03393] Optimized Table Tokenization for Table Structure Recognition](http://arxiv.org/abs/2305.03393) #transformer`
* Summary: <p>Extracting tables from documents is a crucial task in any document conversion
pipeline. Recently, transformer-based models have demonstrated that
table-structure can be recognized with impressive accuracy using
Image-to-Markup-Sequence (Im2Seq) approaches. Taking only the image of a table,
such models predict a sequence of tokens (e.g. in HTML, LaTeX) which represent
the structure of the table. Since the token representation of the table
structure has a significant impact on the accuracy and run-time performance of
any Im2Seq model, we investigate in this paper how table-structure
representation can be optimised. We propose a new, optimised table-structure
language (OTSL) with a minimized vocabulary and specific rules. The benefits of
OTSL are that it reduces the number of tokens to 5 (HTML needs 28+) and
shortens the sequence length to half of HTML on average. Consequently, model
accuracy improves significantly, inference time is halved compared to
HTML-based models, and the predicted table structures are always syntactically
correct. This in turn eliminates most post-processing needs.
</p>

### Title: HSCNet++: Hierarchical Scene Coordinate Classification and Regression for Visual Localization with Transformer. (arXiv:2305.03595v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03595](http://arxiv.org/abs/2305.03595)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03595] HSCNet++: Hierarchical Scene Coordinate Classification and Regression for Visual Localization with Transformer](http://arxiv.org/abs/2305.03595) #transformer`
* Summary: <p>Visual localization is critical to many applications in computer vision and
robotics. To address single-image RGB localization, state-of-the-art
feature-based methods match local descriptors between a query image and a
pre-built 3D model. Recently, deep neural networks have been exploited to
regress the mapping between raw pixels and 3D coordinates in the scene, and
thus the matching is implicitly performed by the forward pass through the
network. However, in a large and ambiguous environment, learning such a
regression task directly can be difficult for a single network. In this work,
we present a new hierarchical scene coordinate network to predict pixel scene
coordinates in a coarse-to-fine manner from a single RGB image. The proposed
method, which is an extension of HSCNet, allows us to train compact models
which scale robustly to large environments. It sets a new state-of-the-art for
single-image localization on the 7-Scenes, 12 Scenes, Cambridge Landmarks
datasets, and the combined indoor scenes.
</p>

### Title: COLA: How to adapt vision-language models to Compose Objects Localized with Attributes?. (arXiv:2305.03689v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03689](http://arxiv.org/abs/2305.03689)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03689] COLA: How to adapt vision-language models to Compose Objects Localized with Attributes?](http://arxiv.org/abs/2305.03689) #transformer`
* Summary: <p>Compositional reasoning is a hallmark of human visual intelligence; yet
despite the size of large vision-language models, they struggle to represent
simple compositions by combining objects with their attributes. To measure this
lack of compositional capability, we design Cola, a text-to-image retrieval
benchmark to Compose Objects Localized with Attributes. Using Cola as a
testbed, we explore modeling designs to adapt pre-trained vision-language
models to reason compositionally about multiple attributes attached to multiple
objects. We explore 6 finetuning strategies on 2 seminal vision-language
models, using 3 finetuning datasets and 2 test benchmarks (Cola and CREPE).
Surprisingly, our optimal finetuning strategy improves a 151M parameter CLIP,
which disjointly encodes image and language during pretraining, to perform as
well as a 241M parameter FLAVA, which uses a multi-modal transformer encoder
during pretraining to attend over both vision and language modalities. This
optimal finetuning strategy is a lightweight multi-modal adapter that jointly
attends over both image and language features generated by the pretrained
model. We show this works better than common strategies such as
prompt/fine-tuning, or tuning a comparable number of unimodal layers.
</p>

### Title: Otter: A Multi-Modal Model with In-Context Instruction Tuning. (arXiv:2305.03726v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03726](http://arxiv.org/abs/2305.03726)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03726] Otter: A Multi-Modal Model with In-Context Instruction Tuning](http://arxiv.org/abs/2305.03726) #transformer`
* Summary: <p>Large language models (LLMs) have demonstrated significant universal
capabilities as few/zero-shot learners in various tasks due to their
pre-training on vast amounts of text data, as exemplified by GPT-3, which
boosted to InstrctGPT and ChatGPT, effectively following natural language
instructions to accomplish real-world tasks. In this paper, we propose to
introduce instruction tuning into multi-modal models, motivated by the Flamingo
model's upstream interleaved format pretraining dataset. We adopt a similar
approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT)
dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo
(open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and
showcasing improved instruction-following ability and in-context learning. We
also optimize OpenFlamingo's implementation for researchers, democratizing the
required training resources from 1$\times$ A100 GPU to 4$\times$ RTX-3090 GPUs,
and integrate both OpenFlamingo and Otter into Huggingface Transformers for
more researchers to incorporate the models into their customized training and
inference pipelines.
</p>

### Title: Curating corpora with classifiers: A case study of clean energy sentiment online. (arXiv:2305.03092v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03092](http://arxiv.org/abs/2305.03092)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03092] Curating corpora with classifiers: A case study of clean energy sentiment online](http://arxiv.org/abs/2305.03092) #transformer`
* Summary: <p>Well curated, large-scale corpora of social media posts containing broad
public opinion offer an alternative data source to complement traditional
surveys. While surveys are effective at collecting representative samples and
are capable of achieving high accuracy, they can be both expensive to run and
lag public opinion by days or weeks. Both of these drawbacks could be overcome
with a real-time, high volume data stream and fast analysis pipeline. A central
challenge in orchestrating such a data pipeline is devising an effective method
for rapidly selecting the best corpus of relevant documents for analysis.
Querying with keywords alone often includes irrelevant documents that are not
easily disambiguated with bag-of-words natural language processing methods.
Here, we explore methods of corpus curation to filter irrelevant tweets using
pre-trained transformer-based models, fine-tuned for our binary classification
task on hand-labeled tweets. We are able to achieve F1 scores of up to 0.95.
The low cost and high performance of fine-tuning such a model suggests that our
approach could be of broad benefit as a pre-processing step for social media
datasets with uncertain corpus boundaries.
</p>

### Title: Chain-of-Skills: A Configurable Model for Open-domain Question Answering. (arXiv:2305.03130v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03130](http://arxiv.org/abs/2305.03130)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03130] Chain-of-Skills: A Configurable Model for Open-domain Question Answering](http://arxiv.org/abs/2305.03130) #transformer`
* Summary: <p>The retrieval model is an indispensable component for real-world
knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As
separate retrieval skills are annotated for different datasets, recent work
focuses on customized methods, limiting the model transferability and
scalability. In this work, we propose a modular retriever where individual
modules correspond to key skills that can be reused across datasets. Our
approach supports flexible skill configurations based on the target domain to
boost performance. To mitigate task interference, we design a novel
modularization parameterization inspired by sparse Transformer. We demonstrate
that our model can benefit from self-supervised pretraining on Wikipedia and
fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our
approach outperforms recent self-supervised retrievers in zero-shot evaluations
and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA
and OTT-QA.
</p>

### Title: The Role of Global and Local Context in Named Entity Recognition. (arXiv:2305.03132v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03132](http://arxiv.org/abs/2305.03132)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03132] The Role of Global and Local Context in Named Entity Recognition](http://arxiv.org/abs/2305.03132) #transformer`
* Summary: <p>Pre-trained transformer-based models have recently shown great performance
when applied to Named Entity Recognition (NER). As the complexity of their
self-attention mechanism prevents them from processing long documents at once,
these models are usually applied in a sequential fashion. Such an approach
unfortunately only incorporates local context and prevents leveraging global
document context in long documents such as novels, which might hinder
performance. In this article, we explore the impact of global document context,
and its relationships with local context. We find that correctly retrieving
global document context has a greater impact on performance than only
leveraging local context, prompting for further research on how to better
retrieve that context.
</p>

### Title: Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing. (arXiv:2305.03195v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03195](http://arxiv.org/abs/2305.03195)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03195] Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing](http://arxiv.org/abs/2305.03195) #transformer`
* Summary: <p>Generative Pre-trained Transformer 4 (GPT-4) is the fourth-generation
language model in the GPT series, developed by OpenAI, which promises
significant advancements in the field of natural language processing (NLP). In
this research article, we have discussed the features of GPT-4, its potential
applications, and the challenges that it might face. We have also compared
GPT-4 with its predecessor, GPT-3. GPT-4 has a larger model size (more than one
trillion), better multilingual capabilities, improved contextual understanding,
and reasoning capabilities than GPT-3. Some of the potential applications of
GPT-4 include chatbots, personal assistants, language translation, text
summarization, and question-answering. However, GPT-4 poses several challenges
and limitations such as computational requirements, data requirements, and
ethical concerns.
</p>

### Title: Neuromodulation Gated Transformer. (arXiv:2305.03232v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03232](http://arxiv.org/abs/2305.03232)
* Code URL: [https://github.com/kobeknowles/neuromodulation-gated-transformer](https://github.com/kobeknowles/neuromodulation-gated-transformer)
* Copy Paste: `<input type="checkbox">[[2305.03232] Neuromodulation Gated Transformer](http://arxiv.org/abs/2305.03232) #transformer`
* Summary: <p>We introduce a novel architecture, the Neuromodulation Gated Transformer
(NGT), which is a simple implementation of neuromodulation in transformers via
a multiplicative effect. We compare it to baselines and show that it results in
the best average performance on the SuperGLUE benchmark validation sets.
</p>

### Title: Online Gesture Recognition using Transformer and Natural Language Processing. (arXiv:2305.03407v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03407](http://arxiv.org/abs/2305.03407)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03407] Online Gesture Recognition using Transformer and Natural Language Processing](http://arxiv.org/abs/2305.03407) #transformer`
* Summary: <p>The Transformer architecture is shown to provide a powerful machine
transduction framework for online handwritten gestures corresponding to glyph
strokes of natural language sentences. The attention mechanism is successfully
used to create latent representations of an end-to-end encoder-decoder model,
solving multi-level segmentation while also learning some language features and
syntax rules. The additional use of a large decoding space with some learned
Byte-Pair-Encoding (BPE) is shown to provide robustness to ablated inputs and
syntax rules. The encoder stack was directly fed with spatio-temporal data
tokens potentially forming an infinitely large input vocabulary, an approach
that finds applications beyond that of this work. Encoder transfer learning
capabilities is also demonstrated on several languages resulting in faster
optimisation and shared parameters. A new supervised dataset of online
handwriting gestures suitable for generic handwriting recognition tasks was
used to successfully train a small transformer model to an average normalised
Levenshtein accuracy of 96% on English or German sentences and 94% in French.
</p>

### Title: Using ChatGPT for Entity Matching. (arXiv:2305.03423v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03423](http://arxiv.org/abs/2305.03423)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03423] Using ChatGPT for Entity Matching](http://arxiv.org/abs/2305.03423) #transformer`
* Summary: <p>Entity Matching is the task of deciding if two entity descriptions refer to
the same real-world entity. State-of-the-art entity matching methods often rely
on fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacks
of using these models for entity matching are that (i) the models require
significant amounts of fine-tuning data for reaching a good performance and
(ii) the fine-tuned models are not robust concerning out-of-distribution
entities. In this paper, we investigate using ChatGPT for entity matching as a
more robust, training data-efficient alternative to traditional Transformer
models. We perform experiments along three dimensions: (i) general prompt
design, (ii) in-context learning, and (iii) provision of higher-level matching
knowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model,
reaching an average zero-shot performance of 83% F1 on a challenging matching
task on which RoBERTa requires 2000 training examples for reaching a similar
performance. Adding in-context demonstrations to the prompts further improves
the F1 by up to 5% even using only a small set of 20 handpicked examples.
Finally, we show that guiding the zero-shot model by stating higher-level
matching rules leads to similar gains as providing in-context examples.
</p>

### Title: Cancer Hallmark Classification Using Bidirectional Encoder Representations From Transformers. (arXiv:2305.03501v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03501](http://arxiv.org/abs/2305.03501)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03501] Cancer Hallmark Classification Using Bidirectional Encoder Representations From Transformers](http://arxiv.org/abs/2305.03501) #transformer`
* Summary: <p>This paper presents a novel approach to accurately classify the hallmarks of
cancer, which is a crucial task in cancer research. Our proposed method
utilizes the Bidirectional Encoder Representations from Transformers (BERT)
architecture, which has shown exceptional performance in various downstream
applications. By applying transfer learning, we fine-tuned the pre-trained BERT
model on a small corpus of biomedical text documents related to cancer. The
outcomes of our experimental investigations demonstrate that our approach
attains a noteworthy accuracy of 94.45%, surpassing almost all prior findings
with a substantial increase of at least 8.04% as reported in the literature.
These findings highlight the effectiveness of our proposed model in accurately
classifying and comprehending text documents for cancer research, thus
contributing significantly to the field. As cancer remains one of the top ten
leading causes of death globally, our approach holds great promise in advancing
cancer research and improving patient outcomes.
</p>

### Title: White-Box Multi-Objective Adversarial Attack on Dialogue Generation. (arXiv:2305.03655v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03655](http://arxiv.org/abs/2305.03655)
* Code URL: [https://github.com/yul091/dgslow](https://github.com/yul091/dgslow)
* Copy Paste: `<input type="checkbox">[[2305.03655] White-Box Multi-Objective Adversarial Attack on Dialogue Generation](http://arxiv.org/abs/2305.03655) #transformer`
* Summary: <p>Pre-trained transformers are popular in state-of-the-art dialogue generation
(DG) systems. Such language models are, however, vulnerable to various
adversarial samples as studied in traditional tasks such as text
classification, which inspires our curiosity about their robustness in DG
systems. One main challenge of attacking DG models is that perturbations on the
current sentence can hardly degrade the response accuracy because the unchanged
chat histories are also considered for decision-making. Instead of merely
pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that
crafting adversarial samples to force longer generation outputs benefits attack
effectiveness -- the generated responses are typically irrelevant, lengthy, and
repetitive. To this end, we propose a white-box multi-objective attack method
called DGSlow. Specifically, DGSlow balances two objectives -- generation
accuracy and length, via a gradient-based multi-objective optimizer and applies
an adaptive searching mechanism to iteratively craft adversarial samples with
only a few modifications. Comprehensive experiments on four benchmark datasets
demonstrate that DGSlow could significantly degrade state-of-the-art DG models
with a higher success rate than traditional accuracy-based methods. Besides,
our crafted sentences also exhibit strong transferability in attacking other
models.
</p>

### Title: Predicting COVID-19 and pneumonia complications from admission texts. (arXiv:2305.03661v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03661](http://arxiv.org/abs/2305.03661)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03661] Predicting COVID-19 and pneumonia complications from admission texts](http://arxiv.org/abs/2305.03661) #transformer`
* Summary: <p>In this paper we present a novel approach to risk assessment for patients
hospitalized with pneumonia or COVID-19 based on their admission reports. We
applied a Longformer neural network to admission reports and other textual data
available shortly after admission to compute risk scores for the patients. We
used patient data of multiple European hospitals to demonstrate that our
approach outperforms the Transformer baselines. Our experiments show that the
proposed model generalises across institutions and diagnoses. Also, our method
has several other advantages described in the paper.
</p>

### Title: G-MATT: Single-step Retrosynthesis Prediction using Molecular Grammar Tree Transformer. (arXiv:2305.03153v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.03153](http://arxiv.org/abs/2305.03153)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03153] G-MATT: Single-step Retrosynthesis Prediction using Molecular Grammar Tree Transformer](http://arxiv.org/abs/2305.03153) #transformer`
* Summary: <p>In recent years, several reaction templates-based and template-free
approaches have been reported for single-step retrosynthesis prediction. Even
though many of these approaches perform well from traditional data-driven
metrics standpoint, there is a disconnect between model architectures used and
underlying chemistry principles governing retrosynthesis. Here, we propose a
novel chemistry-aware retrosynthesis prediction framework that combines
powerful data-driven models with chemistry knowledge. We report a
tree-to-sequence transformer architecture based on hierarchical SMILES grammar
trees as input containing underlying chemistry information that is otherwise
ignored by models based on purely SMILES-based representations. The proposed
framework, grammar-based molecular attention tree transformer (G-MATT),
achieves significant performance improvements compared to baseline
retrosynthesis models. G-MATT achieves a top-1 accuracy of 51% (top-10 accuracy
of 79.1%), invalid rate of 1.5%, and bioactive similarity rate of 74.8%.
Further analyses based on attention maps demonstrate G-MATT's ability to
preserve chemistry knowledge without having to use extremely complex model
architectures.
</p>

### Title: A technical note on bilinear layers for interpretability. (arXiv:2305.03452v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.03452](http://arxiv.org/abs/2305.03452)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03452] A technical note on bilinear layers for interpretability](http://arxiv.org/abs/2305.03452) #transformer`
* Summary: <p>The ability of neural networks to represent more features than neurons makes
interpreting them challenging. This phenomenon, known as superposition, has
spurred efforts to find architectures that are more interpretable than standard
multilayer perceptrons (MLPs) with elementwise activation functions. In this
note, I examine bilinear layers, which are a type of MLP layer that are
mathematically much easier to analyze while simultaneously performing better
than standard MLPs. Although they are nonlinear functions of their input, I
demonstrate that bilinear layers can be expressed using only linear operations
and third order tensors. We can integrate this expression for bilinear layers
into a mathematical framework for transformer circuits, which was previously
limited to attention-only transformers. These results suggest that bilinear
layers are easier to analyze mathematically than current architectures and thus
may lend themselves to deeper safety insights by allowing us to talk more
formally about circuits in neural networks. Additionally, bilinear layers may
offer an alternative path for mechanistic interpretability through
understanding the mechanisms of feature construction instead of enumerating a
(potentially exponentially) large number of features in large models.
</p>

## generative
### Title: VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation. (arXiv:2305.03204v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03204](http://arxiv.org/abs/2305.03204)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03204] VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation](http://arxiv.org/abs/2305.03204) #generative`
* Summary: <p>We propose a new two-stage pre-training framework for video-to-text
generation tasks such as video captioning and video question answering: A
generative encoder-decoder model is first jointly pre-trained on massive
image-text data to learn fundamental vision-language concepts, and then adapted
to video data in an intermediate video-text pre-training stage to learn
video-specific skills such as spatio-temporal reasoning. As a result, our
VideoOFA model achieves new state-of-the-art performance on four Video
Captioning benchmarks, beating prior art by an average of 9.7 points in CIDEr
score. It also outperforms existing models on two open-ended Video Question
Answering datasets, showcasing its generalization capability as a universal
video-to-text model.
</p>

### Title: Data Curation for Image Captioning with Text-to-Image Generative Models. (arXiv:2305.03610v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03610](http://arxiv.org/abs/2305.03610)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03610] Data Curation for Image Captioning with Text-to-Image Generative Models](http://arxiv.org/abs/2305.03610) #generative`
* Summary: <p>Recent advances in image captioning are mainly driven by large-scale
vision-language pretraining, relying heavily on computational resources and
increasingly large multimodal datasets. Instead of scaling up pretraining data,
we ask whether it is possible to improve performance by improving the quality
of the samples in existing datasets. We pursue this question through two
approaches to data curation: one that assumes that some examples should be
avoided due to mismatches between the image and caption, and one that assumes
that the mismatch can be addressed by replacing the image, for which we use the
state-of-the-art Stable Diffusion model. These approaches are evaluated using
the BLIP model on MS COCO and Flickr30K in both finetuning and few-shot
learning settings. Our simple yet effective approaches consistently outperform
baselines, indicating that better image captioning models can be trained by
curating existing resources. Finally, we conduct a human study to understand
the errors made by the Stable Diffusion model and highlight directions for
future work in text-to-image generation.
</p>

### Title: A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding. (arXiv:2305.03668v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03668](http://arxiv.org/abs/2305.03668)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03668] A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding](http://arxiv.org/abs/2305.03668) #generative`
* Summary: <p>Webpages have been a rich, scalable resource for vision-language and language
only tasks. Yet only pieces of webpages are kept: image-caption pairs, long
text articles, or raw HTML, never all in one place. Webpage tasks have
resultingly received little attention and structured image-text data left
underused. To study multimodal webpage understanding, we introduce the
Wikipedia Webpage suite (WikiWeb2M) of 2M pages. We verify its utility on three
generative tasks: page description generation, section summarization, and
contextual image captioning. We design a novel attention mechanism Prefix
Global, which selects the most relevant image and text content as global tokens
to attend to the rest of the webpage for context. By using page structure to
separate such tokens, it performs better than full attention with lower
computational complexity. Experiments show that the new annotations from
WikiWeb2M improve task performance compared to data from prior work. We also
include ablations on sequence length, input features, and model size.
</p>

### Title: Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models. (arXiv:2305.03660v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03660](http://arxiv.org/abs/2305.03660)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03660] Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models](http://arxiv.org/abs/2305.03660) #generative`
* Summary: <p>We propose Retrieval Augmented Generation (RAG) as an approach for automated
radiology report writing that leverages multimodally aligned embeddings from a
contrastively pretrained vision language model for retrieval of relevant
candidate radiology text for an input radiology image and a general domain
generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for
report generation using the relevant radiology text retrieved. This approach
keeps hallucinated generations under check and provides capabilities to
generate report content in the format we desire leveraging the instruction
following capabilities of these generative models. Our approach achieves better
clinical metrics with a BERTScore of 0.2865 ({\Delta}+ 25.88%) and Semb score
of 0.4026 ({\Delta}+ 6.31%). Our approach can be broadly relevant for different
clinical settings as it allows to augment the automated radiology report
generation process with content relevant for that setting while also having the
ability to inject user intents and requirements in the prompts as part of the
report generation process to modulate the content and format of the generated
reports as applicable for that clinical setting.
</p>

### Title: A Generative Modeling Framework for Inferring Families of Biomechanical Constitutive Laws in Data-Sparse Regimes. (arXiv:2305.03184v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.03184](http://arxiv.org/abs/2305.03184)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03184] A Generative Modeling Framework for Inferring Families of Biomechanical Constitutive Laws in Data-Sparse Regimes](http://arxiv.org/abs/2305.03184) #generative`
* Summary: <p>Quantifying biomechanical properties of the human vasculature could deepen
our understanding of cardiovascular diseases. Standard nonlinear regression in
constitutive modeling requires considerable high-quality data and an explicit
form of the constitutive model as prior knowledge. By contrast, we propose a
novel approach that combines generative deep learning with Bayesian inference
to efficiently infer families of constitutive relationships in data-sparse
regimes. Inspired by the concept of functional priors, we develop a generative
adversarial network (GAN) that incorporates a neural operator as the generator
and a fully-connected neural network as the discriminator. The generator takes
a vector of noise conditioned on measurement data as input and yields the
predicted constitutive relationship, which is scrutinized by the discriminator
in the following step. We demonstrate that this framework can accurately
estimate means and standard deviations of the constitutive relationships of the
murine aorta using data collected either from model-generated synthetic data or
ex vivo experiments for mice with genetic deficiencies. In addition, the
framework learns priors of constitutive models without explicitly knowing their
functional form, providing a new model-agnostic approach to learning hidden
constitutive behaviors from data.
</p>

## label correction
## noise
### Title: Contrastive Learning for Low-light Raw Denoising. (arXiv:2305.03352v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03352](http://arxiv.org/abs/2305.03352)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03352] Contrastive Learning for Low-light Raw Denoising](http://arxiv.org/abs/2305.03352) #noise`
* Summary: <p>Image/video denoising in low-light scenes is an extremely challenging problem
due to limited photon count and high noise. In this paper, we propose a novel
approach with contrastive learning to address this issue. Inspired by the
success of contrastive learning used in some high-level computer vision tasks,
we bring in this idea to the low-level denoising task. In order to achieve this
goal, we introduce a new denoising contrastive regularization (DCR) to exploit
the information of noisy images and clean images. In the feature space, DCR
makes the denoised image closer to the clean image and far away from the noisy
image. In addition, we build a new feature embedding network called Wnet, which
is more effective to extract high-frequency information. We conduct the
experiments on a real low-light dataset that captures still images taken on a
moonless clear night in 0.6 millilux and videos under starlight (no moon
present, &lt;0.001 lux). The results show that our method can achieve a higher
PSNR and better visual quality compared with existing methods
</p>

### Title: Block the Label and Noise: An N-Gram Masked Speller for Chinese Spell Checking. (arXiv:2305.03314v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03314](http://arxiv.org/abs/2305.03314)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03314] Block the Label and Noise: An N-Gram Masked Speller for Chinese Spell Checking](http://arxiv.org/abs/2305.03314) #noise`
* Summary: <p>Recently, Chinese Spell Checking(CSC), a task to detect erroneous characters
in a sentence and correct them, has attracted extensive interest because of its
wide applications in various NLP tasks. Most of the existing methods have
utilized BERT to extract semantic information for CSC task. However, these
methods directly take sentences with only a few errors as inputs, where the
correct characters may leak answers to the model and dampen its ability to
capture distant context; while the erroneous characters may disturb the
semantic encoding process and result in poor representations. Based on such
observations, this paper proposes an n-gram masking layer that masks current
and/or surrounding tokens to avoid label leakage and error disturbance.
Moreover, considering that the mask strategy may ignore multi-modal information
indicated by errors, a novel dot-product gating mechanism is proposed to
integrate the phonological and morphological information with semantic
representation. Extensive experiments on SIGHAN datasets have demonstrated that
the pluggable n-gram masking mechanism can improve the performance of prevalent
CSC models and the proposed methods in this paper outperform multiple powerful
state-of-the-art models.
</p>

### Title: Over-the-Air Federated Averaging with Limited Power and Privacy Budgets. (arXiv:2305.03547v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.03547](http://arxiv.org/abs/2305.03547)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03547] Over-the-Air Federated Averaging with Limited Power and Privacy Budgets](http://arxiv.org/abs/2305.03547) #noise`
* Summary: <p>To jointly overcome the communication bottleneck and privacy leakage of
wireless federated learning (FL), this paper studies a differentially private
over-the-air federated averaging (DP-OTA-FedAvg) system with a limited sum
power budget. With DP-OTA-FedAvg, the gradients are aligned by an alignment
coefficient and aggregated over the air, and channel noise is employed to
protect privacy. We aim to improve the learning performance by jointly
designing the device scheduling, alignment coefficient, and the number of
aggregation rounds of federated averaging (FedAvg) subject to sum power and
privacy constraints. We first present the privacy analysis based on
differential privacy (DP) to quantify the impact of the alignment coefficient
on privacy preservation in each communication round. Furthermore, to study how
the device scheduling, alignment coefficient, and the number of the global
aggregation affect the learning process, we conduct the convergence analysis of
DP-OTA-FedAvg in the cases of convex and non-convex loss functions. Based on
these analytical results, we formulate an optimization problem to minimize the
optimality gap of the DP-OTA-FedAvg subject to limited sum power and privacy
budgets. The problem is solved by decoupling it into two sub-problems. Given
the number of communication rounds, we conclude the relationship between the
number of scheduled devices and the alignment coefficient, which offers a set
of potential optimal solution pairs of device scheduling and the alignment
coefficient. Thanks to the reduced search space, the optimal solution can be
efficiently obtained. The effectiveness of the proposed policy is validated
through simulations.
</p>

### Title: Optimizing Hyperparameters with Conformal Quantile Regression. (arXiv:2305.03623v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.03623](http://arxiv.org/abs/2305.03623)
* Code URL: [https://github.com/geoalgo/syne-tune](https://github.com/geoalgo/syne-tune)
* Copy Paste: `<input type="checkbox">[[2305.03623] Optimizing Hyperparameters with Conformal Quantile Regression](http://arxiv.org/abs/2305.03623) #noise`
* Summary: <p>Many state-of-the-art hyperparameter optimization (HPO) algorithms rely on
model-based optimizers that learn surrogate models of the target function to
guide the search. Gaussian processes are the de facto surrogate model due to
their ability to capture uncertainty but they make strong assumptions about the
observation noise, which might not be warranted in practice. In this work, we
propose to leverage conformalized quantile regression which makes minimal
assumptions about the observation noise and, as a result, models the target
function in a more realistic and robust fashion which translates to quicker HPO
convergence on empirical benchmarks. To apply our method in a multi-fidelity
setting, we propose a simple, yet effective, technique that aggregates observed
results across different resource levels and outperforms conventional methods
across many empirical tasks.
</p>

## diffusion
### Title: DisenBooth: Disentangled Parameter-Efficient Tuning for Subject-Driven Text-to-Image Generation. (arXiv:2305.03374v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03374](http://arxiv.org/abs/2305.03374)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03374] DisenBooth: Disentangled Parameter-Efficient Tuning for Subject-Driven Text-to-Image Generation](http://arxiv.org/abs/2305.03374) #diffusion`
* Summary: <p>Given a small set of images of a specific subject, subject-driven
text-to-image generation aims to generate customized images of the subject
according to new text descriptions, which has attracted increasing attention in
the community recently. Current subject-driven text-to-image generation methods
are mainly based on finetuning a pretrained large-scale text-to-image
generation model. However, these finetuning methods map the images of the
subject into an embedding highly entangled with subject-identity-unrelated
information, which may result in the inconsistency between the generated images
and the text descriptions and the changes in the subject identity. To tackle
the problem, we propose DisenBooth, a disentangled parameter-efficient tuning
framework for subject-driven text-to-image generation. DisenBooth enables
generating new images that simultaneously preserve the subject identity and
conform to the text descriptions, by disentangling the embedding into an
identity-related and an identity-unrelated part. Specifically, DisenBooth is
based on the pretrained diffusion models and conducts finetuning in the
diffusion denoising process, where a shared identity embedding and an
image-specific identity-unrelated embedding are utilized jointly for denoising
each image. To make the two embeddings disentangled, two auxiliary objectives
are proposed. Additionally, to improve the finetuning efficiency, a
parameter-efficient finetuning strategy is adopted. Extensive experiments show
that our DisenBooth can faithfully learn well-disentangled identity-related and
identity-unrelated embeddings. With the shared identity embedding, DisenBooth
demonstrates superior subject-driven text-to-image generation ability.
Additionally, DisenBooth provides a more flexible and controllable framework
with different combinations of the disentangled embeddings.
</p>

### Title: Guided Image Synthesis via Initial Image Editing in Diffusion Model. (arXiv:2305.03382v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03382](http://arxiv.org/abs/2305.03382)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03382] Guided Image Synthesis via Initial Image Editing in Diffusion Model](http://arxiv.org/abs/2305.03382) #diffusion`
* Summary: <p>Diffusion models have the ability to generate high quality images by
denoising pure Gaussian noise images. While previous research has primarily
focused on improving the control of image generation through adjusting the
denoising process, we propose a novel direction of manipulating the initial
noise to control the generated image. Through experiments on stable diffusion,
we show that blocks of pixels in the initial latent images have a preference
for generating specific content, and that modifying these blocks can
significantly influence the generated image. In particular, we show that
modifying a part of the initial image affects the corresponding region of the
generated image while leaving other regions unaffected, which is useful for
repainting tasks. Furthermore, we find that the generation preferences of pixel
blocks are primarily determined by their values, rather than their position. By
moving pixel blocks with a tendency to generate user-desired content to
user-specified regions, our approach achieves state-of-the-art performance in
layout-to-image generation. Our results highlight the flexibility and power of
initial image manipulation in controlling the generated image.
</p>

### Title: Conditional Diffusion Feature Refinement for Continuous Sign Language Recognition. (arXiv:2305.03614v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03614](http://arxiv.org/abs/2305.03614)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03614] Conditional Diffusion Feature Refinement for Continuous Sign Language Recognition](http://arxiv.org/abs/2305.03614) #diffusion`
* Summary: <p>In this work, we are dedicated to leveraging the denoising diffusion models'
success and formulating feature refinement as the autoencoder-formed diffusion
process. The state-of-the-art CSLR framework consists of a spatial module, a
visual module, a sequence module, and a sequence learning function. However,
this framework has faced sequence module overfitting caused by the objective
function and small-scale available benchmarks, resulting in insufficient model
training. To overcome the overfitting problem, some CSLR studies enforce the
sequence module to learn more visual temporal information or be guided by more
informative supervision to refine its representations. In this work, we propose
a novel autoencoder-formed conditional diffusion feature refinement~(ACDR) to
refine the sequence representations to equip desired properties by learning the
encoding-decoding optimization process in an end-to-end way. Specifically, for
the ACDR, a noising Encoder is proposed to progressively add noise equipped
with semantic conditions to the sequence representations. And a denoising
Decoder is proposed to progressively denoise the noisy sequence representations
with semantic conditions. Therefore, the sequence representations can be imbued
with the semantics of provided semantic conditions. Further, a semantic
constraint is employed to prevent the denoised sequence representations from
semantic corruption. Extensive experiments are conducted to validate the
effectiveness of our ACDR, benefiting state-of-the-art methods and achieving a
notable gain on three benchmarks.
</p>

### Title: Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03509](http://arxiv.org/abs/2305.03509)
* Code URL: [https://github.com/poloclub/diffusion-explainer](https://github.com/poloclub/diffusion-explainer)
* Copy Paste: `<input type="checkbox">[[2305.03509] Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion](http://arxiv.org/abs/2305.03509) #diffusion`
* Summary: <p>Diffusion-based generative models' impressive ability to create convincing
images has captured global attention. However, their complex internal
structures and operations often make them difficult for non-experts to
understand. We present Diffusion Explainer, the first interactive visualization
tool that explains how Stable Diffusion transforms text prompts into images.
Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's
complex components with detailed explanations of their underlying operations,
enabling users to fluidly transition between multiple levels of abstraction
through animations and interactive elements. By comparing the evolutions of
image representations guided by two related text prompts over refinement
timesteps, users can discover the impact of prompts on image generation.
Diffusion Explainer runs locally in users' web browsers without the need for
installation or specialized hardware, broadening the public's education access
to modern AI techniques. Our open-sourced tool is available at:
https://poloclub.github.io/diffusion-explainer/.
</p>

## LLM
## segmentation
### Title: HAISTA-NET: Human Assisted Instance Segmentation Through Attention. (arXiv:2305.03105v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03105](http://arxiv.org/abs/2305.03105)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03105] HAISTA-NET: Human Assisted Instance Segmentation Through Attention](http://arxiv.org/abs/2305.03105) #segmentation`
* Summary: <p>Instance segmentation is a form of image detection which has a range of
applications, such as object refinement, medical image analysis, and
image/video editing, all of which demand a high degree of accuracy. However,
this precision is often beyond the reach of what even state-of-the-art, fully
automated instance segmentation algorithms can deliver. The performance gap
becomes particularly prohibitive for small and complex objects. Practitioners
typically resort to fully manual annotation, which can be a laborious process.
In order to overcome this problem, we propose a novel approach to enable more
precise predictions and generate higher-quality segmentation masks for
high-curvature, complex and small-scale objects. Our human-assisted
segmentation model, HAISTA-NET, augments the existing Strong Mask R-CNN network
to incorporate human-specified partial boundaries. We also present a dataset of
hand-drawn partial object boundaries, which we refer to as human attention
maps. In addition, the Partial Sketch Object Boundaries (PSOB) dataset contains
hand-drawn partial object boundaries which represent curvatures of an object's
ground truth mask with several pixels. Through extensive evaluation using the
PSOB dataset, we show that HAISTA-NET outperforms state-of-the art methods such
as Mask R-CNN, Strong Mask R-CNN, and Mask2Former, achieving respective
increases of +36.7, +29.6, and +26.5 points in AP-Mask metrics for these three
models. We hope that our novel approach will set a baseline for future
human-aided deep learning models by combining fully automated and interactive
instance segmentation architectures.
</p>

### Title: Smaller3d: Smaller Models for 3D Semantic Segmentation Using Minkowski Engine and Knowledge Distillation Methods. (arXiv:2305.03188v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03188](http://arxiv.org/abs/2305.03188)
* Code URL: [https://github.com/madanela/smaller3d](https://github.com/madanela/smaller3d)
* Copy Paste: `<input type="checkbox">[[2305.03188] Smaller3d: Smaller Models for 3D Semantic Segmentation Using Minkowski Engine and Knowledge Distillation Methods](http://arxiv.org/abs/2305.03188) #segmentation`
* Summary: <p>There are various optimization techniques in the realm of 3D, including point
cloud-based approaches that use mesh, texture, and voxels which optimize how
you store, and how do calculate in 3D. These techniques employ methods such as
feed-forward networks, 3D convolutions, graph neural networks, transformers,
and sparse tensors. However, the field of 3D is one of the most computationally
expensive fields, and these methods have yet to achieve their full potential
due to their large capacity, complexity, and computation limits. This paper
proposes the application of knowledge distillation techniques, especially for
sparse tensors in 3D deep learning, to reduce model sizes while maintaining
performance. We analyze and purpose different loss functions, including
standard methods and combinations of various losses, to simulate the
performance of state-of-the-art models of different Sparse Convolutional NNs.
Our experiments are done on the standard ScanNet V2 dataset, and we achieved
around 2.6\% mIoU difference with a 4 times smaller model and around 8\% with a
16 times smaller model on the latest state-of-the-art spacio-temporal convents
based models.
</p>

### Title: Clothes Grasping and Unfolding Based on RGB-D Semantic Segmentation. (arXiv:2305.03259v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03259](http://arxiv.org/abs/2305.03259)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03259] Clothes Grasping and Unfolding Based on RGB-D Semantic Segmentation](http://arxiv.org/abs/2305.03259) #segmentation`
* Summary: <p>Clothes grasping and unfolding is a core step in robotic-assisted dressing.
Most existing works leverage depth images of clothes to train a deep
learning-based model to recognize suitable grasping points. These methods often
utilize physics engines to synthesize depth images to reduce the cost of real
labeled data collection. However, the natural domain gap between synthetic and
real images often leads to poor performance of these methods on real data.
Furthermore, these approaches often struggle in scenarios where grasping points
are occluded by the clothing item itself. To address the above challenges, we
propose a novel Bi-directional Fractal Cross Fusion Network (BiFCNet) for
semantic segmentation, enabling recognition of graspable regions in order to
provide more possibilities for grasping. Instead of using depth images only, we
also utilize RGB images with rich color features as input to our network in
which the Fractal Cross Fusion (FCF) module fuses RGB and depth data by
considering global complex features based on fractal geometry. To reduce the
cost of real data collection, we further propose a data augmentation method
based on an adversarial strategy, in which the color and geometric
transformations simultaneously process RGB and depth data while maintaining the
label correspondence. Finally, we present a pipeline for clothes grasping and
unfolding from the perspective of semantic segmentation, through the addition
of a strategy for grasp point selection from segmentation regions based on
clothing flatness measures, while taking into account the grasping direction.
We evaluate our BiFCNet on the public dataset NYUDv2 and obtained comparable
performance to current state-of-the-art models. We also deploy our model on a
Baxter robot, running extensive grasping and unfolding experiments as part of
our ablation studies, achieving an 84% success rate.
</p>

### Title: BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks. (arXiv:2305.03289v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03289](http://arxiv.org/abs/2305.03289)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03289] BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks](http://arxiv.org/abs/2305.03289) #segmentation`
* Summary: <p>Recently, the Segment Anything Model (SAM) has gained significant attention
as an image segmentation foundation model due to its strong performance on
various downstream tasks. However, it has been found that SAM does not always
perform satisfactorily when faced with challenging downstream tasks. This has
led downstream users to demand a customized SAM model that can be adapted to
these downstream tasks. In this paper, we present BadSAM, the first backdoor
attack on the image segmentation foundation model. Our preliminary experiments
on the CAMO dataset demonstrate the effectiveness of BadSAM.
</p>

### Title: Asynchronous Events-based Panoptic Segmentation using Graph Mixer Neural Network. (arXiv:2305.03640v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03640](http://arxiv.org/abs/2305.03640)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03640] Asynchronous Events-based Panoptic Segmentation using Graph Mixer Neural Network](http://arxiv.org/abs/2305.03640) #segmentation`
* Summary: <p>In the context of robotic grasping, object segmentation encounters several
difficulties when faced with dynamic conditions such as real-time operation,
occlusion, low lighting, motion blur, and object size variability. In response
to these challenges, we propose the Graph Mixer Neural Network that includes a
novel collaborative contextual mixing layer, applied to 3D event graphs formed
on asynchronous events. The proposed layer is designed to spread spatiotemporal
correlation within an event graph at four nearest neighbor levels parallelly.
We evaluate the effectiveness of our proposed method on the Event-based
Segmentation (ESD) Dataset, which includes five unique image degradation
challenges, including occlusion, blur, brightness, trajectory, scale variance,
and segmentation of known and unknown objects. The results show that our
proposed approach outperforms state-of-the-art methods in terms of mean
intersection over the union and pixel accuracy. Code available at:
https://github.com/sanket0707/GNN-Mixer.git
</p>

### Title: Investigating Lexical Sharing in Multilingual Machine Translation for Indian Languages. (arXiv:2305.03207v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.03207](http://arxiv.org/abs/2305.03207)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03207] Investigating Lexical Sharing in Multilingual Machine Translation for Indian Languages](http://arxiv.org/abs/2305.03207) #segmentation`
* Summary: <p>Multilingual language models have shown impressive cross-lingual transfer
ability across a diverse set of languages and tasks. To improve the
cross-lingual ability of these models, some strategies include transliteration
and finer-grained segmentation into characters as opposed to subwords. In this
work, we investigate lexical sharing in multilingual machine translation (MT)
from Hindi, Gujarati, Nepali into English. We explore the trade-offs that exist
in translation performance between data sampling and vocabulary size, and we
explore whether transliteration is useful in encouraging cross-script
generalisation. We also verify how the different settings generalise to unseen
languages (Marathi and Bengali). We find that transliteration does not give
pronounced improvements and our analysis suggests that our multilingual MT
models trained on original scripts seem to already be robust to cross-script
differences even for relatively low-resource languages
</p>

## object detection
### Title: GAANet: Ghost Auto Anchor Network for Detecting Varying Size Drones in Dark. (arXiv:2305.03425v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03425](http://arxiv.org/abs/2305.03425)
* Code URL: [https://github.com/zeeshankaleem/ghostautoanchornet](https://github.com/zeeshankaleem/ghostautoanchornet)
* Copy Paste: `<input type="checkbox">[[2305.03425] GAANet: Ghost Auto Anchor Network for Detecting Varying Size Drones in Dark](http://arxiv.org/abs/2305.03425) #object detection`
* Summary: <p>The usage of drones has tremendously increased in different sectors spanning
from military to industrial applications. Despite all the benefits they offer,
their misuse can lead to mishaps, and tackling them becomes more challenging
particularly at night due to their small size and low visibility conditions. To
overcome those limitations and improve the detection accuracy at night, we
propose an object detector called Ghost Auto Anchor Network (GAANet) for
infrared (IR) images. The detector uses a YOLOv5 core to address challenges in
object detection for IR images, such as poor accuracy and a high false alarm
rate caused by extended altitudes, poor lighting, and low image resolution. To
improve performance, we implemented auto anchor calculation, modified the
conventional convolution block to ghost-convolution, adjusted the input channel
size, and used the AdamW optimizer. To enhance the precision of multiscale tiny
object recognition, we also introduced an additional extra-small object feature
extractor and detector. Experimental results in a custom IR dataset with
multiple classes (birds, drones, planes, and helicopters) demonstrate that
GAANet shows improvement compared to state-of-the-art detectors. In comparison
to GhostNet-YOLOv5, GAANet has higher overall mean average precision (mAP@50),
recall, and precision around 2.5\%, 2.3\%, and 1.4\%, respectively. The dataset
and code for this paper are available as open source at
https://github.com/ZeeshanKaleem/GhostAutoAnchorNet.
</p>

### Title: Human Attention-Guided Explainable Artificial Intelligence for Computer Vision Models. (arXiv:2305.03601v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03601](http://arxiv.org/abs/2305.03601)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.03601] Human Attention-Guided Explainable Artificial Intelligence for Computer Vision Models](http://arxiv.org/abs/2305.03601) #object detection`
* Summary: <p>We examined whether embedding human attention knowledge into saliency-based
explainable AI (XAI) methods for computer vision models could enhance their
plausibility and faithfulness. We first developed new gradient-based XAI
methods for object detection models to generate object-specific explanations by
extending the current methods for image classification models. Interestingly,
while these gradient-based methods worked well for explaining image
classification models, when being used for explaining object detection models,
the resulting saliency maps generally had lower faithfulness than human
attention maps when performing the same task. We then developed Human
Attention-Guided XAI (HAG-XAI) to learn from human attention how to best
combine explanatory information from the models to enhance explanation
plausibility by using trainable activation functions and smoothing kernels to
maximize XAI saliency map's similarity to human attention maps. While for image
classification models, HAG-XAI enhanced explanation plausibility at the expense
of faithfulness, for object detection models it enhanced plausibility and
faithfulness simultaneously and outperformed existing methods. The learned
functions were model-specific, well generalizable to other databases.
</p>

### Title: DSPDet3D: Dynamic Spatial Pruning for 3D Small Object Detection. (arXiv:2305.03716v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.03716](http://arxiv.org/abs/2305.03716)
* Code URL: [https://github.com/xuxw98/dspdet3d](https://github.com/xuxw98/dspdet3d)
* Copy Paste: `<input type="checkbox">[[2305.03716] DSPDet3D: Dynamic Spatial Pruning for 3D Small Object Detection](http://arxiv.org/abs/2305.03716) #object detection`
* Summary: <p>In this paper, we propose a new detection framework for 3D small object
detection. Although deep learning-based 3D object detection methods have
achieved great success in recent years, current methods still struggle on small
objects due to weak geometric information. With in-depth study, we find
increasing the spatial resolution of the feature maps significantly boosts the
performance of 3D small object detection. And more interestingly, though the
computational overhead increases dramatically with resolution, the growth
mainly comes from the upsampling operation of the decoder. Inspired by this, we
present a high-resolution multi-level detector with dynamic spatial pruning
named DSPDet3D, which detects objects from large to small by iterative
upsampling and meanwhile prunes the spatial representation of the scene at
regions where there is no smaller object to be detected in higher resolution.
As the 3D detector only needs to predict sparse bounding boxes, pruning a large
amount of uninformative features does not degrade the detection performance but
significantly reduces the computational cost of upsampling. In this way, our
DSPDet3D achieves high accuracy on small object detection while requiring even
less memory footprint and inference time. On ScanNet and TO-SCENE dataset, our
method improves the detection performance of small objects to a new level while
achieving leading inference speed among all mainstream indoor 3D object
detection methods.
</p>

