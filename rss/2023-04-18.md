## data-free
## transformer
### Title: Fairness in Visual Clustering: A Novel Transformer Clustering Approach. (arXiv:2304.07408v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07408](http://arxiv.org/abs/2304.07408)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07408] Fairness in Visual Clustering: A Novel Transformer Clustering Approach](http://arxiv.org/abs/2304.07408) #transformer`
* Summary: <p>Promoting fairness for deep clustering models in unsupervised clustering
settings to reduce demographic bias is a challenging goal. This is because of
the limitation of large-scale balanced data with well-annotated labels for
sensitive or protected attributes. In this paper, we first evaluate demographic
bias in deep clustering models from the perspective of cluster purity, which is
measured by the ratio of positive samples within a cluster to their correlation
degree. This measurement is adopted as an indication of demographic bias. Then,
a novel loss function is introduced to encourage a purity consistency for all
clusters to maintain the fairness aspect of the learned clustering model.
Moreover, we present a novel attention mechanism, Cross-attention, to measure
correlations between multiple clusters, strengthening faraway positive samples
and improving the purity of clusters during the learning process. Experimental
results on a large-scale dataset with numerous attribute settings have
demonstrated the effectiveness of the proposed approach on both clustering
accuracy and fairness enhancement on several sensitive attributes.
</p>

### Title: Masked Pre-Training of Transformers for Histology Image Analysis. (arXiv:2304.07434v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07434](http://arxiv.org/abs/2304.07434)
* Code URL: [https://github.com/bmirds/wsi-plp](https://github.com/bmirds/wsi-plp)
* Copy Paste: `<input type="checkbox">[[2304.07434] Masked Pre-Training of Transformers for Histology Image Analysis](http://arxiv.org/abs/2304.07434) #transformer`
* Summary: <p>In digital pathology, whole slide images (WSIs) are widely used for
applications such as cancer diagnosis and prognosis prediction. Visual
transformer models have recently emerged as a promising method for encoding
large regions of WSIs while preserving spatial relationships among patches.
However, due to the large number of model parameters and limited labeled data,
applying transformer models to WSIs remains challenging. Inspired by masked
language models, we propose a pretext task for training the transformer model
without labeled data to address this problem. Our model, MaskHIT, uses the
transformer output to reconstruct masked patches and learn representative
histological features based on their positions and visual features. The
experimental results demonstrate that MaskHIT surpasses various multiple
instance learning approaches by 3% and 2% on survival prediction and cancer
subtype classification tasks, respectively. Furthermore, MaskHIT also
outperforms two of the most recent state-of-the-art transformer-based methods.
Finally, a comparison between the attention maps generated by the MaskHIT model
with pathologist's annotations indicates that the model can accurately identify
clinically relevant histological structures in each task.
</p>

### Title: MA-ViT: Modality-Agnostic Vision Transformers for Face Anti-Spoofing. (arXiv:2304.07549v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07549](http://arxiv.org/abs/2304.07549)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07549] MA-ViT: Modality-Agnostic Vision Transformers for Face Anti-Spoofing](http://arxiv.org/abs/2304.07549) #transformer`
* Summary: <p>The existing multi-modal face anti-spoofing (FAS) frameworks are designed
based on two strategies: halfway and late fusion. However, the former requires
test modalities consistent with the training input, which seriously limits its
deployment scenarios. And the latter is built on multiple branches to process
different modalities independently, which limits their use in applications with
low memory or fast execution requirements. In this work, we present a single
branch based Transformer framework, namely Modality-Agnostic Vision Transformer
(MA-ViT), which aims to improve the performance of arbitrary modal attacks with
the help of multi-modal data. Specifically, MA-ViT adopts the early fusion to
aggregate all the available training modalities data and enables flexible
testing of any given modal samples. Further, we develop the Modality-Agnostic
Transformer Block (MATB) in MA-ViT, which consists of two stacked attentions
named Modal-Disentangle Attention (MDA) and Cross-Modal Attention (CMA), to
eliminate modality-related information for each modal sequences and supplement
modality-agnostic liveness features from another modal sequences, respectively.
Experiments demonstrate that the single model trained based on MA-ViT can not
only flexibly evaluate different modal samples, but also outperforms existing
single-modal frameworks by a large margin, and approaches the multi-modal
frameworks introduced with smaller FLOPs and model parameters.
</p>

### Title: Obstacle-Transformer: A Trajectory Prediction Network Based on Surrounding Trajectories. (arXiv:2304.07711v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07711](http://arxiv.org/abs/2304.07711)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07711] Obstacle-Transformer: A Trajectory Prediction Network Based on Surrounding Trajectories](http://arxiv.org/abs/2304.07711) #transformer`
* Summary: <p>Recurrent Neural Network, Long Short-Term Memory, and Transformer have made
great progress in predicting the trajectories of moving objects. Although the
trajectory element with the surrounding scene features has been merged to
improve performance, there still exist some problems to be solved. One is that
the time series processing models will increase the inference time with the
increase of the number of prediction sequences. Another lies in which the
features can not be extracted from the scene's image and point cloud in some
situations. Therefore, this paper proposes an Obstacle-Transformer to predict
trajectory in a constant inference time. An ``obstacle'' is designed by the
surrounding trajectory rather than images or point clouds, making
Obstacle-Transformer more applicable in a wider range of scenarios. Experiments
are conducted on ETH and UCY data sets to verify the performance of our model.
</p>

### Title: PCPNet: An Efficient and Semantic-Enhanced Transformer Network for Point Cloud Prediction. (arXiv:2304.07773v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07773](http://arxiv.org/abs/2304.07773)
* Code URL: [https://github.com/blurryface0814/pcpnet](https://github.com/blurryface0814/pcpnet)
* Copy Paste: `<input type="checkbox">[[2304.07773] PCPNet: An Efficient and Semantic-Enhanced Transformer Network for Point Cloud Prediction](http://arxiv.org/abs/2304.07773) #transformer`
* Summary: <p>The ability to predict future structure features of environments based on
past perception information is extremely needed by autonomous vehicles, which
helps to make the following decision-making and path planning more reasonable.
Recently, point cloud prediction (PCP) is utilized to predict and describe
future environmental structures by the point cloud form. In this letter, we
propose a novel efficient Transformer-based network to predict the future LiDAR
point clouds exploiting the past point cloud sequences. We also design a
semantic auxiliary training strategy to make the predicted LiDAR point cloud
sequence semantically similar to the ground truth and thus improves the
significance of the deployment for more tasks in real-vehicle applications. Our
approach is completely self-supervised, which means it does not require any
manual labeling and has a solid generalization ability toward different
environments. The experimental results show that our method outperforms the
state-of-the-art PCP methods on the prediction results and semantic similarity,
and has a good real-time performance. Our open-source code and pre-trained
models are available at https://github.com/Blurryface0814/PCPNet.
</p>

### Title: EGformer: Equirectangular Geometry-biased Transformer for 360 Depth Estimation. (arXiv:2304.07803v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07803](http://arxiv.org/abs/2304.07803)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07803] EGformer: Equirectangular Geometry-biased Transformer for 360 Depth Estimation](http://arxiv.org/abs/2304.07803) #transformer`
* Summary: <p>Estimating the depths of equirectangular (360) images (EIs) is challenging
given the distorted 180 x 360 field-of-view, which is hard to be addressed via
convolutional neural network (CNN). Although a transformer with global
attention achieves significant improvements over CNN for EI depth estimation
task, it is computationally inefficient, which raises the need for transformer
with local attention. However, to apply local attention successfully for EIs, a
specific strategy, which addresses distorted equirectangular geometry and
limited receptive field simultaneously, is required. Prior works have only
cared either of them, resulting in unsatisfactory depths occasionally. In this
paper, we propose an equirectangular geometry-biased transformer termed
EGformer, which enables local attention extraction in a global manner
considering the equirectangular geometry. To achieve this, we actively utilize
the equirectangular geometry as the bias for the local attention instead of
struggling to reduce the distortion of EIs. As compared to the most recent
transformer based EI depth estimation studies, the proposed approach yields the
best depth outcomes overall with the lowest computational cost and the fewest
parameters, demonstrating the effectiveness of the proposed methods.
</p>

### Title: A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer. (arXiv:2304.07874v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07874](http://arxiv.org/abs/2304.07874)
* Code URL: [https://github.com/yangyiliu21/ntire2023_itbdehaze](https://github.com/yangyiliu21/ntire2023_itbdehaze)
* Copy Paste: `<input type="checkbox">[[2304.07874] A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer](http://arxiv.org/abs/2304.07874) #transformer`
* Summary: <p>Recent years have witnessed an increased interest in image dehazing. Many
deep learning methods have been proposed to tackle this challenge, and have
made significant accomplishments dealing with homogeneous haze. However, these
solutions cannot maintain comparable performance when they are applied to
images with non-homogeneous haze, e.g., NH-HAZE23 dataset introduced by NTIRE
challenges. One of the reasons for such failures is that non-homogeneous haze
does not obey one of the assumptions that is required for modeling homogeneous
haze. In addition, a large number of pairs of non-homogeneous hazy image and
the clean counterpart is required using traditional end-to-end training
approaches, while NH-HAZE23 dataset is of limited quantities. Although it is
possible to augment the NH-HAZE23 dataset by leveraging other non-homogeneous
dehazing datasets, we observe that it is necessary to design a proper
data-preprocessing approach that reduces the distribution gaps between the
target dataset and the augmented one. This finding indeed aligns with the
essence of data-centric AI. With a novel network architecture and a principled
data-preprocessing approach that systematically enhances data quality, we
present an innovative dehazing method. Specifically, we apply RGB-channel-wise
transformations on the augmented datasets, and incorporate the state-of-the-art
transformers as the backbone in the two-branch framework. We conduct extensive
experiments and ablation study to demonstrate the effectiveness of our proposed
method.
</p>

### Title: A CTC Alignment-based Non-autoregressive Transformer for End-to-end Automatic Speech Recognition. (arXiv:2304.07611v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.07611](http://arxiv.org/abs/2304.07611)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07611] A CTC Alignment-based Non-autoregressive Transformer for End-to-end Automatic Speech Recognition](http://arxiv.org/abs/2304.07611) #transformer`
* Summary: <p>Recently, end-to-end models have been widely used in automatic speech
recognition (ASR) systems. Two of the most representative approaches are
connectionist temporal classification (CTC) and attention-based encoder-decoder
(AED) models. Autoregressive transformers, variants of AED, adopt an
autoregressive mechanism for token generation and thus are relatively slow
during inference. In this paper, we present a comprehensive study of a CTC
Alignment-based Single-Step Non-Autoregressive Transformer (CASS-NAT) for
end-to-end ASR. In CASS-NAT, word embeddings in the autoregressive transformer
(AT) are substituted with token-level acoustic embeddings (TAE) that are
extracted from encoder outputs with the acoustical boundary information offered
by the CTC alignment. TAE can be obtained in parallel, resulting in a parallel
generation of output tokens. During training, Viterbi-alignment is used for TAE
generation, and multiple training strategies are further explored to improve
the word error rate (WER) performance. During inference, an error-based
alignment sampling method is investigated in depth to reduce the alignment
mismatch in the training and testing processes. Experimental results show that
the CASS-NAT has a WER that is close to AT on various ASR tasks, while
providing a ~24x inference speedup. With and without self-supervised learning,
we achieve new state-of-the-art results for non-autoregressive models on
several datasets. We also analyze the behavior of the CASS-NAT decoder to
explain why it can perform similarly to AT. We find that TAEs have similar
functionality to word embeddings for grammatical structures, which might
indicate the possibility of learning some semantic information from TAEs
without a language model.
</p>

### Title: MLRegTest: A Benchmark for the Machine Learning of Regular Languages. (arXiv:2304.07687v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.07687](http://arxiv.org/abs/2304.07687)
* Code URL: [https://github.com/heinz-jeffrey/subregular-learning](https://github.com/heinz-jeffrey/subregular-learning)
* Copy Paste: `<input type="checkbox">[[2304.07687] MLRegTest: A Benchmark for the Machine Learning of Regular Languages](http://arxiv.org/abs/2304.07687) #transformer`
* Summary: <p>Evaluating machine learning (ML) systems on their ability to learn known
classifiers allows fine-grained examination of the patterns they can learn,
which builds confidence when they are applied to the learning of unknown
classifiers. This article presents a new benchmark for ML systems on sequence
classification called MLRegTest, which contains training, development, and test
sets from 1,800 regular languages.
</p>
<p>Different kinds of formal languages represent different kinds of
long-distance dependencies, and correctly identifying long-distance
dependencies in sequences is a known challenge for ML systems to generalize
successfully. MLRegTest organizes its languages according to their logical
complexity (monadic second order, first order, propositional, or monomial
expressions) and the kind of logical literals (string, tier-string,
subsequence, or combinations thereof). The logical complexity and choice of
literal provides a systematic way to understand different kinds of
long-distance dependencies in regular languages, and therefore to understand
the capacities of different ML systems to learn such long-distance
dependencies.
</p>
<p>Finally, the performance of different neural networks (simple RNN, LSTM, GRU,
transformer) on MLRegTest is examined. The main conclusion is that their
performance depends significantly on the kind of test set, the class of
language, and the neural network architecture.
</p>

### Title: MisRoB{\AE}RTa: Transformers versus Misinformation. (arXiv:2304.07759v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.07759](http://arxiv.org/abs/2304.07759)
* Code URL: [https://github.com/cipriantruica/misrobaerta_transformers-vs-misinformation](https://github.com/cipriantruica/misrobaerta_transformers-vs-misinformation)
* Copy Paste: `<input type="checkbox">[[2304.07759] MisRoB{\AE}RTa: Transformers versus Misinformation](http://arxiv.org/abs/2304.07759) #transformer`
* Summary: <p>Misinformation is considered a threat to our democratic values and
principles. The spread of such content on social media polarizes society and
undermines public discourse by distorting public perceptions and generating
social unrest while lacking the rigor of traditional journalism. Transformers
and transfer learning proved to be state-of-the-art methods for multiple
well-known natural language processing tasks. In this paper, we propose
MisRoB{\AE}RTa, a novel transformer-based deep neural ensemble architecture for
misinformation detection. MisRoB{\AE}RTa takes advantage of two transformers
(BART \&amp; RoBERTa) to improve the classification performance. We also
benchmarked and evaluated the performances of multiple transformers on the task
of misinformation detection. For training and testing, we used a large
real-world news articles dataset labeled with 10 classes, addressing two
shortcomings in the current research: increasing the size of the dataset from
small to large, and moving the focus of fake news detection from binary
classification to multi-class classification. For this dataset, we manually
verified the content of the news articles to ensure that they were correctly
labeled. The experimental results show that the accuracy of transformers on the
misinformation detection problem was significantly influenced by the method
employed to learn the context, dataset size, and vocabulary dimension. We
observe empirically that the best accuracy performance among the classification
models that use only one transformer is obtained by BART, while DistilRoBERTa
obtains the best accuracy in the least amount of time required for fine-tuning
and training. The proposed MisRoB{\AE}RTa outperforms the other transformer
models in the task of misinformation detection. To arrive at this conclusion,
we performed ample ablation and sensitivity testing with MisRoB{\AE}RTa on two
datasets.
</p>

### Title: Shuffled Transformer for Privacy-Preserving Split Learning. (arXiv:2304.07735v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2304.07735](http://arxiv.org/abs/2304.07735)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07735] Shuffled Transformer for Privacy-Preserving Split Learning](http://arxiv.org/abs/2304.07735) #transformer`
* Summary: <p>In conventional split learning, training and testing data often face severe
privacy leakage threats. Existing solutions often have to trade learning
accuracy for data privacy, or the other way around. We propose a lossless
privacy-preserving split learning framework, on the basis of the permutation
equivalence properties which are inherent to many neural network modules. We
adopt Transformer as the example building block to the framework. It is proved
that the Transformer encoder block is permutation equivalent, and thus
training/testing could be done equivalently on permuted data. We further
introduce shuffling-based privacy guarantee and enhance it by mix-up training.
All properties are verified by conducted experiments, which also show strong
defence against privacy attacks compared to the state-of-the-art, yet without
any accuracy decline.
</p>

### Title: Automated Program Repair Based on Code Review: How do Pre-trained Transformer Models Perform?. (arXiv:2304.07840v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.07840](http://arxiv.org/abs/2304.07840)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07840] Automated Program Repair Based on Code Review: How do Pre-trained Transformer Models Perform?](http://arxiv.org/abs/2304.07840) #transformer`
* Summary: <p>Sequence-to-sequence models have been used to transform erroneous programs
into correct ones when trained with a large enough dataset. Some recent studies
also demonstrated strong empirical evidence that code review (natural language
instruction about suggestive changes in code) can improve the program repair
further. Large language models, trained with Natural Language (NL) and computer
program corpora, have the capacity to contain inherent knowledge of both. In
this study, we investigate if this inherent knowledge of code and NL can be
utilized to improve automated program repair. We applied PLBART and CodeT5, two
state-of-the-art language models that are pre-trained with both Programming
Language (PL) and Natural Language (NL), on two such natural language-based
program repair datasets and found that the pre-trained language models
fine-tuned with datasets containing both code review and subsequent code
changes notably outperform each of the previous models. We observed that the
pre-trained models improve the previously best-reported results by 9.91% on the
Review4Repair dataset and by 24.72% on the dataset by Tufano et al. This
suggests that a pre-trained sequential model has a better understanding of
natural language and can utilize it much better. We performed an ablation study
to assess the contribution of the pre-training mechanism and the model
architecture. We found that pre-training was significantly more important in
the performance gain than the model architecture. The practical application of
using pre-trained transformer models in the context of automated program repair
is still a long way off. However, our study demonstrates the substantial value
of employing pre-trained models, paving the path for future studies to use more
of these.
</p>

## generative
### Title: Text-Conditional Contextualized Avatars For Zero-Shot Personalization. (arXiv:2304.07410v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07410](http://arxiv.org/abs/2304.07410)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07410] Text-Conditional Contextualized Avatars For Zero-Shot Personalization](http://arxiv.org/abs/2304.07410) #generative`
* Summary: <p>Recent large-scale text-to-image generation models have made significant
improvements in the quality, realism, and diversity of the synthesized images
and enable users to control the created content through language. However, the
personalization aspect of these generative models is still challenging and
under-explored. In this work, we propose a pipeline that enables
personalization of image generation with avatars capturing a user's identity in
a delightful way. Our pipeline is zero-shot, avatar texture and style agnostic,
and does not require training on the avatar at all - it is scalable to millions
of users who can generate a scene with their avatar. To render the avatar in a
pose faithful to the given text prompt, we propose a novel text-to-3D pose
diffusion model trained on a curated large-scale dataset of in-the-wild human
poses improving the performance of the SOTA text-to-motion models
significantly. We show, for the first time, how to leverage large-scale image
datasets to learn human 3D pose parameters and overcome the limitations of
motion capture datasets.
</p>

### Title: ID2image: Leakage of non-ID information into face descriptors and inversion from descriptors to images. (arXiv:2304.07522v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07522](http://arxiv.org/abs/2304.07522)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07522] ID2image: Leakage of non-ID information into face descriptors and inversion from descriptors to images](http://arxiv.org/abs/2304.07522) #generative`
* Summary: <p>Embedding a face image to a descriptor vector using a deep CNN is a widely
used technique in face recognition. Via several possible training strategies,
such embeddings are supposed to capture only identity information. Information
about the environment (such as background and lighting) or changeable aspects
of the face (such as pose, expression, presence of glasses, hat etc.) should be
discarded since they are not useful for recognition. In this paper, we present
a surprising result that this is not the case. We show that non-ID attributes,
as well as landmark positions and the image histogram can be recovered from the
ID embedding of state-of-the-art face embedding networks (VGGFace2 and
ArcFace). In fact, these non-ID attributes can be predicted from ID embeddings
with similar accuracy to a prediction from the original image. Going further,
we present an optimisation strategy that uses a generative model (specifically
StyleGAN2 for faces) to recover images from an ID embedding. We show
photorealistic inversion from ID embedding to face image in which not only is
the ID realistically reconstructed but the pose, lighting and
background/apparel to some extent as well.
</p>

### Title: A Novel end-to-end Framework for Occluded Pixel Reconstruction with Spatio-temporal Features for Improved Person Re-identification. (arXiv:2304.07721v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07721](http://arxiv.org/abs/2304.07721)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07721] A Novel end-to-end Framework for Occluded Pixel Reconstruction with Spatio-temporal Features for Improved Person Re-identification](http://arxiv.org/abs/2304.07721) #generative`
* Summary: <p>Person re-identification is vital for monitoring and tracking crowd movement
to enhance public security. However, re-identification in the presence of
occlusion substantially reduces the performance of existing systems and is a
challenging area. In this work, we propose a plausible solution to this problem
by developing effective occlusion detection and reconstruction framework for
RGB images/videos consisting of Deep Neural Networks. Specifically, a CNN-based
occlusion detection model classifies individual input frames, followed by a
Conv-LSTM and Autoencoder to reconstruct the occluded pixels corresponding to
the occluded frames for sequential (video) and non-sequential (image) data,
respectively. The quality of the reconstructed RGB frames is further refined
and fine-tuned using a Conditional Generative Adversarial Network (cGAN). Our
method is evaluated on four well-known public data sets of the domain, and the
qualitative reconstruction results are indeed appealing. Quantitative
evaluation in terms of re-identification accuracy of the Siamese network showed
an exceptional Rank-1 accuracy after occluded pixel reconstruction on various
datasets. A comparative analysis with state-of-the-art approaches also
demonstrates the robustness of our work for use in real-life surveillance
systems.
</p>

### Title: ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models. (arXiv:2304.07666v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.07666](http://arxiv.org/abs/2304.07666)
* Code URL: [https://github.com/huhailinguist/argugpt](https://github.com/huhailinguist/argugpt)
* Copy Paste: `<input type="checkbox">[[2304.07666] ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models](http://arxiv.org/abs/2304.07666) #generative`
* Summary: <p>AI generated content (AIGC) presents considerable challenge to educators
around the world. Instructors need to be able to detect such text generated by
large language models, either with the naked eye or with the help of some
tools. There is also growing need to understand the lexical, syntactic and
stylistic features of AIGC. To address these challenges in English language
teaching, we first present ArguGPT, a balanced corpus of 4,038 argumentative
essays generated by 7 GPT models in response to essay prompts from three
sources: (1) in-class or homework exercises, (2) TOEFL and (3) GRE writing
tasks. Machine-generated texts are paired with roughly equal number of
human-written essays with three score levels matched in essay prompts. We then
hire English instructors to distinguish machine essays from human ones. Results
show that when first exposed to machine-generated essays, the instructors only
have an accuracy of 61% in detecting them. But the number rises to 67% after
one round of minimal self-training. Next, we perform linguistic analyses of
these essays, which show that machines produce sentences with more complex
syntactic structures while human essays tend to be lexically more complex.
Finally, we test existing AIGC detectors and build our own detectors using SVMs
and RoBERTa. Results suggest that a RoBERTa fine-tuned with the training set of
ArguGPT achieves above 90% accuracy in both essay- and sentence-level
classification. To the best of our knowledge, this is the first comprehensive
analysis of argumentative essays produced by generative large language models.
Machine-authored essays in ArguGPT and our models will be made publicly
available at https://github.com/huhailinguist/ArguGPT
</p>

### Title: SikuGPT: A Generative Pre-trained Model for Intelligent Information Processing of Ancient Texts from the Perspective of Digital Humanities. (arXiv:2304.07778v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.07778](http://arxiv.org/abs/2304.07778)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07778] SikuGPT: A Generative Pre-trained Model for Intelligent Information Processing of Ancient Texts from the Perspective of Digital Humanities](http://arxiv.org/abs/2304.07778) #generative`
* Summary: <p>The rapid advance in artificial intelligence technology has facilitated the
prosperity of digital humanities research. Against such backdrop, research
methods need to be transformed in the intelligent processing of ancient texts,
which is a crucial component of digital humanities research, so as to adapt to
new development trends in the wave of AIGC. In this study, we propose a GPT
model called SikuGPT based on the corpus of Siku Quanshu. The model's
performance in tasks such as intralingual translation and text classification
exceeds that of other GPT-type models aimed at processing ancient texts.
SikuGPT's ability to process traditional Chinese ancient texts can help promote
the organization of ancient information and knowledge services, as well as the
international dissemination of Chinese ancient culture.
</p>

### Title: ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human. (arXiv:2304.07849v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2304.07849](http://arxiv.org/abs/2304.07849)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07849] ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human](http://arxiv.org/abs/2304.07849) #generative`
* Summary: <p>In this paper, we present ChatPLUG, a Chinese open-domain dialogue system for
digital human applications that instruction finetunes on a wide range of
dialogue tasks in a unified internet-augmented format. Different from other
open-domain dialogue models that focus on large-scale pre-training and scaling
up model size or dialogue corpus, we aim to build a powerful and practical
dialogue system for digital human with diverse skills and good multi-task
generalization by internet-augmented instruction tuning. To this end, we first
conduct large-scale pre-training on both common document corpus and dialogue
data with curriculum learning, so as to inject various world knowledge and
dialogue abilities into ChatPLUG. Then, we collect a wide range of dialogue
tasks spanning diverse features of knowledge, personality, multi-turn memory,
and empathy, on which we further instruction tune \modelname via unified
natural language instruction templates. External knowledge from an internet
search is also used during instruction finetuning for alleviating the problem
of knowledge hallucinations. We show that \modelname outperforms
state-of-the-art Chinese dialogue systems on both automatic and human
evaluation, and demonstrates strong multi-task generalization on a variety of
text understanding and generation tasks. In addition, we deploy \modelname to
real-world applications such as Smart Speaker and Instant Message applications
with fast inference. Our models and code will be made publicly available on
ModelScope~\footnote{\small{https://modelscope.cn/models/damo/ChatPLUG-3.7B}}
and Github~\footnote{\small{https://github.com/X-PLUG/ChatPLUG}}.
</p>

### Title: Regularized Complete Cycle Consistent GAN for Anomaly Detection. (arXiv:2304.07769v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.07769](http://arxiv.org/abs/2304.07769)
* Code URL: [https://github.com/zahradehghanian97/rcalad](https://github.com/zahradehghanian97/rcalad)
* Copy Paste: `<input type="checkbox">[[2304.07769] Regularized Complete Cycle Consistent GAN for Anomaly Detection](http://arxiv.org/abs/2304.07769) #generative`
* Summary: <p>This study presents an adversarial method for anomaly detection in real-world
applications, leveraging the power of generative adversarial neural networks
(GANs) through cycle consistency in reconstruction error. Previous methods
suffer from the high variance between class-wise accuracy which leads to not
being applicable for all types of anomalies. The proposed method named RCALAD
tries to solve this problem by introducing a novel discriminator to the
structure, which results in a more efficient training process. Additionally,
RCALAD employs a supplementary distribution in the input space to steer
reconstructions toward the normal data distribution, effectively separating
anomalous samples from their reconstructions and facilitating more accurate
anomaly detection. To further enhance the performance of the model, two novel
anomaly scores are introduced. The proposed model has been thoroughly evaluated
through extensive experiments on six various datasets, yielding results that
demonstrate its superiority over existing state-of-the-art models. The code is
readily available to the research community at
https://github.com/zahraDehghanian97/RCALAD.
</p>

## label correction
## noise
### Title: Within-Camera Multilayer Perceptron DVS Denoising. (arXiv:2304.07543v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07543](http://arxiv.org/abs/2304.07543)
* Code URL: [https://github.com/sensorsini/dnd_hls](https://github.com/sensorsini/dnd_hls)
* Copy Paste: `<input type="checkbox">[[2304.07543] Within-Camera Multilayer Perceptron DVS Denoising](http://arxiv.org/abs/2304.07543) #noise`
* Summary: <p>In-camera event denoising reduces the data rate of event cameras by filtering
out noise at the source. A lightweight multilayer perceptron denoising filter
(MLPF) provides state-of-the-art low-cost denoising accuracy. It processes a
small neighborhood of pixels from the timestamp image around each event to
discriminate signal and noise events. This paper proposes two digital logic
implementations of the MLPF denoiser and quantifies their resource cost, power,
and latency. The hardware MLPF quantizes the weights and hidden unit
activations to 4 bits and has about 1k weights with about 40% sparsity. The
Area-Under-Curve Receiver Operating Characteristic accuracy is nearly
indistinguishable from that of the floating point network. The FPGA MLPF
processes each event in 10 clock cycles. In FPGA, it uses 3.5k flip flops and
11.5k LUTs. Our ASIC implementation in 65nm digital technology for a 346x260
pixel camera occupies an area of 4.3mm^2 and consumes 4nJ of energy per event
at event rates up to 25MHz. The MLPF can be easily integrated into an event
camera using an FPGA or as an ASIC directly on the camera chip or in the same
package. This denoising could dramatically reduce the energy consumed by the
communication and host processor and open new areas of always-on event camera
application under scavenged and battery power. Code:
https://github.com/SensorsINI/dnd_hls
</p>

### Title: Robust Cross-Modal Knowledge Distillation for Unconstrained Videos. (arXiv:2304.07775v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07775](http://arxiv.org/abs/2304.07775)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07775] Robust Cross-Modal Knowledge Distillation for Unconstrained Videos](http://arxiv.org/abs/2304.07775) #noise`
* Summary: <p>Cross-modal distillation has been widely used to transfer knowledge across
different modalities, enriching the representation of the target unimodal one.
Recent studies highly relate the temporal synchronization between vision and
sound to the semantic consistency for cross-modal distillation. However, such
semantic consistency from the synchronization is hard to guarantee in
unconstrained videos, due to the irrelevant modality noise and differentiated
semantic correlation. To this end, we first propose a \textit{Modality Noise
Filter} (MNF) module to erase the irrelevant noise in teacher modality with
cross-modal context. After this purification, we then design a
\textit{Contrastive Semantic Calibration} (CSC) module to adaptively distill
useful knowledge for target modality, by referring to the differentiated
sample-wise semantic correlation in a contrastive fashion. Extensive
experiments show that our method could bring a performance boost compared with
other distillation methods in both visual action recognition and video
retrieval task. We also extend to the audio tagging task to prove the
generalization of our method. The source code is available at
\href{https://github.com/GeWu-Lab/cross-modal-distillation}{https://github.com/GeWu-Lab/cross-modal-distillation}.
</p>

### Title: Communication and Energy Efficient Wireless Federated Learning with Intrinsic Privacy. (arXiv:2304.07460v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.07460](http://arxiv.org/abs/2304.07460)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07460] Communication and Energy Efficient Wireless Federated Learning with Intrinsic Privacy](http://arxiv.org/abs/2304.07460) #noise`
* Summary: <p>Federated Learning (FL) is a collaborative learning framework that enables
edge devices to collaboratively learn a global model while keeping raw data
locally. Although FL avoids leaking direct information from local datasets,
sensitive information can still be inferred from the shared models. To address
the privacy issue in FL, differential privacy (DP) mechanisms are leveraged to
provide formal privacy guarantee. However, when deploying FL at the wireless
edge with over-the-air computation, ensuring client-level DP faces significant
challenges. In this paper, we propose a novel wireless FL scheme called private
federated edge learning with sparsification (PFELS) to provide client-level DP
guarantee with intrinsic channel noise while reducing communication and energy
overhead and improving model accuracy. The key idea of PFELS is for each device
to first compress its model update and then adaptively design the transmit
power of the compressed model update according to the wireless channel status
without any artificial noise addition. We provide a privacy analysis for PFELS
and prove the convergence of PFELS under general non-convex and non-IID
settings. Experimental results show that compared with prior work, PFELS can
improve the accuracy with the same DP guarantee and save communication and
energy costs simultaneously.
</p>

### Title: Explaining, Analyzing, and Probing Representations of Self-Supervised Learning Models for Sensor-based Human Activity Recognition. (arXiv:2304.07304v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.07304](http://arxiv.org/abs/2304.07304)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07304] Explaining, Analyzing, and Probing Representations of Self-Supervised Learning Models for Sensor-based Human Activity Recognition](http://arxiv.org/abs/2304.07304) #noise`
* Summary: <p>In recent years, self-supervised learning (SSL) frameworks have been
extensively applied to sensor-based Human Activity Recognition (HAR) in order
to learn deep representations without data annotations. While SSL frameworks
reach performance almost comparable to supervised models, studies on
interpreting representations learnt by SSL models are limited. Nevertheless,
modern explainability methods could help to unravel the differences between SSL
and supervised representations: how they are being learnt, what properties of
input data they preserve, and when SSL can be chosen over supervised training.
In this paper, we aim to analyze deep representations of two recent SSL
frameworks, namely SimCLR and VICReg. Specifically, the emphasis is made on (i)
comparing the robustness of supervised and SSL models to corruptions in input
data; (ii) explaining predictions of deep learning models using saliency maps
and highlighting what input channels are mostly used for predicting various
activities; (iii) exploring properties encoded in SSL and supervised
representations using probing. Extensive experiments on two single-device
datasets (MobiAct and UCI-HAR) have shown that self-supervised learning
representations are significantly more robust to noise in unseen data compared
to supervised models. In contrast, features learnt by the supervised approaches
are more homogeneous across subjects and better encode the nature of
activities.
</p>

## diffusion
### Title: Identity Encoder for Personalized Diffusion. (arXiv:2304.07429v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07429](http://arxiv.org/abs/2304.07429)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07429] Identity Encoder for Personalized Diffusion](http://arxiv.org/abs/2304.07429) #diffusion`
* Summary: <p>Many applications can benefit from personalized image generation models,
including image enhancement, video conferences, just to name a few. Existing
works achieved personalization by fine-tuning one model for each person. While
being successful, this approach incurs additional computation and storage
overhead for each new identity. Furthermore, it usually expects tens or
hundreds of examples per identity to achieve the best performance. To overcome
these challenges, we propose an encoder-based approach for personalization. We
learn an identity encoder which can extract an identity representation from a
set of reference images of a subject, together with a diffusion generator that
can generate new images of the subject conditioned on the identity
representation. Once being trained, the model can be used to generate images of
arbitrary identities given a few examples even if the model hasn't been trained
on the identity. Our approach greatly reduces the overhead for personalized
image generation and is more applicable in many potential applications.
Empirical results show that our approach consistently outperforms existing
fine-tuning based approach in both image generation and reconstruction, and the
outputs is preferred by users more than 95% of the time compared with the best
performing baseline.
</p>

### Title: HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction. (arXiv:2304.07302v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.07302](http://arxiv.org/abs/2304.07302)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07302] HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction](http://arxiv.org/abs/2304.07302) #diffusion`
* Summary: <p>Temporal link prediction, aiming to predict future edges between paired nodes
in a dynamic graph, is of vital importance in diverse applications. However,
existing methods are mainly built upon uniform Euclidean space, which has been
found to be conflict with the power-law distributions of real-world graphs and
unable to represent the hierarchical connections between nodes effectively.
With respect to the special data characteristic, hyperbolic geometry offers an
ideal alternative due to its exponential expansion property. In this paper, we
propose HGWaveNet, a novel hyperbolic graph neural network that fully exploits
the fitness between hyperbolic spaces and data distributions for temporal link
prediction. Specifically, we design two key modules to learn the spatial
topological structures and temporal evolutionary information separately. On the
one hand, a hyperbolic diffusion graph convolution (HDGC) module effectively
aggregates information from a wider range of neighbors. On the other hand, the
internal order of causal correlation between historical states is captured by
hyperbolic dilated causal convolution (HDCC) modules. The whole model is built
upon the hyperbolic spaces to preserve the hierarchical structural information
in the entire data flow. To prove the superiority of HGWaveNet, extensive
experiments are conducted on six real-world graph datasets and the results show
a relative improvement by up to 6.67% on AUC for temporal link prediction over
SOTA methods.
</p>

### Title: Exact Subspace Diffusion for Decentralized Multitask Learning. (arXiv:2304.07358v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2304.07358](http://arxiv.org/abs/2304.07358)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07358] Exact Subspace Diffusion for Decentralized Multitask Learning](http://arxiv.org/abs/2304.07358) #diffusion`
* Summary: <p>Classical paradigms for distributed learning, such as federated or
decentralized gradient descent, employ consensus mechanisms to enforce
homogeneity among agents. While these strategies have proven effective in
i.i.d. scenarios, they can result in significant performance degradation when
agents follow heterogeneous objectives or data. Distributed strategies for
multitask learning, on the other hand, induce relationships between agents in a
more nuanced manner, and encourage collaboration without enforcing consensus.
We develop a generalization of the exact diffusion algorithm for subspace
constrained multitask learning over networks, and derive an accurate expression
for its mean-squared deviation when utilizing noisy gradient approximations. We
verify numerically the accuracy of the predicted performance expressions, as
well as the improved performance of the proposed approach over alternatives
based on approximate projections.
</p>

## LLM
## segmentation
### Title: Uncovering the Inner Workings of STEGO for Safe Unsupervised Semantic Segmentation. (arXiv:2304.07314v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07314](http://arxiv.org/abs/2304.07314)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07314] Uncovering the Inner Workings of STEGO for Safe Unsupervised Semantic Segmentation](http://arxiv.org/abs/2304.07314) #segmentation`
* Summary: <p>Self-supervised pre-training strategies have recently shown impressive
results for training general-purpose feature extraction backbones in computer
vision. In combination with the Vision Transformer architecture, the DINO
self-distillation technique has interesting emerging properties, such as
unsupervised clustering in the latent space and semantic correspondences of the
produced features without using explicit human-annotated labels. The STEGO
method for unsupervised semantic segmentation contrastively distills feature
correspondences of a DINO-pre-trained Vision Transformer and recently set a new
state of the art. However, the detailed workings of STEGO have yet to be
disentangled, preventing its usage in safety-critical applications. This paper
provides a deeper understanding of the STEGO architecture and training strategy
by conducting studies that uncover the working mechanisms behind STEGO,
reproduce and extend its experimental validation, and investigate the ability
of STEGO to transfer to different datasets. Results demonstrate that the STEGO
architecture can be interpreted as a semantics-preserving dimensionality
reduction technique.
</p>

### Title: CoMaL: Conditional Maximum Likelihood Approach to Self-supervised Domain Adaptation in Long-tail Semantic Segmentation. (arXiv:2304.07372v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07372](http://arxiv.org/abs/2304.07372)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07372] CoMaL: Conditional Maximum Likelihood Approach to Self-supervised Domain Adaptation in Long-tail Semantic Segmentation](http://arxiv.org/abs/2304.07372) #segmentation`
* Summary: <p>The research in self-supervised domain adaptation in semantic segmentation
has recently received considerable attention. Although GAN-based methods have
become one of the most popular approaches to domain adaptation, they have
suffered from some limitations. They are insufficient to model both global and
local structures of a given image, especially in small regions of tail classes.
Moreover, they perform bad on the tail classes containing limited number of
pixels or less training samples. In order to address these issues, we present a
new self-supervised domain adaptation approach to tackle long-tail semantic
segmentation in this paper. Firstly, a new metric is introduced to formulate
long-tail domain adaptation in the segmentation problem. Secondly, a new
Conditional Maximum Likelihood (CoMaL) approach in an autoregressive framework
is presented to solve the problem of long-tail domain adaptation. Although
other segmentation methods work under the pixel independence assumption, the
long-tailed pixel distributions in CoMaL are generally solved in the context of
structural dependency, as that is more realistic. Finally, the proposed method
is evaluated on popular large-scale semantic segmentation benchmarks, i.e.,
"SYNTHIA to Cityscapes" and "GTA to Cityscapes", and outperforms the prior
methods by a large margin in both the standard and the proposed evaluation
protocols.
</p>

### Title: Few-shot Camouflaged Animal Detection and Segmentation. (arXiv:2304.07444v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07444](http://arxiv.org/abs/2304.07444)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07444] Few-shot Camouflaged Animal Detection and Segmentation](http://arxiv.org/abs/2304.07444) #segmentation`
* Summary: <p>Camouflaged object detection and segmentation is a new and challenging
research topic in computer vision. There is a serious issue of lacking data of
camouflaged objects such as camouflaged animals in natural scenes. In this
paper, we address the problem of few-shot learning for camouflaged object
detection and segmentation. To this end, we first collect a new dataset,
CAMO-FS, for the benchmark. We then propose a novel method to efficiently
detect and segment the camouflaged objects in the images. In particular, we
introduce the instance triplet loss and the instance memory storage. The
extensive experiments demonstrated that our proposed method achieves
state-of-the-art performance on the newly collected dataset.
</p>

### Title: Instance-level Few-shot Learning with Class Hierarchy Mining. (arXiv:2304.07459v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07459](http://arxiv.org/abs/2304.07459)
* Code URL: [https://github.com/nvakhoa/superclass-fsis](https://github.com/nvakhoa/superclass-fsis)
* Copy Paste: `<input type="checkbox">[[2304.07459] Instance-level Few-shot Learning with Class Hierarchy Mining](http://arxiv.org/abs/2304.07459) #segmentation`
* Summary: <p>Few-shot learning is proposed to tackle the problem of scarce training data
in novel classes. However, prior works in instance-level few-shot learning have
paid less attention to effectively utilizing the relationship between
categories. In this paper, we exploit the hierarchical information to leverage
discriminative and relevant features of base classes to effectively classify
novel objects. These features are extracted from abundant data of base classes,
which could be utilized to reasonably describe classes with scarce data.
Specifically, we propose a novel superclass approach that automatically creates
a hierarchy considering base and novel classes as fine-grained classes for
few-shot instance segmentation (FSIS). Based on the hierarchical information,
we design a novel framework called Soft Multiple Superclass (SMS) to extract
relevant features or characteristics of classes in the same superclass. A new
class assigned to the superclass is easier to classify by leveraging these
relevant features. Besides, in order to effectively train the
hierarchy-based-detector in FSIS, we apply the label refinement to further
describe the associations between fine-grained classes. The extensive
experiments demonstrate the effectiveness of our method on FSIS benchmarks.
Code is available online.
</p>

### Title: Region-Enhanced Feature Learning for Scene Semantic Segmentation. (arXiv:2304.07486v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07486](http://arxiv.org/abs/2304.07486)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07486] Region-Enhanced Feature Learning for Scene Semantic Segmentation](http://arxiv.org/abs/2304.07486) #segmentation`
* Summary: <p>Semantic segmentation in complex scenes not only relies on local object
appearance but also on object locations and the surrounding environment.
Nonetheless, it is difficult to model long-range context in the format of
pairwise point correlations due to its huge computational cost for large-scale
point clouds.In this paper, we propose to use regions as the intermediate
representation of point clouds instead of fine-grained points or voxels to
reduce the computational burden. We introduce a novel Region-Enhanced Feature
Learning network (REFL-Net) that leverages region correlations to enhance the
features of ambiguous points. We design a Region-based Feature Enhancement
module (RFE) which consists of a Semantic-Spatial Region Extraction (SSRE)
stage and a Region Dependency Modeling (RDM) stage. In the SSRE stage, we group
the input points into a set of regions according to the point distances in both
semantic and spatial space.In the RDM part, we explore region-wise semantic and
spatial relationships via a self-attention block on region features and fuse
point features with the region features to obtain more discriminative
representations. Our proposed RFE module is a plug-and-play module that can be
integrated with common semantic segmentation backbones. We conduct extensive
experiments on ScanNetv2 and S3DIS datasets, and evaluate our RFE module with
different segmentation backbones. Our REFL-Net achieves 1.8% mIoU gain on
ScanNetv2 and 1.0% mIoU gain on S3DIS respectively with negligible
computational cost compared to the backbone networks. Both quantitative and
qualitative results show the powerful long-range context modeling ability and
strong generalization ability of our REFL-Net.
</p>

### Title: S3M: Scalable Statistical Shape Modeling through Unsupervised Correspondences. (arXiv:2304.07515v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07515](http://arxiv.org/abs/2304.07515)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07515] S3M: Scalable Statistical Shape Modeling through Unsupervised Correspondences](http://arxiv.org/abs/2304.07515) #segmentation`
* Summary: <p>Statistical shape models (SSMs) are an established way to geometrically
represent the anatomy of a population with various clinically relevant
applications. However, they typically require domain expertise and
labor-intensive manual segmentations or landmark annotations to generate.
Methods to estimate correspondences for SSMs typically learn with such labels
as supervision signals. We address these shortcomings by proposing an
unsupervised method that leverages deep geometric features and functional
correspondences to learn local and global shape structures across complex
anatomies simultaneously. Our pipeline significantly improves unsupervised
correspondence estimation for SSMs compared to baseline methods, even on highly
irregular surface topologies. We demonstrate this for two different anatomical
structures: the thyroid and a multi-chamber heart dataset. Furthermore, our
method is robust enough to learn from noisy neural network predictions,
enabling scaling SSMs to larger patient populations without manual annotation.
</p>

### Title: Compete to Win: Enhancing Pseudo Labels for Barely-supervised Medical Image Segmentation. (arXiv:2304.07519v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07519](http://arxiv.org/abs/2304.07519)
* Code URL: [https://github.com/huiimin5/comwin](https://github.com/huiimin5/comwin)
* Copy Paste: `<input type="checkbox">[[2304.07519] Compete to Win: Enhancing Pseudo Labels for Barely-supervised Medical Image Segmentation](http://arxiv.org/abs/2304.07519) #segmentation`
* Summary: <p>This study investigates barely-supervised medical image segmentation where
only few labeled data, i.e., single-digit cases are available. We observe the
key limitation of the existing state-of-the-art semi-supervised solution cross
pseudo supervision is the unsatisfactory precision of foreground classes,
leading to a degenerated result under barely-supervised learning. In this
paper, we propose a novel Compete-to-Win method (ComWin) to enhance the pseudo
label quality. In contrast to directly using one model's predictions as pseudo
labels, our key idea is that high-quality pseudo labels should be generated by
comparing multiple confidence maps produced by different networks to select the
most confident one (a compete-to-win strategy). To further refine pseudo labels
at near-boundary areas, an enhanced version of ComWin, namely, ComWin+, is
proposed by integrating a boundary-aware enhancement module. Experiments show
that our method can achieve the best performance on three public medical image
datasets for cardiac structure segmentation, pancreas segmentation and colon
tumor segmentation, respectively. The source code is now available at
https://github.com/Huiimin5/comwin.
</p>

### Title: ALiSNet: Accurate and Lightweight Human Segmentation Network for Fashion E-Commerce. (arXiv:2304.07533v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07533](http://arxiv.org/abs/2304.07533)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07533] ALiSNet: Accurate and Lightweight Human Segmentation Network for Fashion E-Commerce](http://arxiv.org/abs/2304.07533) #segmentation`
* Summary: <p>Accurately estimating human body shape from photos can enable innovative
applications in fashion, from mass customization, to size and fit
recommendations and virtual try-on. Body silhouettes calculated from user
pictures are effective representations of the body shape for downstream tasks.
Smartphones provide a convenient way for users to capture images of their body,
and on-device image processing allows predicting body segmentation while
protecting users privacy. Existing off-the-shelf methods for human segmentation
are closed source and cannot be specialized for our application of body shape
and measurement estimation. Therefore, we create a new segmentation model by
simplifying Semantic FPN with PointRend, an existing accurate model. We
finetune this model on a high-quality dataset of humans in a restricted set of
poses relevant for our application. We obtain our final model, ALiSNet, with a
size of 4MB and 97.6$\pm$1.0$\%$ mIoU, compared to Apple Person Segmentation,
which has an accuracy of 94.4$\pm$5.7$\%$ mIoU on our dataset.
</p>

### Title: TagCLIP: Improving Discrimination Ability of Open-Vocabulary Semantic Segmentation. (arXiv:2304.07547v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07547](http://arxiv.org/abs/2304.07547)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07547] TagCLIP: Improving Discrimination Ability of Open-Vocabulary Semantic Segmentation](http://arxiv.org/abs/2304.07547) #segmentation`
* Summary: <p>Recent success of Contrastive Language-Image Pre-training~(CLIP) has shown
great promise in pixel-level open-vocabulary learning tasks. A general paradigm
utilizes CLIP's text and patch embeddings to generate semantic masks. However,
existing models easily misidentify input pixels from unseen classes, thus
confusing novel classes with semantically-similar ones. In our work, we
disentangle the ill-posed optimization problem into two parallel processes: one
performs semantic matching individually, and the other judges reliability for
improving discrimination ability. Motivated by special tokens in language
modeling that represents sentence-level embeddings, we design a trusty token
that decouples the known and novel category prediction tendency. With almost no
extra overhead, we upgrade the pixel-level generalization capacity of existing
models effectively. Our TagCLIP (CLIP adapting with Trusty-guidance) boosts the
IoU of unseen classes by 7.4% and 1.7% on PASCAL VOC 2012 and COCO-Stuff 164K.
</p>

### Title: Can SAM Segment Polyps?. (arXiv:2304.07583v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07583](http://arxiv.org/abs/2304.07583)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07583] Can SAM Segment Polyps?](http://arxiv.org/abs/2304.07583) #segmentation`
* Summary: <p>Recently, Meta AI Research releases a general Segment Anything Model (SAM),
which has demonstrated promising performance in several segmentation tasks. As
we know, polyp segmentation is a fundamental task in the medical imaging field,
which plays a critical role in the diagnosis and cure of colorectal cancer. In
particular, applying SAM to the polyp segmentation task is interesting. In this
report, we evaluate the performance of SAM in segmenting polyps, in which SAM
is under unprompted settings. We hope this report will provide insights to
advance this polyp segmentation field and promote more interesting works in the
future. This project is publicly at https://github.com/taozh2017/SAMPolyp.
</p>

### Title: An Instance Segmentation Dataset of Yeast Cells in Microstructures. (arXiv:2304.07597v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07597](http://arxiv.org/abs/2304.07597)
* Code URL: [https://github.com/christophreich1996/yeast-in-microstructures-dataset](https://github.com/christophreich1996/yeast-in-microstructures-dataset)
* Copy Paste: `<input type="checkbox">[[2304.07597] An Instance Segmentation Dataset of Yeast Cells in Microstructures](http://arxiv.org/abs/2304.07597) #segmentation`
* Summary: <p>Extracting single-cell information from microscopy data requires accurate
instance-wise segmentations. Obtaining pixel-wise segmentations from microscopy
imagery remains a challenging task, especially with the added complexity of
microstructured environments. This paper presents a novel dataset for
segmenting yeast cells in microstructures. We offer pixel-wise instance
segmentation labels for both cells and trap microstructures. In total, we
release 493 densely annotated microscopy images. To facilitate a unified
comparison between novel segmentation algorithms, we propose a standardized
evaluation strategy for our dataset. The aim of the dataset and evaluation
strategy is to facilitate the development of new cell segmentation approaches.
The dataset is publicly available at
https://christophreich1996.github.io/yeast_in_microstructures_dataset/ .
</p>

### Title: GeoMultiTaskNet: remote sensing unsupervised domain adaptation using geographical coordinates. (arXiv:2304.07750v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07750](http://arxiv.org/abs/2304.07750)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07750] GeoMultiTaskNet: remote sensing unsupervised domain adaptation using geographical coordinates](http://arxiv.org/abs/2304.07750) #segmentation`
* Summary: <p>Land cover maps are a pivotal element in a wide range of Earth Observation
(EO) applications. However, annotating large datasets to develop supervised
systems for remote sensing (RS) semantic segmentation is costly and
time-consuming. Unsupervised Domain Adaption (UDA) could tackle these issues by
adapting a model trained on a source domain, where labels are available, to a
target domain, without annotations. UDA, while gaining importance in computer
vision, is still under-investigated in RS. Thus, we propose a new lightweight
model, GeoMultiTaskNet, based on two contributions: a GeoMultiTask module
(GeoMT), which utilizes geographical coordinates to align the source and target
domains, and a Dynamic Class Sampling (DCS) strategy, to adapt the semantic
segmentation loss to the frequency of classes. This approach is the first to
use geographical metadata for UDA in semantic segmentation. It reaches
state-of-the-art performances (47,22% mIoU), reducing at the same time the
number of parameters (33M), on a subset of the FLAIR dataset, a recently
proposed dataset properly shaped for RS UDA, used for the first time ever for
research scopes here.
</p>

### Title: Deep learning universal crater detection using Segment Anything Model (SAM). (arXiv:2304.07764v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07764](http://arxiv.org/abs/2304.07764)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07764] Deep learning universal crater detection using Segment Anything Model (SAM)](http://arxiv.org/abs/2304.07764) #segmentation`
* Summary: <p>Craters are amongst the most important morphological features in planetary
exploration. To that extent, detecting, mapping and counting craters is a
mainstream process in planetary science, done primarily manually, which is a
very laborious and time-consuming process. Recently, machine learning (ML) and
computer vision have been successfully applied for both detecting craters and
estimating their size. Existing ML approaches for automated crater detection
have been trained in specific types of data e.g. digital elevation model (DEM),
images and associated metadata for orbiters such as the Lunar Reconnaissance
Orbiter Camera (LROC) etc.. Due to that, each of the resulting ML schemes is
applicable and reliable only to the type of data used during the training
process. Data from different sources, angles and setups can compromise the
reliability of these ML schemes. In this paper we present a universal crater
detection scheme that is based on the recently proposed Segment Anything Model
(SAM) from META AI. SAM is a prompt-able segmentation system with zero-shot
generalization to unfamiliar objects and images without the need for additional
training. Using SAM we can successfully identify crater-looking objects in any
type of data (e,g, raw satellite images Level-1 and 2 products, DEMs etc.) for
different setups (e.g. Lunar, Mars) and different capturing angles. Moreover,
using shape indexes, we only keep the segmentation masks of crater-like
features. These masks are subsequently fitted with an ellipse, recovering both
the location and the size/geometry of the detected craters.
</p>

## object detection
### Title: Align-DETR: Improving DETR with Simple IoU-aware BCE loss. (arXiv:2304.07527v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07527](http://arxiv.org/abs/2304.07527)
* Code URL: [https://github.com/felixcaae/aligndetr](https://github.com/felixcaae/aligndetr)
* Copy Paste: `<input type="checkbox">[[2304.07527] Align-DETR: Improving DETR with Simple IoU-aware BCE loss](http://arxiv.org/abs/2304.07527) #object detection`
* Summary: <p>DETR has set up a simple end-to-end pipeline for object detection by
formulating this task as a set prediction problem, showing promising potential.
However, despite the significant progress in improving DETR, this paper
identifies a problem of misalignment in the output distribution, which prevents
the best-regressed samples from being assigned with high confidence, hindering
the model's accuracy. We propose a metric, recall of best-regressed samples, to
quantitively evaluate the misalignment problem. Observing its importance, we
propose a novel Align-DETR that incorporates a localization precision-aware
classification loss in optimization. The proposed loss, IA-BCE, guides the
training of DETR to build a strong correlation between classification score and
localization precision. We also adopt the mixed-matching strategy, to
facilitate DETR-based detectors with faster training convergence while keeping
an end-to-end scheme. Moreover, to overcome the dramatic decrease in sample
quality induced by the sparsity of queries, we introduce a prime sample
weighting mechanism to suppress the interference of unimportant samples.
Extensive experiments are conducted with very competitive results reported. In
particular, it delivers a 46 (+3.8)% AP on the DAB-DETR baseline with the
ResNet-50 backbone and reaches a new SOTA performance of 50.2% AP in the 1x
setting on the COCO validation set when employing the strong baseline DINO. Our
code is available at https://github.com/FelixCaae/AlignDETR.
</p>

### Title: Handling Heavy Occlusion in Dense Crowd Tracking by Focusing on the Heads. (arXiv:2304.07705v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2304.07705](http://arxiv.org/abs/2304.07705)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2304.07705] Handling Heavy Occlusion in Dense Crowd Tracking by Focusing on the Heads](http://arxiv.org/abs/2304.07705) #object detection`
* Summary: <p>With the rapid development of deep learning, object detection and tracking
play a vital role in today's society. Being able to identify and track all the
pedestrians in the dense crowd scene with computer vision approaches is a
typical challenge in this field, also known as the Multiple Object Tracking
(MOT) challenge. Modern trackers are required to operate on more and more
complicated scenes. According to the MOT20 challenge result, the pedestrian is
4 times denser than the MOT17 challenge. Hence, improving the ability to detect
and track in extremely crowded scenes is the aim of this work. In light of the
occlusion issue with the human body, the heads are usually easier to identify.
In this work, we have designed a joint head and body detector in an anchor-free
style to boost the detection recall and precision performance of pedestrians in
both small and medium sizes. Innovatively, our model does not require
information on the statistical head-body ratio for common pedestrians detection
for training. Instead, the proposed model learns the ratio dynamically. To
verify the effectiveness of the proposed model, we evaluate the model with
extensive experiments on different datasets, including MOT20, Crowdhuman, and
HT21 datasets. As a result, our proposed method significantly improves both the
recall and precision rate on small &amp; medium sized pedestrians and achieves
state-of-the-art results in these challenging datasets.
</p>

