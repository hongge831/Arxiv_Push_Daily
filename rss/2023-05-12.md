## data-free
## transformer
### Title: Patch-wise Mixed-Precision Quantization of Vision Transformer. (arXiv:2305.06559v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06559](http://arxiv.org/abs/2305.06559)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06559] Patch-wise Mixed-Precision Quantization of Vision Transformer](http://arxiv.org/abs/2305.06559) #transformer`
* Summary: <p>As emerging hardware begins to support mixed bit-width arithmetic
computation, mixed-precision quantization is widely used to reduce the
complexity of neural networks. However, Vision Transformers (ViTs) require
complex self-attention computation to guarantee the learning of powerful
feature representations, which makes mixed-precision quantization of ViTs still
challenging. In this paper, we propose a novel patch-wise mixed-precision
quantization (PMQ) for efficient inference of ViTs. Specifically, we design a
lightweight global metric, which is faster than existing methods, to measure
the sensitivity of each component in ViTs to quantization errors. Moreover, we
also introduce a pareto frontier approach to automatically allocate the optimal
bit-precision according to the sensitivity. To further reduce the computational
complexity of self-attention in inference stage, we propose a patch-wise module
to reallocate bit-width of patches in each layer. Extensive experiments on the
ImageNet dataset shows that our method greatly reduces the search cost and
facilitates the application of mixed-precision quantization to ViTs.
</p>

### Title: Undercover Deepfakes: Detecting Fake Segments in Videos. (arXiv:2305.06564v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06564](http://arxiv.org/abs/2305.06564)
* Code URL: [https://github.com/sanjaysaha1311/temporal-deepfake-segmentation](https://github.com/sanjaysaha1311/temporal-deepfake-segmentation)
* Copy Paste: `<input type="checkbox">[[2305.06564] Undercover Deepfakes: Detecting Fake Segments in Videos](http://arxiv.org/abs/2305.06564) #transformer`
* Summary: <p>The recent renaissance in generative models, driven primarily by the advent
of diffusion models and iterative improvement in GAN methods, has enabled many
creative applications. However, each advancement is also accompanied by a rise
in the potential for misuse. In the arena of deepfake generation this is a key
societal issue. In particular, the ability to modify segments of videos using
such generative techniques creates a new paradigm of deepfakes which are mostly
real videos altered slightly to distort the truth. Current deepfake detection
methods in the academic literature are not evaluated on this paradigm. In this
paper, we present a deepfake detection method able to address this issue by
performing both frame and video level deepfake prediction. To facilitate
testing our method we create a new benchmark dataset where videos have both
real and fake frame sequences. Our method utilizes the Vision Transformer,
Scaling and Shifting pretraining and Timeseries Transformer to temporally
segment videos to help facilitate the interpretation of possible deepfakes.
Extensive experiments on a variety of deepfake generation methods show
excellent results on temporal segmentation and classical video level
predictions as well. In particular, the paradigm we introduce will form a
powerful tool for the moderation of deepfakes, where human oversight can be
better targeted to the parts of videos suspected of being deepfakes. All
experiments can be reproduced at:
https://github.com/sanjaysaha1311/temporal-deepfake-segmentation.
</p>

### Title: Exploiting Fine-Grained DCT Representations for Hiding Image-Level Messages within JPEG Images. (arXiv:2305.06582v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06582](http://arxiv.org/abs/2305.06582)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06582] Exploiting Fine-Grained DCT Representations for Hiding Image-Level Messages within JPEG Images](http://arxiv.org/abs/2305.06582) #transformer`
* Summary: <p>Unlike hiding bit-level messages, hiding image-level messages is more
challenging, which requires large capacity, high imperceptibility, and high
security. Although recent advances in hiding image-level messages have been
remarkable, existing schemes are limited to lossless spatial images as covers
and cannot be directly applied to JPEG images, the ubiquitous lossy format
images in daily life. The difficulties of migration are caused by the lack of
targeted design and the loss of details due to lossy decompression and
re-compression. Considering that taking DCT densely on $8\times8$ image patches
is the core of the JPEG compression standard, we design a novel model called
\textsf{EFDR}, which can comprehensively \underline{E}xploit
\underline{F}ine-grained \underline{D}CT \underline{R}epresentations and embed
the secret image into quantized DCT coefficients to avoid the lossy process.
Specifically, we transform the JPEG cover image and hidden secret image into
fine-grained DCT representations that compact the frequency and are associated
with the inter-block and intra-block correlations. Subsequently, the
fine-grained DCT representations are further enhanced by a sub-band features
enhancement module. Afterward, a transformer-based invertibility module is
designed to fuse enhanced sub-band features. Such a design enables a
fine-grained self-attention on each sub-band and captures long-range
dependencies while maintaining excellent reversibility for hiding and recovery.
To our best knowledge, this is the first attempt to embed a color image of
equal size in a color JPEG image. Extensive experiments demonstrate the
effectiveness of our \textsf{EFDR} with superior performance.
</p>

### Title: PVT-SSD: Single-Stage 3D Object Detector with Point-Voxel Transformer. (arXiv:2305.06621v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06621](http://arxiv.org/abs/2305.06621)
* Code URL: [https://github.com/nightmare-n/pvt-ssd](https://github.com/nightmare-n/pvt-ssd)
* Copy Paste: `<input type="checkbox">[[2305.06621] PVT-SSD: Single-Stage 3D Object Detector with Point-Voxel Transformer](http://arxiv.org/abs/2305.06621) #transformer`
* Summary: <p>Recent Transformer-based 3D object detectors learn point cloud features
either from point- or voxel-based representations. However, the former requires
time-consuming sampling while the latter introduces quantization errors. In
this paper, we present a novel Point-Voxel Transformer for single-stage 3D
detection (PVT-SSD) that takes advantage of these two representations.
Specifically, we first use voxel-based sparse convolutions for efficient
feature encoding. Then, we propose a Point-Voxel Transformer (PVT) module that
obtains long-range contexts in a cheap manner from voxels while attaining
accurate positions from points. The key to associating the two different
representations is our introduced input-dependent Query Initialization module,
which could efficiently generate reference points and content queries. Then,
PVT adaptively fuses long-range contextual and local geometric information
around reference points into content queries. Further, to quickly find the
neighboring points of reference points, we design the Virtual Range Image
module, which generalizes the native range image to multi-sensor and
multi-frame. The experiments on several autonomous driving benchmarks verify
the effectiveness and efficiency of the proposed method. Code will be available
at https://github.com/Nightmare-n/PVT-SSD.
</p>

### Title: Cascaded Cross-Attention Networks for Data-Efficient Whole-Slide Image Classification Using Transformers. (arXiv:2305.06963v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06963](http://arxiv.org/abs/2305.06963)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06963] Cascaded Cross-Attention Networks for Data-Efficient Whole-Slide Image Classification Using Transformers](http://arxiv.org/abs/2305.06963) #transformer`
* Summary: <p>Whole-Slide Imaging allows for the capturing and digitization of
high-resolution images of histological specimen. An automated analysis of such
images using deep learning models is therefore of high demand. The transformer
architecture has been proposed as a possible candidate for effectively
leveraging the high-resolution information. Here, the whole-slide image is
partitioned into smaller image patches and feature tokens are extracted from
these image patches. However, while the conventional transformer allows for a
simultaneous processing of a large set of input tokens, the computational
demand scales quadratically with the number of input tokens and thus
quadratically with the number of image patches. To address this problem we
propose a novel cascaded cross-attention network (CCAN) based on the
cross-attention mechanism that scales linearly with the number of extracted
patches. Our experiments demonstrate that this architecture is at least on-par
with and even outperforms other attention-based state-of-the-art methods on two
public datasets: On the use-case of lung cancer (TCGA NSCLC) our model reaches
a mean area under the receiver operating characteristic (AUC) of 0.970 $\pm$
0.008 and on renal cancer (TCGA RCC) reaches a mean AUC of 0.985 $\pm$ 0.004.
Furthermore, we show that our proposed model is efficient in low-data regimes,
making it a promising approach for analyzing whole-slide images in
resource-limited settings. To foster research in this direction, we make our
code publicly available on GitHub: XXX.
</p>

### Title: Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.07011](http://arxiv.org/abs/2305.07011)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.07011] Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers](http://arxiv.org/abs/2305.07011) #transformer`
* Summary: <p>We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a
contrastive image-text pretraining recipe to bridge the gap between image-level
pretraining and open-vocabulary object detection. At the pretraining phase, we
propose to randomly crop and resize regions of positional embeddings instead of
using the whole image positional embeddings. This better matches the use of
positional embeddings at region-level in the detection finetuning phase. In
addition, we replace the common softmax cross entropy loss in contrastive
learning with focal loss to better learn the informative yet difficult
examples. Finally, we leverage recent advances in novel object proposals to
improve open-vocabulary detection finetuning. We evaluate our full model on the
LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer.
RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best
existing approach by +5.8 points in addition to competitive zero-shot transfer
detection. Surprisingly, RO-ViT improves the image-level representation as well
and achieves the state of the art on 9 out of 12 metrics on COCO and Flickr
image-text retrieval benchmarks, outperforming competitive approaches with
larger models.
</p>

### Title: SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input Views. (arXiv:2305.07024v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.07024](http://arxiv.org/abs/2305.07024)
* Code URL: [https://github.com/xt4d/sparsegnv](https://github.com/xt4d/sparsegnv)
* Copy Paste: `<input type="checkbox">[[2305.07024] SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input Views](http://arxiv.org/abs/2305.07024) #transformer`
* Summary: <p>We study to generate novel views of indoor scenes given sparse input views.
The challenge is to achieve both photorealism and view consistency. We present
SparseGNV: a learning framework that incorporates 3D structures and image
generative models to generate novel views with three modules. The first module
builds a neural point cloud as underlying geometry, providing contextual
information and guidance for the target novel view. The second module utilizes
a transformer-based network to map the scene context and the guidance into a
shared latent space and autoregressively decodes the target view in the form of
discrete image tokens. The third module reconstructs the tokens into the image
of the target view. SparseGNV is trained across a large indoor scene dataset to
learn generalizable priors. Once trained, it can efficiently generate novel
views of an unseen indoor scene in a feed-forward manner. We evaluate SparseGNV
on both real-world and synthetic indoor scenes and demonstrate that it
outperforms state-of-the-art methods based on either neural radiance fields or
conditional image generation.
</p>

### Title: EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention. (arXiv:2305.07027v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.07027](http://arxiv.org/abs/2305.07027)
* Code URL: [https://github.com/microsoft/cream](https://github.com/microsoft/cream)
* Copy Paste: `<input type="checkbox">[[2305.07027] EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention](http://arxiv.org/abs/2305.07027) #transformer`
* Summary: <p>Vision transformers have shown great success due to their high model
capabilities. However, their remarkable performance is accompanied by heavy
computation costs, which makes them unsuitable for real-time applications. In
this paper, we propose a family of high-speed vision transformers named
EfficientViT. We find that the speed of existing transformer models is commonly
bounded by memory inefficient operations, especially the tensor reshaping and
element-wise functions in MHSA. Therefore, we design a new building block with
a sandwich layout, i.e., using a single memory-bound MHSA between efficient FFN
layers, which improves memory efficiency while enhancing channel communication.
Moreover, we discover that the attention maps share high similarities across
heads, leading to computational redundancy. To address this, we present a
cascaded group attention module feeding attention heads with different splits
of the full feature, which not only saves computation cost but also improves
attention diversity. Comprehensive experiments demonstrate EfficientViT
outperforms existing efficient models, striking a good trade-off between speed
and accuracy. For instance, our EfficientViT-M5 surpasses MobileNetV3-Large by
1.9% in accuracy, while getting 40.4% and 45.2% higher throughput on Nvidia
V100 GPU and Intel Xeon CPU, respectively. Compared to the recent efficient
model MobileViT-XXS, EfficientViT-M2 achieves 1.8% superior accuracy, while
running 5.8x/3.7x faster on the GPU/CPU, and 7.4x faster when converted to ONNX
format. Code and models are available at
https://github.com/microsoft/Cream/tree/main/EfficientViT.
</p>

### Title: A Method to Automate the Discharge Summary Hospital Course for Neurology Patients. (arXiv:2305.06416v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.06416](http://arxiv.org/abs/2305.06416)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06416] A Method to Automate the Discharge Summary Hospital Course for Neurology Patients](http://arxiv.org/abs/2305.06416) #transformer`
* Summary: <p>Generation of automated clinical notes have been posited as a strategy to
mitigate physician burnout. In particular, an automated narrative summary of a
patient's hospital stay could supplement the hospital course section of the
discharge summary that inpatient physicians document in electronic health
record (EHR) systems. In the current study, we developed and evaluated an
automated method for summarizing the hospital course section using
encoder-decoder sequence-to-sequence transformer models. We fine tuned BERT and
BART models and optimized for factuality through constraining beam search,
which we trained and tested using EHR data from patients admitted to the
neurology unit of an academic medical center. The approach demonstrated good
ROUGE scores with an R-2 of 13.76. In a blind evaluation, two board-certified
physicians rated 62% of the automated summaries as meeting the standard of
care, which suggests the method may be useful clinically. To our knowledge,
this study is among the first to demonstrate an automated method for generating
a discharge summary hospital course that approaches a quality level of what a
physician would write.
</p>

### Title: SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2). (arXiv:2305.06586v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.06586](http://arxiv.org/abs/2305.06586)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06586] SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2)](http://arxiv.org/abs/2305.06586) #transformer`
* Summary: <p>We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual
Named Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task
focused on methods to identify complex fine-grained named entities (like
WRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual and
multilingual scenarios, as well as noisy settings. The task used the MultiCoNER
V2 dataset, composed of 2.2 million instances in Bangla, Chinese, English,
Farsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, and
Ukrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. It
attracted 842 submissions from 47 teams, and 34 teams submitted system papers.
Results showed that complex entity types such as media titles and product names
were the most challenging. Methods fusing external knowledge into transformer
models achieved the best performance, and the largest gains were on the
Creative Work and Group classes, which are still challenging even with external
knowledge. Some fine-grained classes proved to be more challenging than others,
such as SCIENTIST, ARTWORK, and PRIVATECORP. We also observed that noisy data
has a significant impact on model performance, with an average drop of 10% on
the noisy subset. The task highlights the need for future research on improving
NER robustness on noisy data containing complex entities.
</p>

### Title: Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*. (arXiv:2305.06721v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.06721](http://arxiv.org/abs/2305.06721)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06721] Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*](http://arxiv.org/abs/2305.06721) #transformer`
* Summary: <p>To advance the neural encoding of Portuguese (PT), and a fortiori the
technological preparation of this language for the digital age, we developed a
Transformer-based foundation model that sets a new state of the art in this
respect for two of its variants, namely European Portuguese from Portugal
(PT-PT) and American Portuguese from Brazil (PT-BR).
</p>
<p>To develop this encoder, which we named Albertina PT-*, a strong model was
used as a starting point, DeBERTa, and its pre-training was done over data sets
of Portuguese, namely over a data set we gathered for PT-PT and over the brWaC
corpus for PT-BR. The performance of Albertina and competing models was
assessed by evaluating them on prominent downstream language processing tasks
adapted for Portuguese.
</p>
<p>Both Albertina PT-PT and PT-BR versions are distributed free of charge and
under the most permissive license possible and can be run on consumer-grade
hardware, thus seeking to contribute to the advancement of research and
innovation in language technology for Portuguese.
</p>

### Title: COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks. (arXiv:2305.06754v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.06754](http://arxiv.org/abs/2305.06754)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06754] COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks](http://arxiv.org/abs/2305.06754) #transformer`
* Summary: <p>Transformer architectures are complex and their use in NLP, while it has
engendered many successes, makes their interpretability or explainability
challenging. Recent debates have shown that attention maps and attribution
methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this
paper, we present some of their limitations and introduce COCKATIEL, which
successfully addresses some of them. COCKATIEL is a novel, post-hoc,
concept-based, model-agnostic XAI technique that generates meaningful
explanations from the last layer of a neural net model trained on an NLP
classification task by using Non-Negative Matrix Factorization (NMF) to
discover the concepts the model leverages to make predictions and by exploiting
a Sensitivity Analysis to estimate accurately the importance of each of these
concepts for the model. It does so without compromising the accuracy of the
underlying model or requiring a new one to be trained. We conduct experiments
in single and multi-aspect sentiment analysis tasks and we show COCKATIEL's
superior ability to discover concepts that align with humans' on Transformer
models without any supervision, we objectively verify the faithfulness of its
explanations through fidelity metrics, and we showcase its ability to provide
meaningful explanations in two different datasets.
</p>

### Title: Detecting Idiomatic Multiword Expressions in Clinical Terminology using Definition-Based Representation Learning. (arXiv:2305.06801v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.06801](http://arxiv.org/abs/2305.06801)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06801] Detecting Idiomatic Multiword Expressions in Clinical Terminology using Definition-Based Representation Learning](http://arxiv.org/abs/2305.06801) #transformer`
* Summary: <p>This paper shines a light on the potential of definition-based semantic
models for detecting idiomatic and semi-idiomatic multiword expressions (MWEs)
in clinical terminology. Our study focuses on biomedical entities defined in
the UMLS ontology and aims to help prioritize the translation efforts of these
entities. In particular, we develop an effective tool for scoring the
idiomaticity of biomedical MWEs based on the degree of similarity between the
semantic representations of those MWEs and a weighted average of the
representation of their constituents. We achieve this using a biomedical
language model trained to produce similar representations for entity names and
their definitions, called BioLORD. The importance of this definition-based
approach is highlighted by comparing the BioLORD model to two other
state-of-the-art biomedical language models based on Transformer: SapBERT and
CODER. Our results show that the BioLORD model has a strong ability to identify
idiomatic MWEs, not replicated in other models. Our corpus-free idiomaticity
estimation helps ontology translators to focus on more challenging MWEs.
</p>

### Title: IUST_NLP at SemEval-2023 Task 10: Explainable Detecting Sexism with Transformers and Task-adaptive Pretraining. (arXiv:2305.06892v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.06892](http://arxiv.org/abs/2305.06892)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06892] IUST_NLP at SemEval-2023 Task 10: Explainable Detecting Sexism with Transformers and Task-adaptive Pretraining](http://arxiv.org/abs/2305.06892) #transformer`
* Summary: <p>This paper describes our system on SemEval-2023 Task 10: Explainable
Detection of Online Sexism (EDOS). This work aims to design an automatic system
for detecting and classifying sexist content in online spaces. We propose a set
of transformer-based pre-trained models with task-adaptive pretraining and
ensemble learning. The main contributions of our system include analyzing the
performance of different transformer-based pre-trained models and combining
these models, as well as providing an efficient method using large amounts of
unlabeled data for model adaptive pretraining. We have also explored several
other strategies. On the test dataset, our system achieves F1-scores of 83%,
64%, and 47% on subtasks A, B, and C, respectively.
</p>

### Title: A General-Purpose Multilingual Document Encoder. (arXiv:2305.07016v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.07016](http://arxiv.org/abs/2305.07016)
* Code URL: [https://github.com/ogaloglu/pre-training-multilingual-document-encoders](https://github.com/ogaloglu/pre-training-multilingual-document-encoders)
* Copy Paste: `<input type="checkbox">[[2305.07016] A General-Purpose Multilingual Document Encoder](http://arxiv.org/abs/2305.07016) #transformer`
* Summary: <p>Massively multilingual pretrained transformers (MMTs) have tremendously
pushed the state of the art on multilingual NLP and cross-lingual transfer of
NLP models in particular. While a large body of work leveraged MMTs to mine
parallel data and induce bilingual document embeddings, much less effort has
been devoted to training general-purpose (massively) multilingual document
encoder that can be used for both supervised and unsupervised document-level
tasks. In this work, we pretrain a massively multilingual document encoder as a
hierarchical transformer model (HMDE) in which a shallow document transformer
contextualizes sentence representations produced by a state-of-the-art
pretrained multilingual sentence encoder. We leverage Wikipedia as a readily
available source of comparable documents for creating training data, and train
HMDE by means of a cross-lingual contrastive objective, further exploiting the
category hierarchy of Wikipedia for creation of difficult negatives. We
evaluate the effectiveness of HMDE in two arguably most common and prominent
cross-lingual document-level tasks: (1) cross-lingual transfer for topical
document classification and (2) cross-lingual document retrieval. HMDE is
significantly more effective than (i) aggregations of segment-based
representations and (ii) multilingual Longformer. Crucially, owing to its
massively multilingual lower transformer, HMDE successfully generalizes to
languages unseen in document-level pretraining. We publicly release our code
and models at
https://github.com/ogaloglu/pre-training-multilingual-document-encoders .
</p>

### Title: Assault and Battery: Evaluating the Security of Power Conversion Systems Against Electromagnetic Injection Attacks. (arXiv:2305.06901v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2305.06901](http://arxiv.org/abs/2305.06901)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06901] Assault and Battery: Evaluating the Security of Power Conversion Systems Against Electromagnetic Injection Attacks](http://arxiv.org/abs/2305.06901) #transformer`
* Summary: <p>Many modern devices, including critical infrastructures, depend on the
reliable operation of electrical power conversion systems. The small size and
versatility of switched-mode power converters has resulted in their widespread
adoption. Whereas transformer-based systems passively convert voltage,
switched-mode converters feature an actively regulated feedback loop, which
relies on accurate sensor measurements. Previous academic work has shown that
many types of sensors are vulnerable to Intentional Electromagnetic
Interference (IEMI) attacks, and it has been postulated that power converters,
too, are affected.
</p>
<p>In this paper, we present the first detailed study on switched-mode power
converters by targeting their voltage and current sensors through IEMI attacks.
We present a theoretical framework for evaluating IEMI attacks against
feedback-based power supplies in the general case. We experimentally validate
our theoretical predictions by analyzing multiple AC-DC and DC-DC converters,
automotive grade current sensors, and dedicated battery chargers, and
demonstrate the systematic vulnerability of all examined categories under
real-world conditions. Finally, we demonstrate that sensor attacks on power
converters can cause permanent damage to Li-Ion batteries during the charging
process.
</p>

### Title: Dynamic Graph Representation Learning for Depression Screening with Transformer. (arXiv:2305.06447v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.06447](http://arxiv.org/abs/2305.06447)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06447] Dynamic Graph Representation Learning for Depression Screening with Transformer](http://arxiv.org/abs/2305.06447) #transformer`
* Summary: <p>Early detection of mental disorder is crucial as it enables prompt
intervention and treatment, which can greatly improve outcomes for individuals
suffering from debilitating mental affliction. The recent proliferation of
mental health discussions on social media platforms presents research
opportunities to investigate mental health and potentially detect instances of
mental illness. However, existing depression detection methods are constrained
due to two major limitations: (1) the reliance on feature engineering and (2)
the lack of consideration for time-varying factors. Specifically, these methods
require extensive feature engineering and domain knowledge, which heavily rely
on the amount, quality, and type of user-generated content. Moreover, these
methods ignore the important impact of time-varying factors on depression
detection, such as the dynamics of linguistic patterns and interpersonal
interactive behaviors over time on social media (e.g., replies, mentions, and
quote-tweets). To tackle these limitations, we propose an early depression
detection framework, ContrastEgo treats each user as a dynamic time-evolving
attributed graph (ego-network) and leverages supervised contrastive learning to
maximize the agreement of users' representations at different scales while
minimizing the agreement of users' representations to differentiate between
depressed and control groups. ContrastEgo embraces four modules, (1)
constructing users' heterogeneous interactive graphs, (2) extracting the
representations of users' interaction snapshots using graph neural networks,
(3) modeling the sequences of snapshots using attention mechanism, and (4)
depression detection using contrastive learning. Extensive experiments on
Twitter data demonstrate that ContrastEgo significantly outperforms the
state-of-the-art methods in terms of all the effectiveness metrics in various
experimental settings.
</p>

## generative
### Title: Text-To-Concept (and Back) via Cross-Model Alignment. (arXiv:2305.06386v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06386](http://arxiv.org/abs/2305.06386)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06386] Text-To-Concept (and Back) via Cross-Model Alignment](http://arxiv.org/abs/2305.06386) #generative`
* Summary: <p>We observe that the mapping between an image's representation in one model to
its representation in another can be learned surprisingly well with just a
linear layer, even across diverse models. Building on this observation, we
propose $\textit{text-to-concept}$, where features from a fixed pretrained
model are aligned linearly to the CLIP space, so that text embeddings from
CLIP's text encoder become directly comparable to the aligned features. With
text-to-concept, we convert fixed off-the-shelf vision encoders to surprisingly
strong zero-shot classifiers for free, with accuracy at times even surpassing
that of CLIP, despite being much smaller models and trained on a small fraction
of the data compared to CLIP. We show other immediate use-cases of
text-to-concept, like building concept bottleneck models with no concept
supervision, diagnosing distribution shifts in terms of human concepts, and
retrieving images satisfying a set of text-based constraints. Lastly, we
demonstrate the feasibility of $\textit{concept-to-text}$, where vectors in a
model's feature space are decoded by first aligning to the CLIP before being
fed to a GPT-based generative model. Our work suggests existing deep models,
with presumably diverse architectures and training, represent input samples
relatively similarly, and a two-way communication across model representation
spaces and to humans (through language) is viable.
</p>

### Title: Evaluating Open-Domain Question Answering in the Era of Large Language Models. (arXiv:2305.06984v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.06984](http://arxiv.org/abs/2305.06984)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06984] Evaluating Open-Domain Question Answering in the Era of Large Language Models](http://arxiv.org/abs/2305.06984) #generative`
* Summary: <p>Lexical matching remains the de facto evaluation method for open-domain
question answering (QA). Unfortunately, lexical matching fails completely when
a plausible candidate answer does not appear in the list of gold answers, which
is increasingly the case as we shift from extractive to generative models. The
recent success of large language models (LLMs) for QA aggravates lexical
matching failures since candidate answers become longer, thereby making
matching with the gold answers even more challenging. Without accurate
evaluation, the true progress in open-domain QA remains unknown. In this paper,
we conduct a thorough analysis of various open-domain QA models, including
LLMs, by manually evaluating their answers on a subset of NQ-open, a popular
benchmark. Our assessments reveal that while the true performance of all models
is significantly underestimated, the performance of the InstructGPT (zero-shot)
LLM increases by nearly +60%, making it on par with existing top models, and
the InstructGPT (few-shot) model actually achieves a new state-of-the-art on
NQ-open. We also find that more than 50% of lexical matching failures are
attributed to semantically equivalent answers. We further demonstrate that
regex matching ranks QA models consistent with human judgments, although still
suffering from unnecessary strictness. Finally, we demonstrate that automated
evaluation models are a reasonable surrogate for lexical matching in some
circumstances, but not for long-form answers generated by LLMs. The automated
models struggle in detecting hallucinations in LLM answers and are thus unable
to evaluate LLMs. At this time, there appears to be no substitute for human
evaluation.
</p>

## label correction
## noise
### Title: Bot or Human? Detecting ChatGPT Imposters with A Single Question. (arXiv:2305.06424v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.06424](http://arxiv.org/abs/2305.06424)
* Code URL: [https://github.com/hongwang600/flair](https://github.com/hongwang600/flair)
* Copy Paste: `<input type="checkbox">[[2305.06424] Bot or Human? Detecting ChatGPT Imposters with A Single Question](http://arxiv.org/abs/2305.06424) #noise`
* Summary: <p>Large language models like ChatGPT have recently demonstrated impressive
capabilities in natural language understanding and generation, enabling various
applications including translation, essay writing, and chit-chatting. However,
there is a concern that they can be misused for malicious purposes, such as
fraud or denial-of-service attacks. Therefore, it is crucial to develop methods
for detecting whether the party involved in a conversation is a bot or a human.
In this paper, we propose a framework named FLAIR, Finding Large language model
Authenticity via a single Inquiry and Response, to detect conversational bots
in an online manner. Specifically, we target a single question scenario that
can effectively differentiate human users from bots. The questions are divided
into two categories: those that are easy for humans but difficult for bots
(e.g., counting, substitution, positioning, noise filtering, and ASCII art),
and those that are easy for bots but difficult for humans (e.g., memorization
and computation). Our approach shows different strengths of these questions in
their effectiveness, providing a new way for online service providers to
protect themselves against nefarious activities and ensure that they are
serving real users. We open-sourced our dataset on
https://github.com/hongwang600/FLAIR and welcome contributions from the
community to enrich such detection datasets.
</p>

### Title: When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.06626](http://arxiv.org/abs/2305.06626)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06626] When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks](http://arxiv.org/abs/2305.06626) #noise`
* Summary: <p>Though majority vote among annotators is typically used for ground truth
labels in natural language processing, annotator disagreement in tasks such as
hate speech detection may reflect differences among group opinions, not noise.
Thus, a crucial problem in hate speech detection is whether a statement is
offensive to the demographic group that it targets, which may constitute a
small fraction of the annotator pool. We construct a model that predicts
individual annotator ratings on potentially offensive text and combines this
information with the predicted target group of the text to model the opinions
of target group members. We show gains across a range of metrics, including
raising performance over the baseline by 22% at predicting individual
annotators' ratings and 33% at predicting variance among annotators, which
provides a method of measuring model uncertainty downstream. We find that
annotators' ratings can be predicted using their demographic information and
opinions on online content, without the need to track identifying annotator IDs
that link each annotator to their ratings. We also find that use of
non-invasive survey questions on annotators' online experiences helps to
maximize privacy and minimize unnecessary collection of demographic information
when predicting annotators' opinions.
</p>

### Title: Securing Distributed SGD against Gradient Leakage Threats. (arXiv:2305.06473v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.06473](http://arxiv.org/abs/2305.06473)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06473] Securing Distributed SGD against Gradient Leakage Threats](http://arxiv.org/abs/2305.06473) #noise`
* Summary: <p>This paper presents a holistic approach to gradient leakage resilient
distributed Stochastic Gradient Descent (SGD). First, we analyze two types of
strategies for privacy-enhanced federated learning: (i) gradient pruning with
random selection or low-rank filtering and (ii) gradient perturbation with
additive random noise or differential privacy noise. We analyze the inherent
limitations of these approaches and their underlying impact on privacy
guarantee, model accuracy, and attack resilience. Next, we present a gradient
leakage resilient approach to securing distributed SGD in federated learning,
with differential privacy controlled noise as the tool. Unlike conventional
methods with the per-client federated noise injection and fixed noise parameter
strategy, our approach keeps track of the trend of per-example gradient
updates. It makes adaptive noise injection closely aligned throughout the
federated model training. Finally, we provide an empirical privacy analysis on
the privacy guarantee, model utility, and attack resilience of the proposed
approach. Extensive evaluation using five benchmark datasets demonstrates that
our gradient leakage resilient approach can outperform the state-of-the-art
methods with competitive accuracy performance, strong differential privacy
guarantee, and high resilience against gradient leakage attacks. The code
associated with this paper can be found:
https://github.com/git-disl/Fed-alphaCDP.
</p>

### Title: Watch This Space: Securing Satellite Communication through Resilient Transmitter Fingerprinting. (arXiv:2305.06947v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2305.06947](http://arxiv.org/abs/2305.06947)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06947] Watch This Space: Securing Satellite Communication through Resilient Transmitter Fingerprinting](http://arxiv.org/abs/2305.06947) #noise`
* Summary: <p>Due to an increase in the availability of cheap off-the-shelf radio hardware,
spoofing and replay attacks on satellite ground systems have become more
accessible than ever. This is particularly a problem for legacy systems, many
of which do not offer cryptographic security and cannot be patched to support
novel security measures.
</p>
<p>In this paper we explore radio transmitter fingerprinting in satellite
systems. We introduce the SatIQ system, proposing novel techniques for
authenticating transmissions using characteristics of transmitter hardware
expressed as impairments on the downlinked signal. We look in particular at
high sample rate fingerprinting, making fingerprints difficult to forge without
similarly high sample rate transmitting hardware, thus raising the budget for
attacks. We also examine the difficulty of this approach with high levels of
atmospheric noise and multipath scattering, and analyze potential solutions to
this problem.
</p>
<p>We focus on the Iridium satellite constellation, for which we collected
1010464 messages at a sample rate of 25 MS/s. We use this data to train a
fingerprinting model consisting of an autoencoder combined with a Siamese
neural network, enabling the model to learn an efficient encoding of message
headers that preserves identifying information.
</p>
<p>We demonstrate the system's robustness under attack by replaying messages
using a Software-Defined Radio, achieving an Equal Error Rate of 0.120, and ROC
AUC of 0.946. Finally, we analyze its stability over time by introducing a time
gap between training and testing data, and its extensibility by introducing new
transmitters which have not been seen before. We conclude that our techniques
are useful for building systems that are stable over time, can be used
immediately with new transmitters without retraining, and provide robustness
against spoofing and replay by raising the required budget for attacks.
</p>

### Title: A fast topological approach for predicting anomalies in time-varying graphs. (arXiv:2305.06523v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.06523](http://arxiv.org/abs/2305.06523)
* Code URL: [https://github.com/tdavecs/vab](https://github.com/tdavecs/vab)
* Copy Paste: `<input type="checkbox">[[2305.06523] A fast topological approach for predicting anomalies in time-varying graphs](http://arxiv.org/abs/2305.06523) #noise`
* Summary: <p>Large time-varying graphs are increasingly common in financial, social and
biological settings. Feature extraction that efficiently encodes the complex
structure of sparse, multi-layered, dynamic graphs presents computational and
methodological challenges. In the past decade, a persistence diagram (PD) from
topological data analysis (TDA) has become a popular descriptor of shape of
data with a well-defined distance between points. However, applications of TDA
to graphs, where there is no intrinsic concept of distance between the nodes,
remain largely unexplored. This paper addresses this gap in the literature by
introducing a computationally efficient framework to extract shape information
from graph data. Our framework has two main steps: first, we compute a PD using
the so-called lower-star filtration which utilizes quantitative node
attributes, and then vectorize it by averaging the associated Betti function
over successive scale values on a one-dimensional grid. Our approach avoids
embedding a graph into a metric space and has stability properties against
input noise. In simulation studies, we show that the proposed vector summary
leads to improved change point detection rate in time-varying graphs. In a real
data application, our approach provides up to 22% gain in anomalous price
prediction for the Ethereum cryptocurrency transaction networks.
</p>

### Title: Active Learning in the Predict-then-Optimize Framework: A Margin-Based Approach. (arXiv:2305.06584v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.06584](http://arxiv.org/abs/2305.06584)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06584] Active Learning in the Predict-then-Optimize Framework: A Margin-Based Approach](http://arxiv.org/abs/2305.06584) #noise`
* Summary: <p>We develop the first active learning method in the predict-then-optimize
framework. Specifically, we develop a learning method that sequentially decides
whether to request the "labels" of feature samples from an unlabeled data
stream, where the labels correspond to the parameters of an optimization model
for decision-making. Our active learning method is the first to be directly
informed by the decision error induced by the predicted parameters, which is
referred to as the Smart Predict-then-Optimize (SPO) loss. Motivated by the
structure of the SPO loss, our algorithm adopts a margin-based criterion
utilizing the concept of distance to degeneracy and minimizes a tractable
surrogate of the SPO loss on the collected data. In particular, we develop an
efficient active learning algorithm with both hard and soft rejection variants,
each with theoretical excess risk (i.e., generalization) guarantees. We further
derive bounds on the label complexity, which refers to the number of samples
whose labels are acquired to achieve a desired small level of SPO risk. Under
some natural low-noise conditions, we show that these bounds can be better than
the naive supervised learning approach that labels all samples. Furthermore,
when using the SPO+ loss function, a specialized surrogate of the SPO loss, we
derive a significantly smaller label complexity under separability conditions.
We also present numerical evidence showing the practical value of our proposed
algorithms in the settings of personalized pricing and the shortest path
problem.
</p>

## diffusion
### Title: Analyzing Bias in Diffusion-based Face Generation Models. (arXiv:2305.06402v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06402](http://arxiv.org/abs/2305.06402)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06402] Analyzing Bias in Diffusion-based Face Generation Models](http://arxiv.org/abs/2305.06402) #diffusion`
* Summary: <p>Diffusion models are becoming increasingly popular in synthetic data
generation and image editing applications. However, these models can amplify
existing biases and propagate them to downstream applications. Therefore, it is
crucial to understand the sources of bias in their outputs. In this paper, we
investigate the presence of bias in diffusion-based face generation models with
respect to attributes such as gender, race, and age. Moreover, we examine how
dataset size affects the attribute composition and perceptual quality of both
diffusion and Generative Adversarial Network (GAN) based face generation models
across various attribute classes. Our findings suggest that diffusion models
tend to worsen distribution bias in the training data for various attributes,
which is heavily influenced by the size of the dataset. Conversely, GAN models
trained on balanced datasets with a larger number of samples show less bias
across different attributes.
</p>

### Title: Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator. (arXiv:2305.06710v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06710](http://arxiv.org/abs/2305.06710)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06710] Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator](http://arxiv.org/abs/2305.06710) #diffusion`
* Summary: <p>Classifier-free guidance is an effective sampling technique in diffusion
models that has been widely adopted. The main idea is to extrapolate the model
in the direction of text guidance and away from null-text guidance. In this
paper, we demonstrate that null-text guidance in diffusion models is secretly a
cartoon-style creator, i.e., the generated images can be efficiently
transformed into cartoons by simply perturbing the null-text guidance.
Specifically, we proposed two disturbance methods, i.e., Rollback disturbance
(Back-D) and Image disturbance (Image-D), to construct misalignment between the
noisy images used for predicting null-text guidance and text guidance
(subsequently referred to as \textbf{null-text noisy image} and \textbf{text
noisy image} respectively) in the sampling process. Back-D achieves
cartoonization by altering the noise level of null-text noisy image via
replacing $x_t$ with $x_{t+\Delta t}$. Image-D, alternatively, produces
high-fidelity, diverse cartoons by defining $x_t$ as a clean input image, which
further improves the incorporation of finer image details. Through
comprehensive experiments, we delved into the principle of noise disturbing for
null-text and uncovered that the efficacy of disturbance depends on the
correlation between the null-text noisy image and the source image. Moreover,
our proposed techniques, which can generate cartoon images and cartoonize
specific ones, are training-free and easily integrated as a plug-and-play
component in any classifier-free guided diffusion model. Project page is
available at \url{https://nulltextforcartoon.github.io/}.
</p>

### Title: Exploiting Diffusion Prior for Real-World Image Super-Resolution. (arXiv:2305.07015v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.07015](http://arxiv.org/abs/2305.07015)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.07015] Exploiting Diffusion Prior for Real-World Image Super-Resolution](http://arxiv.org/abs/2305.07015) #diffusion`
* Summary: <p>We present a novel approach to leverage prior knowledge encapsulated in
pre-trained text-to-image diffusion models for blind super-resolution (SR).
Specifically, by employing our time-aware encoder, we can achieve promising
restoration results without altering the pre-trained synthesis model, thereby
preserving the generative prior and minimizing training cost. To remedy the
loss of fidelity caused by the inherent stochasticity of diffusion models, we
introduce a controllable feature wrapping module that allows users to balance
quality and fidelity by simply adjusting a scalar value during the inference
process. Moreover, we develop a progressive aggregation sampling strategy to
overcome the fixed-size constraints of pre-trained diffusion models, enabling
adaptation to resolutions of any size. A comprehensive evaluation of our method
using both synthetic and real-world benchmarks demonstrates its superiority
over current state-of-the-art approaches.
</p>

## LLM
## segmentation
### Title: An Empirical Study on the Robustness of the Segment Anything Model (SAM). (arXiv:2305.06422v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06422](http://arxiv.org/abs/2305.06422)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06422] An Empirical Study on the Robustness of the Segment Anything Model (SAM)](http://arxiv.org/abs/2305.06422) #segmentation`
* Summary: <p>The Segment Anything Model (SAM) is a foundation model for general image
segmentation. Although it exhibits impressive performance predominantly on
natural images, understanding its robustness against various image
perturbations and domains is critical for real-world applications where such
challenges frequently arise. In this study we conduct a comprehensive
robustness investigation of SAM under diverse real-world conditions. Our
experiments encompass a wide range of image perturbations. Our experimental
results demonstrate that SAM's performance generally declines under perturbed
images, with varying degrees of vulnerability across different perturbations.
By customizing prompting techniques and leveraging domain knowledge based on
the unique characteristics of each dataset, the model's resilience to these
perturbations can be enhanced, addressing dataset-specific challenges. This
work sheds light on the limitations and strengths of SAM in real-world
applications, promoting the development of more robust and versatile image
segmentation solutions.
</p>

### Title: WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents. (arXiv:2305.06553v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06553](http://arxiv.org/abs/2305.06553)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06553] WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents](http://arxiv.org/abs/2305.06553) #segmentation`
* Summary: <p>In this paper, we introduce WeLayout, a novel system for segmenting the
layout of corporate documents, which stands for WeChat Layout Analysis System.
Our approach utilizes a sophisticated ensemble of DINO and YOLO models,
specifically developed for the ICDAR 2023 Competition on Robust Layout
Segmentation. Our method significantly surpasses the baseline, securing a top
position on the leaderboard with a mAP of 70.0. To achieve this performance, we
concentrated on enhancing various aspects of the task, such as dataset
augmentation, model architecture, bounding box refinement, and model ensemble
techniques. Additionally, we trained the data separately for each document
category to ensure a higher mean submission score. We also developed an
algorithm for cell matching to further improve our performance. To identify the
optimal weights and IoU thresholds for our model ensemble, we employed a
Bayesian optimization algorithm called the Tree-Structured Parzen Estimator.
Our approach effectively demonstrates the benefits of combining query-based and
anchor-free models for achieving robust layout segmentation in corporate
documents.
</p>

### Title: Segment and Track Anything. (arXiv:2305.06558v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06558](http://arxiv.org/abs/2305.06558)
* Code URL: [https://github.com/z-x-yang/segment-and-track-anything](https://github.com/z-x-yang/segment-and-track-anything)
* Copy Paste: `<input type="checkbox">[[2305.06558] Segment and Track Anything](http://arxiv.org/abs/2305.06558) #segmentation`
* Summary: <p>This report presents a framework called Segment And Track Anything (SAMTrack)
that allows users to precisely and effectively segment and track any object in
a video. Additionally, SAM-Track employs multimodal interaction methods that
enable users to select multiple objects in videos for tracking, corresponding
to their specific requirements. These interaction methods comprise click,
stroke, and text, each possessing unique benefits and capable of being employed
in combination. As a result, SAM-Track can be used across an array of fields,
ranging from drone technology, autonomous driving, medical imaging, augmented
reality, to biological analysis. SAM-Track amalgamates Segment Anything Model
(SAM), an interactive key-frame segmentation model, with our proposed AOT-based
tracking model (DeAOT), which secured 1st place in four tracks of the VOT 2022
challenge, to facilitate object tracking in video. In addition, SAM-Track
incorporates Grounding-DINO, which enables the framework to support text-based
interaction. We have demonstrated the remarkable capabilities of SAM-Track on
DAVIS-2016 Val (92.0%), DAVIS-2017 Test (79.2%)and its practicability in
diverse applications. The project page is available at:
https://github.com/z-x-yang/Segment-and-Track-Anything.
</p>

### Title: Convolutional Neural Networks Rarely Learn Shape for Semantic Segmentation. (arXiv:2305.06568v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06568](http://arxiv.org/abs/2305.06568)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06568] Convolutional Neural Networks Rarely Learn Shape for Semantic Segmentation](http://arxiv.org/abs/2305.06568) #segmentation`
* Summary: <p>Shape learning, or the ability to leverage shape information, could be a
desirable property of convolutional neural networks (CNNs) when target objects
have specific shapes. While some research on the topic is emerging, there is no
systematic study to conclusively determine whether and under what circumstances
CNNs learn shape. Here, we present such a study in the context of segmentation
networks where shapes are particularly important. We define shape and propose a
new behavioral metric to measure the extent to which a CNN utilizes shape
information. We then execute a set of experiments with synthetic and real-world
data to progressively uncover under which circumstances CNNs learn shape and
what can be done to encourage such behavior. We conclude that (i) CNNs do not
learn shape in typical settings but rather rely on other features available to
identify the objects of interest, (ii) CNNs can learn shape, but only if the
shape is the only feature available to identify the object, (iii) sufficiently
large receptive field size relative to the size of target objects is necessary
for shape learning; (iv) a limited set of augmentations can encourage shape
learning; (v) learning shape is indeed useful in the presence of
out-of-distribution data.
</p>

### Title: Bi-level Dynamic Learning for Jointly Multi-modality Image Fusion and Beyond. (arXiv:2305.06720v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06720](http://arxiv.org/abs/2305.06720)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06720] Bi-level Dynamic Learning for Jointly Multi-modality Image Fusion and Beyond](http://arxiv.org/abs/2305.06720) #segmentation`
* Summary: <p>Recently, multi-modality scene perception tasks, e.g., image fusion and scene
understanding, have attracted widespread attention for intelligent vision
systems. However, early efforts always consider boosting a single task
unilaterally and neglecting others, seldom investigating their underlying
connections for joint promotion. To overcome these limitations, we establish
the hierarchical dual tasks-driven deep model to bridge these tasks.
Concretely, we firstly construct an image fusion module to fuse complementary
characteristics and cascade dual task-related modules, including a
discriminator for visual effects and a semantic network for feature
measurement. We provide a bi-level perspective to formulate image fusion and
follow-up downstream tasks. To incorporate distinct task-related responses for
image fusion, we consider image fusion as a primary goal and dual modules as
learnable constraints. Furthermore, we develop an efficient first-order
approximation to compute corresponding gradients and present dynamic weighted
aggregation to balance the gradients for fusion learning. Extensive experiments
demonstrate the superiority of our method, which not only produces visually
pleasant fused results but also realizes significant promotion for detection
and segmentation than the state-of-the-art approaches.
</p>

### Title: Towards a Better Understanding of the Computer Vision Research Community in Africa. (arXiv:2305.06773v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06773](http://arxiv.org/abs/2305.06773)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06773] Towards a Better Understanding of the Computer Vision Research Community in Africa](http://arxiv.org/abs/2305.06773) #segmentation`
* Summary: <p>Computer vision is a broad field of study that encompasses different tasks
(e.g., object detection, semantic segmentation, 3D reconstruction). Although
computer vision is relevant to the African communities in various applications,
yet computer vision research is under-explored in the continent and constructs
only 0.06% of top-tier publications in the last 10 years. In this paper, our
goal is to have a better understanding of the computer vision research
conducted in Africa and provide pointers on whether there is equity in research
or not. We do this through an empirical analysis of the African computer vision
publications that are Scopus indexed. We first study the opportunities
available for African institutions to publish in top-tier computer vision
venues. We show that African publishing trends in top-tier venues over the
years do not exhibit consistent growth. We also devise a novel way to retrieve
African authors through their affiliation history to have a better
understanding of their contributions in top-tier venues. Moreover, we study all
computer vision publications beyond top-tier venues in different African
regions to find that mainly Northern and Southern Africa are publishing in
computer vision with more than 85% of African publications. Finally, we present
the most recurring keywords in computer vision publications. In summary, our
analysis reveals that African researchers are key contributors to African
research, yet there exists multiple barriers to publish in top-tier venues and
the current trend of topics published in the continent might not necessarily
reflect the communities' needs. This work is part of a community based effort
that is focused on improving computer vision research in Africa.
</p>

### Title: Meta-Learners for Few-Shot Weakly-Supervised Medical Image Segmentation. (arXiv:2305.06912v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06912](http://arxiv.org/abs/2305.06912)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06912] Meta-Learners for Few-Shot Weakly-Supervised Medical Image Segmentation](http://arxiv.org/abs/2305.06912) #segmentation`
* Summary: <p>Most uses of Meta-Learning in visual recognition are very often applied to
image classification, with a relative lack of works in other tasks {such} as
segmentation and detection. We propose a generic Meta-Learning framework for
few-shot weakly-supervised segmentation in medical imaging domains. We conduct
a comparative analysis of meta-learners from distinct paradigms adapted to
few-shot image segmentation in different sparsely annotated radiological tasks.
The imaging modalities include 2D chest, mammographic and dental X-rays, as
well as 2D slices of volumetric tomography and resonance images. Our
experiments consider a total of 9 meta-learners, 4 backbones and multiple
target organ segmentation tasks. We explore small-data scenarios in radiology
with varying weak annotation styles and densities. Our analysis shows that
metric-based meta-learning approaches achieve better segmentation results in
tasks with smaller domain shifts in comparison to the meta-training datasets,
while some gradient- and fusion-based meta-learners are more generalizable to
larger domain shifts.
</p>

### Title: FreePoint: Unsupervised Point Cloud Instance Segmentation. (arXiv:2305.06973v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06973](http://arxiv.org/abs/2305.06973)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06973] FreePoint: Unsupervised Point Cloud Instance Segmentation](http://arxiv.org/abs/2305.06973) #segmentation`
* Summary: <p>Instance segmentation of point clouds is a crucial task in 3D field with
numerous applications that involve localizing and segmenting objects in a
scene. However, achieving satisfactory results requires a large number of
manual annotations, which is a time-consuming and expensive process. To
alleviate dependency on annotations, we propose a method, called FreePoint, for
underexplored unsupervised class-agnostic instance segmentation on point
clouds. In detail, we represent the point features by combining coordinates,
colors, normals, and self-supervised deep features. Based on the point
features, we perform a multicut algorithm to segment point clouds into coarse
instance masks as pseudo labels, which are used to train a point cloud instance
segmentation model. To alleviate the inaccuracy of coarse masks during
training, we propose a weakly-supervised training strategy and corresponding
loss. Our work can also serve as an unsupervised pre-training pretext for
supervised semantic instance segmentation with limited annotations. For
class-agnostic instance segmentation on point clouds, FreePoint largely fills
the gap with its fully-supervised counterpart based on the state-of-the-art
instance segmentation model Mask3D and even surpasses some previous
fully-supervised methods. When serving as a pretext task and fine-tuning on
S3DIS, FreePoint outperforms training from scratch by 5.8% AP with only 10%
mask annotations.
</p>

### Title: Meta-hallucinator: Towards Few-Shot Cross-Modality Cardiac Image Segmentation. (arXiv:2305.06978v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06978](http://arxiv.org/abs/2305.06978)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06978] Meta-hallucinator: Towards Few-Shot Cross-Modality Cardiac Image Segmentation](http://arxiv.org/abs/2305.06978) #segmentation`
* Summary: <p>Domain shift and label scarcity heavily limit deep learning applications to
various medical image analysis tasks. Unsupervised domain adaptation (UDA)
techniques have recently achieved promising cross-modality medical image
segmentation by transferring knowledge from a label-rich source domain to an
unlabeled target domain. However, it is also difficult to collect annotations
from the source domain in many clinical applications, rendering most prior
works suboptimal with the label-scarce source domain, particularly for few-shot
scenarios, where only a few source labels are accessible. To achieve efficient
few-shot cross-modality segmentation, we propose a novel
transformation-consistent meta-hallucination framework, meta-hallucinator, with
the goal of learning to diversify data distributions and generate useful
examples for enhancing cross-modality performance. In our framework,
hallucination and segmentation models are jointly trained with the
gradient-based meta-learning strategy to synthesize examples that lead to good
segmentation performance on the target domain. To further facilitate data
hallucination and cross-domain knowledge transfer, we develop a self-ensembling
model with a hallucination-consistent property. Our meta-hallucinator can
seamlessly collaborate with the meta-segmenter for learning to hallucinate with
mutual benefits from a combined view of meta-learning and self-ensembling
learning. Extensive studies on MM-WHS 2017 dataset for cross-modality cardiac
segmentation demonstrate that our method performs favorably against various
approaches by a lot in the few-shot UDA scenario.
</p>

### Title: Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation. (arXiv:2305.07005v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.07005](http://arxiv.org/abs/2305.07005)
* Code URL: [https://github.com/francois-meyer/ssmt](https://github.com/francois-meyer/ssmt)
* Copy Paste: `<input type="checkbox">[[2305.07005] Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation](http://arxiv.org/abs/2305.07005) #segmentation`
* Summary: <p>Subword segmenters like BPE operate as a preprocessing step in neural machine
translation and other (conditional) language models. They are applied to
datasets before training, so translation or text generation quality relies on
the quality of segmentations. We propose a departure from this paradigm, called
subword segmental machine translation (SSMT). SSMT unifies subword segmentation
and MT in a single trainable model. It learns to segment target sentence words
while jointly learning to generate target sentences. To use SSMT during
inference we propose dynamic decoding, a text generation algorithm that adapts
segmentations as it generates translations. Experiments across 6 translation
directions show that SSMT improves chrF scores for morphologically rich
agglutinative languages. Gains are strongest in the very low-resource scenario.
SSMT also learns subwords that are closer to morphemes compared to baselines
and proves more robust on a test set constructed for evaluating morphological
compositional generalisation.
</p>

## object detection
### Title: DeepSTEP -- Deep Learning-Based Spatio-Temporal End-To-End Perception for Autonomous Vehicles. (arXiv:2305.06820v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06820](http://arxiv.org/abs/2305.06820)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.06820] DeepSTEP -- Deep Learning-Based Spatio-Temporal End-To-End Perception for Autonomous Vehicles](http://arxiv.org/abs/2305.06820) #object detection`
* Summary: <p>Autonomous vehicles demand high accuracy and robustness of perception
algorithms. To develop efficient and scalable perception algorithms, the
maximum information should be extracted from the available sensor data. In this
work, we present our concept for an end-to-end perception architecture, named
DeepSTEP. The deep learning-based architecture processes raw sensor data from
the camera, LiDAR, and RaDAR, and combines the extracted data in a deep fusion
network. The output of this deep fusion network is a shared feature space,
which is used by perception head networks to fulfill several perception tasks,
such as object detection or local mapping. DeepSTEP incorporates multiple ideas
to advance state of the art: First, combining detection and localization into a
single pipeline allows for efficient processing to reduce computational
overhead and further improves overall performance. Second, the architecture
leverages the temporal domain by using a self-attention mechanism that focuses
on the most important features. We believe that our concept of DeepSTEP will
advance the development of end-to-end perception systems. The network will be
deployed on our research vehicle, which will be used as a platform for data
collection, real-world testing, and validation. In conclusion, DeepSTEP
represents a significant advancement in the field of perception for autonomous
vehicles. The architecture's end-to-end design, time-aware attention mechanism,
and integration of multiple perception tasks make it a promising solution for
real-world deployment. This research is a work in progress and presents the
first concept of establishing a novel perception pipeline.
</p>

### Title: SalienDet: A Saliency-based Feature Enhancement Algorithm for Object Detection for Autonomous Driving. (arXiv:2305.06940v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.06940](http://arxiv.org/abs/2305.06940)
* Code URL: [https://github.com/dingmike001/saliendet-open-detection](https://github.com/dingmike001/saliendet-open-detection)
* Copy Paste: `<input type="checkbox">[[2305.06940] SalienDet: A Saliency-based Feature Enhancement Algorithm for Object Detection for Autonomous Driving](http://arxiv.org/abs/2305.06940) #object detection`
* Summary: <p>Object detection (OD) is crucial to autonomous driving. Unknown objects are
one of the reasons that hinder autonomous vehicles from driving beyond the
operational domain. We propose a saliency-based OD algorithm (SalienDet) to
detect objects that do not appear in the training sample set. SalienDet
utilizes a saliency-based algorithm to enhance image features for object
proposal generation. Then, we design a dataset relabeling approach to
differentiate the unknown objects from all objects to achieve open-world
detection. We evaluate SalienDet on KITTI, NuScenes, and BDD datasets, and the
result indicates that it outperforms existing algorithms for unknown object
detection. Additionally, SalienDet can be easily adapted for incremental
learning in open-world detection tasks.
</p>

