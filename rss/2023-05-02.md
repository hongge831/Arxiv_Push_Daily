## data-free
## transformer
### Title: MMViT: Multiscale Multiview Vision Transformers. (arXiv:2305.00104v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00104](http://arxiv.org/abs/2305.00104)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00104] MMViT: Multiscale Multiview Vision Transformers](http://arxiv.org/abs/2305.00104) #transformer`
* Summary: <p>We present Multiscale Multiview Vision Transformers (MMViT), which introduces
multiscale feature maps and multiview encodings to transformer models. Our
model encodes different views of the input signal and builds several
channel-resolution feature stages to process the multiple views of the input at
different resolutions in parallel. At each scale stage, we use a
cross-attention block to fuse information across different views. This enables
the MMViT model to acquire complex high-dimensional representations of the
input at different resolutions. The proposed model can serve as a backbone
model in multiple domains. We demonstrate the effectiveness of MMViT on audio
and image classification tasks, achieving state-of-the-art results.
</p>

### Title: Searching from Area to Point: A Hierarchical Framework for Semantic-Geometric Combined Feature Matching. (arXiv:2305.00194v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00194](http://arxiv.org/abs/2305.00194)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00194] Searching from Area to Point: A Hierarchical Framework for Semantic-Geometric Combined Feature Matching](http://arxiv.org/abs/2305.00194) #transformer`
* Summary: <p>Feature matching is a crucial technique in computer vision. Essentially, it
can be considered as a searching problem to establish correspondences between
images. The key challenge in this task lies in the lack of a well-defined
search space, leading to inaccurate point matching of current methods. In
pursuit of a reasonable matching search space, this paper introduces a
hierarchical feature matching framework: Area to Point Matching (A2PM), to
first find semantic area matches between images, and then perform point
matching on area matches, thus setting the search space as the area matches
with salient features to achieve high matching precision. This proper search
space of A2PM framework also alleviates the accuracy limitation in
state-of-the-art Transformer-based matching methods. To realize this framework,
we further propose Semantic and Geometry Area Matching (SGAM) method, which
utilizes semantic prior and geometry consistency to establish accurate area
matches between images. By integrating SGAM with off-the-shelf
Transformer-based matchers, our feature matching methods, adopting the A2PM
framework, achieve encouraging precision improvements in massive point matching
and pose estimation experiments for present arts.
</p>

### Title: Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT. (arXiv:2305.00201v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00201](http://arxiv.org/abs/2305.00201)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00201] Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT](http://arxiv.org/abs/2305.00201) #transformer`
* Summary: <p>Prompts have been proven to play a crucial role in large language models, and
in recent years, vision models have also been using prompts to improve
scalability for multiple downstream tasks. In this paper, we focus on adapting
prompt design based on instruction tuning into a visual transformer model for
image classification which we called Instruction-ViT. The key idea is to
implement multi-modal prompts (text or image prompt) related to category
information to guide the fine-tuning of the model. Based on the experiments of
several image captionining tasks, the performance and domain adaptability were
improved. Our work provided an innovative strategy to fuse multi-modal prompts
with better performance and faster adaptability for visual classification
models.
</p>

### Title: Modality-invariant Visual Odometry for Embodied Vision. (arXiv:2305.00348v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00348](http://arxiv.org/abs/2305.00348)
* Code URL: [https://github.com/memmelma/vo-transformer](https://github.com/memmelma/vo-transformer)
* Copy Paste: `<input type="checkbox">[[2305.00348] Modality-invariant Visual Odometry for Embodied Vision](http://arxiv.org/abs/2305.00348) #transformer`
* Summary: <p>Effectively localizing an agent in a realistic, noisy setting is crucial for
many embodied vision tasks. Visual Odometry (VO) is a practical substitute for
unreliable GPS and compass sensors, especially in indoor environments. While
SLAM-based methods show a solid performance without large data requirements,
they are less flexible and robust w.r.t. to noise and changes in the sensor
suite compared to learning-based approaches. Recent deep VO models, however,
limit themselves to a fixed set of input modalities, e.g., RGB and depth, while
training on millions of samples. When sensors fail, sensor suites change, or
modalities are intentionally looped out due to available resources, e.g., power
consumption, the models fail catastrophically. Furthermore, training these
models from scratch is even more expensive without simulator access or suitable
existing models that can be fine-tuned. While such scenarios get mostly ignored
in simulation, they commonly hinder a model's reusability in real-world
applications. We propose a Transformer-based modality-invariant VO approach
that can deal with diverse or changing sensor suites of navigation agents. Our
model outperforms previous methods while training on only a fraction of the
data. We hope this method opens the door to a broader range of real-world
applications that can benefit from flexible and learned VO models.
</p>

### Title: MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer. (arXiv:2305.00355v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00355](http://arxiv.org/abs/2305.00355)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00355] MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer](http://arxiv.org/abs/2305.00355) #transformer`
* Summary: <p>With the increasing demand for video understanding, video moment and
highlight detection (MHD) has emerged as a critical research topic. MHD aims to
localize all moments and predict clip-wise saliency scores simultaneously.
Despite progress made by existing DETR-based methods, we observe that these
methods coarsely fuse features from different modalities, which weakens the
temporal intra-modal context and results in insufficient cross-modal
interaction. To address this issue, we propose MH-DETR (Moment and Highlight
Detection Transformer) tailored for MHD. Specifically, we introduce a simple
yet efficient pooling operator within the uni-modal encoder to capture global
intra-modal context. Moreover, to obtain temporally aligned cross-modal
features, we design a plug-and-play cross-modal interaction module between the
encoder and decoder, seamlessly integrating visual and textual features.
Comprehensive experiments on QVHighlights, Charades-STA, Activity-Net, and
TVSum datasets show that MH-DETR outperforms existing state-of-the-art methods,
demonstrating its effectiveness and superiority. Our code is available at
https://github.com/YoucanBaby/MH-DETR.
</p>

### Title: TransCAR: Transformer-based Camera-And-Radar Fusion for 3D Object Detection. (arXiv:2305.00397v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00397](http://arxiv.org/abs/2305.00397)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00397] TransCAR: Transformer-based Camera-And-Radar Fusion for 3D Object Detection](http://arxiv.org/abs/2305.00397) #transformer`
* Summary: <p>Despite radar's popularity in the automotive industry, for fusion-based 3D
object detection, most existing works focus on LiDAR and camera fusion. In this
paper, we propose TransCAR, a Transformer-based Camera-And-Radar fusion
solution for 3D object detection. Our TransCAR consists of two modules. The
first module learns 2D features from surround-view camera images and then uses
a sparse set of 3D object queries to index into these 2D features. The
vision-updated queries then interact with each other via transformer
self-attention layer. The second module learns radar features from multiple
radar scans and then applies transformer decoder to learn the interactions
between radar features and vision-updated queries. The cross-attention layer
within the transformer decoder can adaptively learn the soft-association
between the radar features and vision-updated queries instead of
hard-association based on sensor calibration only. Finally, our model estimates
a bounding box per query using set-to-set Hungarian loss, which enables the
method to avoid non-maximum suppression. TransCAR improves the velocity
estimation using the radar scans without temporal information. The superior
experimental results of our TransCAR on the challenging nuScenes datasets
illustrate that our TransCAR outperforms state-of-the-art Camera-Radar
fusion-based 3D object detection approaches.
</p>

### Title: Discriminative Co-Saliency and Background Mining Transformer for Co-Salient Object Detection. (arXiv:2305.00514v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00514](http://arxiv.org/abs/2305.00514)
* Code URL: [https://github.com/dragonlee258079/DMT](https://github.com/dragonlee258079/DMT)
* Copy Paste: `<input type="checkbox">[[2305.00514] Discriminative Co-Saliency and Background Mining Transformer for Co-Salient Object Detection](http://arxiv.org/abs/2305.00514) #transformer`
* Summary: <p>Most previous co-salient object detection works mainly focus on extracting
co-salient cues via mining the consistency relations across images while
ignoring explicit exploration of background regions. In this paper, we propose
a Discriminative co-saliency and background Mining Transformer framework (DMT)
based on several economical multi-grained correlation modules to explicitly
mine both co-saliency and background information and effectively model their
discrimination. Specifically, we first propose a region-to-region correlation
module for introducing inter-image relations to pixel-wise segmentation
features while maintaining computational efficiency. Then, we use two types of
pre-defined tokens to mine co-saliency and background information via our
proposed contrast-induced pixel-to-token correlation and co-saliency
token-to-token correlation modules. We also design a token-guided feature
refinement module to enhance the discriminability of the segmentation features
under the guidance of the learned tokens. We perform iterative mutual promotion
for the segmentation feature extraction and token construction. Experimental
results on three benchmark datasets demonstrate the effectiveness of our
proposed method. The source code is available at:
https://github.com/dragonlee258079/DMT.
</p>

### Title: Multimodal Graph Transformer for Multimodal Question Answering. (arXiv:2305.00581v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00581](http://arxiv.org/abs/2305.00581)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00581] Multimodal Graph Transformer for Multimodal Question Answering](http://arxiv.org/abs/2305.00581) #transformer`
* Summary: <p>Despite the success of Transformer models in vision and language tasks, they
often learn knowledge from enormous data implicitly and cannot utilize
structured input data directly. On the other hand, structured learning
approaches such as graph neural networks (GNNs) that integrate prior
information can barely compete with Transformer models. In this work, we aim to
benefit from both worlds and propose a novel Multimodal Graph Transformer for
question answering tasks that requires performing reasoning across multiple
modalities. We introduce a graph-involved plug-and-play quasi-attention
mechanism to incorporate multimodal graph information, acquired from text and
visual data, to the vanilla self-attention as effective prior. In particular,
we construct the text graph, dense region graph, and semantic graph to generate
adjacency matrices, and then compose them with input vision and language
features to perform downstream reasoning. Such a way of regularizing
self-attention with graph information significantly improves the inferring
ability and helps align features from different modalities. We validate the
effectiveness of Multimodal Graph Transformer over its Transformer baselines on
GQA, VQAv2, and MultiModalQA datasets.
</p>

### Title: Consolidator: Mergeable Adapter with Grouped Connections for Visual Adaptation. (arXiv:2305.00603v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00603](http://arxiv.org/abs/2305.00603)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00603] Consolidator: Mergeable Adapter with Grouped Connections for Visual Adaptation](http://arxiv.org/abs/2305.00603) #transformer`
* Summary: <p>Recently, transformers have shown strong ability as visual feature
extractors, surpassing traditional convolution-based models in various
scenarios. However, the success of vision transformers largely owes to their
capacity to accommodate numerous parameters. As a result, new challenges for
adapting large models to downstream tasks arise. On the one hand, classic
fine-tuning tunes all parameters in a huge model for every task and thus easily
falls into overfitting, leading to inferior performance. On the other hand, on
resource-limited devices, fine-tuning stores a full copy of parameters and thus
is usually impracticable for the shortage of storage space. However, few works
have focused on how to efficiently and effectively transfer knowledge in a
vision transformer. Existing methods did not dive into the properties of visual
features, leading to inferior performance. Moreover, some of them bring heavy
inference cost though benefiting storage. To tackle these problems, we propose
consolidator to modify the pre-trained model with the addition of a small set
of tunable parameters to temporarily store the task-specific knowledge while
freezing the backbone model. Motivated by the success of group-wise
convolution, we adopt grouped connections across the features extracted by
fully connected layers to construct tunable parts in a consolidator. To further
enhance the model's capacity to transfer knowledge under a constrained storage
budget and keep inference efficient, we consolidate the parameters in two
stages: 1. between adaptation and storage, and 2. between loading and
inference. On a series of downstream visual tasks, our consolidator can reach
up to 7.56 better accuracy than full fine-tuning with merely 0.35% parameters,
and outperform state-of-the-art parameter-efficient tuning methods by a clear
margin. Code is available at https://github.com/beyondhtx/Consolidator.
</p>

### Title: End to End Lane detection with One-to-Several Transformer. (arXiv:2305.00675v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00675](http://arxiv.org/abs/2305.00675)
* Code URL: [https://github.com/zkyseu/O2SFormer](https://github.com/zkyseu/O2SFormer)
* Copy Paste: `<input type="checkbox">[[2305.00675] End to End Lane detection with One-to-Several Transformer](http://arxiv.org/abs/2305.00675) #transformer`
* Summary: <p>Although lane detection methods have shown impressive performance in
real-world scenarios, most of methods require post-processing which is not
robust enough. Therefore, end-to-end detectors like DEtection TRansformer(DETR)
have been introduced in lane detection. However, one-to-one label assignment in
DETR can degrade the training efficiency due to label semantic conflicts.
Besides, positional query in DETR is unable to provide explicit positional
prior, making it difficult to be optimized. In this paper, we present the
One-to-Several Transformer(O2SFormer). We first propose the one-to-several
label assignment, which combines one-to-one and one-to-many label assignments
to improve the training efficiency while keeping end-to-end detection. To
overcome the difficulty in optimizing one-to-one assignment. We further propose
the layer-wise soft label which adjusts the positive weight of positive lane
anchors across different decoder layers. Finally, we design the dynamic
anchor-based positional query to explore positional prior by incorporating lane
anchors into positional query. Experimental results show that O2SFormer
significantly speeds up the convergence of DETR and outperforms
Transformer-based and CNN-based detectors on the CULane dataset. Code will be
available athttps://github.com/zkyseu/O2SFormer.
</p>

### Title: What Do Self-Supervised Vision Transformers Learn?. (arXiv:2305.00729v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00729](http://arxiv.org/abs/2305.00729)
* Code URL: [https://github.com/naver-ai/cl-vs-mim](https://github.com/naver-ai/cl-vs-mim)
* Copy Paste: `<input type="checkbox">[[2305.00729] What Do Self-Supervised Vision Transformers Learn?](http://arxiv.org/abs/2305.00729) #transformer`
* Summary: <p>We present a comparative study on how and why contrastive learning (CL) and
masked image modeling (MIM) differ in their representations and in their
performance of downstream tasks. In particular, we demonstrate that
self-supervised Vision Transformers (ViTs) have the following properties: (1)
CL trains self-attentions to capture longer-range global patterns than MIM,
such as the shape of an object, especially in the later layers of the ViT
architecture. This CL property helps ViTs linearly separate images in their
representation spaces. However, it also makes the self-attentions collapse into
homogeneity for all query tokens and heads. Such homogeneity of self-attention
reduces the diversity of representations, worsening scalability and dense
prediction performance. (2) CL utilizes the low-frequency signals of the
representations, but MIM utilizes high-frequencies. Since low- and
high-frequency information respectively represent shapes and textures, CL is
more shape-oriented and MIM more texture-oriented. (3) CL plays a crucial role
in the later layers, while MIM mainly focuses on the early layers. Upon these
analyses, we find that CL and MIM can complement each other and observe that
even the simplest harmonization can help leverage the advantages of both
methods. The code is available at https://github.com/naver-ai/cl-vs-mim.
</p>

### Title: RViDeformer: Efficient Raw Video Denoising Transformer with a Larger Benchmark Dataset. (arXiv:2305.00767v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00767](http://arxiv.org/abs/2305.00767)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00767] RViDeformer: Efficient Raw Video Denoising Transformer with a Larger Benchmark Dataset](http://arxiv.org/abs/2305.00767) #transformer`
* Summary: <p>In recent years, raw video denoising has garnered increased attention due to
the consistency with the imaging process and well-studied noise modeling in the
raw domain. However, two problems still hinder the denoising performance.
Firstly, there is no large dataset with realistic motions for supervised raw
video denoising, as capturing noisy and clean frames for real dynamic scenes is
difficult. To address this, we propose recapturing existing high-resolution
videos displayed on a 4K screen with high-low ISO settings to construct
noisy-clean paired frames. In this way, we construct a video denoising dataset
(named as ReCRVD) with 120 groups of noisy-clean videos, whose ISO values
ranging from 1600 to 25600. Secondly, while non-local temporal-spatial
attention is beneficial for denoising, it often leads to heavy computation
costs. We propose an efficient raw video denoising transformer network
(RViDeformer) that explores both short and long-distance correlations.
Specifically, we propose multi-branch spatial and temporal attention modules,
which explore the patch correlations from local window, local low-resolution
window, global downsampled window, and neighbor-involved window, and then they
are fused together. We employ reparameterization to reduce computation costs.
Our network is trained in both supervised and unsupervised manners, achieving
the best performance compared with state-of-the-art methods. Additionally, the
model trained with our proposed dataset (ReCRVD) outperforms the model trained
with previous benchmark dataset (CRVD) when evaluated on the real-world outdoor
noisy videos. Our code and dataset will be released after the acceptance of
this work.
</p>

### Title: Scaling Pareto-Efficient Decision Making Via Offline Multi-Objective RL. (arXiv:2305.00567v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.00567](http://arxiv.org/abs/2305.00567)
* Code URL: [https://github.com/baitingzbt/peda](https://github.com/baitingzbt/peda)
* Copy Paste: `<input type="checkbox">[[2305.00567] Scaling Pareto-Efficient Decision Making Via Offline Multi-Objective RL](http://arxiv.org/abs/2305.00567) #transformer`
* Summary: <p>The goal of multi-objective reinforcement learning (MORL) is to learn
policies that simultaneously optimize multiple competing objectives. In
practice, an agent's preferences over the objectives may not be known apriori,
and hence, we require policies that can generalize to arbitrary preferences at
test time. In this work, we propose a new data-driven setup for offline MORL,
where we wish to learn a preference-agnostic policy agent using only a finite
dataset of offline demonstrations of other agents and their preferences. The
key contributions of this work are two-fold. First, we introduce D4MORL,
(D)atasets for MORL that are specifically designed for offline settings. It
contains 1.8 million annotated demonstrations obtained by rolling out reference
policies that optimize for randomly sampled preferences on 6 MuJoCo
environments with 2-3 objectives each. Second, we propose Pareto-Efficient
Decision Agents (PEDA), a family of offline MORL algorithms that builds and
extends Decision Transformers via a novel preference-and-return-conditioned
policy. Empirically, we show that PEDA closely approximates the behavioral
policy on the D4MORL benchmark and provides an excellent approximation of the
Pareto-front with appropriate conditioning, as measured by the hypervolume and
sparsity metrics.
</p>

### Title: Dynamic Transfer Learning across Graphs. (arXiv:2305.00664v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.00664](http://arxiv.org/abs/2305.00664)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00664] Dynamic Transfer Learning across Graphs](http://arxiv.org/abs/2305.00664) #transformer`
* Summary: <p>Transferring knowledge across graphs plays a pivotal role in many high-stake
domains, ranging from transportation networks to e-commerce networks, from
neuroscience to finance. To date, the vast majority of existing works assume
both source and target domains are sampled from a universal and stationary
distribution. However, many real-world systems are intrinsically dynamic, where
the underlying domains are evolving over time. To bridge the gap, we propose to
shift the problem to the dynamic setting and ask: given the label-rich source
graphs and the label-scarce target graphs observed in previous T timestamps,
how can we effectively characterize the evolving domain discrepancy and
optimize the generalization performance of the target domain at the incoming
T+1 timestamp? To answer the question, for the first time, we propose a
generalization bound under the setting of dynamic transfer learning across
graphs, which implies the generalization performance is dominated by domain
evolution and domain discrepancy between source and target domains. Inspired by
the theoretical results, we propose a novel generic framework DyTrans to
improve knowledge transferability across dynamic graphs. In particular, we
start with a transformer-based temporal encoding module to model temporal
information of the evolving domains; then, we further design a dynamic domain
unification module to efficiently learn domain-invariant representations across
the source and target domains. Finally, extensive experiments on various
real-world datasets demonstrate the effectiveness of DyTrans in transferring
knowledge from dynamic source domains to dynamic target domains.
</p>

## generative
### Title: Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features. (arXiv:2305.00067v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00067](http://arxiv.org/abs/2305.00067)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00067] Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features](http://arxiv.org/abs/2305.00067) #generative`
* Summary: <p>Inspired by recent findings that generative diffusion models learn
semantically meaningful representations, we use them to discover the intrinsic
hierarchical structure in biomedical 3D images using unsupervised segmentation.
We show that features of diffusion models from different stages of a
U-Net-based ladder-like architecture capture different hierarchy levels in 3D
biomedical images. We design three losses to train a predictive unsupervised
segmentation network that encourages the decomposition of 3D volumes into
meaningful nested subvolumes that represent a hierarchy. First, we pretrain 3D
diffusion models and use the consistency of their features across subvolumes.
Second, we use the visual consistency between subvolumes. Third, we use the
invariance to photometric augmentations as a regularizer. Our models achieve
better performance than prior unsupervised structure discovery approaches on
challenging biologically-inspired synthetic datasets and on a real-world brain
tumor MRI dataset.
</p>

### Title: Learning Locally Editable Virtual Humans. (arXiv:2305.00121v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00121](http://arxiv.org/abs/2305.00121)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00121] Learning Locally Editable Virtual Humans](http://arxiv.org/abs/2305.00121) #generative`
* Summary: <p>In this paper, we propose a novel hybrid representation and end-to-end
trainable network architecture to model fully editable and customizable neural
avatars. At the core of our work lies a representation that combines the
modeling power of neural fields with the ease of use and inherent 3D
consistency of skinned meshes. To this end, we construct a trainable feature
codebook to store local geometry and texture features on the vertices of a
deformable body model, thus exploiting its consistent topology under
articulation. This representation is then employed in a generative auto-decoder
architecture that admits fitting to unseen scans and sampling of realistic
avatars with varied appearances and geometries. Furthermore, our representation
allows local editing by swapping local features between 3D assets. To verify
our method for avatar creation and editing, we contribute a new high-quality
dataset, dubbed CustomHumans, for training and evaluation. Our experiments
quantitatively and qualitatively show that our method generates diverse
detailed avatars and achieves better model fitting performance compared to
state-of-the-art methods. Our code and dataset are available at
https://custom-humans.github.io/.
</p>

### Title: LD-GAN: Low-Dimensional Generative Adversarial Network for Spectral Image Generation with Variance Regularization. (arXiv:2305.00132v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00132](http://arxiv.org/abs/2305.00132)
* Code URL: [https://github.com/hdspgroup/LD-GAN](https://github.com/hdspgroup/LD-GAN)
* Copy Paste: `<input type="checkbox">[[2305.00132] LD-GAN: Low-Dimensional Generative Adversarial Network for Spectral Image Generation with Variance Regularization](http://arxiv.org/abs/2305.00132) #generative`
* Summary: <p>Deep learning methods are state-of-the-art for spectral image (SI)
computational tasks. However, these methods are constrained in their
performance since available datasets are limited due to the highly expensive
and long acquisition time. Usually, data augmentation techniques are employed
to mitigate the lack of data. Surpassing classical augmentation methods, such
as geometric transformations, GANs enable diverse augmentation by learning and
sampling from the data distribution. Nevertheless, GAN-based SI generation is
challenging since the high-dimensionality nature of this kind of data hinders
the convergence of the GAN training yielding to suboptimal generation. To
surmount this limitation, we propose low-dimensional GAN (LD-GAN), where we
train the GAN employing a low-dimensional representation of the {dataset} with
the latent space of a pretrained autoencoder network. Thus, we generate new
low-dimensional samples which are then mapped to the SI dimension with the
pretrained decoder network. Besides, we propose a statistical regularization to
control the low-dimensional representation variance for the autoencoder
training and to achieve high diversity of samples generated with the GAN. We
validate our method LD-GAN as data augmentation strategy for compressive
spectral imaging, SI super-resolution, and RBG to spectral tasks with
improvements varying from 0.5 to 1 [dB] in each task respectively. We perform
comparisons against the non-data augmentation training, traditional DA, and
with the same GAN adjusted and trained to generate the full-sized SIs. The code
of this paper can be found in https://github.com/romanjacome99/LD_GAN.git
</p>

### Title: Identity-driven Three-Player Generative Adversarial Network for Synthetic-based Face Recognition. (arXiv:2305.00358v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00358](http://arxiv.org/abs/2305.00358)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00358] Identity-driven Three-Player Generative Adversarial Network for Synthetic-based Face Recognition](http://arxiv.org/abs/2305.00358) #generative`
* Summary: <p>Many of the commonly used datasets for face recognition development are
collected from the internet without proper user consent. Due to the increasing
focus on privacy in the social and legal frameworks, the use and distribution
of these datasets are being restricted and strongly questioned. These
databases, which have a realistically high variability of data per identity,
have enabled the success of face recognition models. To build on this success
and to align with privacy concerns, synthetic databases, consisting purely of
synthetic persons, are increasingly being created and used in the development
of face recognition solutions. In this work, we present a three-player
generative adversarial network (GAN) framework, namely IDnet, that enables the
integration of identity information into the generation process. The third
player in our IDnet aims at forcing the generator to learn to generate
identity-separable face images. We empirically proved that our IDnet synthetic
images are of higher identity discrimination in comparison to the conventional
two-player GAN, while maintaining a realistic intra-identity variation. We
further studied the identity link between the authentic identities used to
train the generator and the generated synthetic identities, showing very low
similarities between these identities. We demonstrated the applicability of our
IDnet data in training face recognition models by evaluating these models on a
wide set of face recognition benchmarks. In comparison to the state-of-the-art
works in synthetic-based face recognition, our solution achieved comparable
results to a recent rendering-based approach and outperformed all existing
GAN-based approaches. The training code and the synthetic face image dataset
are publicly available ( https://github.com/fdbtrs/Synthetic-Face-Recognition
).
</p>

### Title: SLSG: Industrial Image Anomaly Detection by Learning Better Feature Embeddings and One-Class Classification. (arXiv:2305.00398v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00398](http://arxiv.org/abs/2305.00398)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00398] SLSG: Industrial Image Anomaly Detection by Learning Better Feature Embeddings and One-Class Classification](http://arxiv.org/abs/2305.00398) #generative`
* Summary: <p>Industrial image anomaly detection under the setting of one-class
classification has significant practical value. However, most existing models
struggle to extract separable feature representations when performing feature
embedding and struggle to build compact descriptions of normal features when
performing one-class classification. One direct consequence of this is that
most models perform poorly in detecting logical anomalies which violate
contextual relationships. Focusing on more effective and comprehensive anomaly
detection, we propose a network based on self-supervised learning and
self-attentive graph convolution (SLSG) for anomaly detection. SLSG uses a
generative pre-training network to assist the encoder in learning the embedding
of normal patterns and the reasoning of position relationships. Subsequently,
SLSG introduces the pseudo-prior knowledge of anomaly through simulated
abnormal samples. By comparing the simulated anomalies, SLSG can better
summarize the normal features and narrow down the hypersphere used for
one-class classification. In addition, with the construction of a more general
graph structure, SLSG comprehensively models the dense and sparse relationships
among elements in the image, which further strengthens the detection of logical
anomalies. Extensive experiments on benchmark datasets show that SLSG achieves
superior anomaly detection performance, demonstrating the effectiveness of our
method.
</p>

### Title: StyleLipSync: Style-based Personalized Lip-sync Video Generation. (arXiv:2305.00521v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00521](http://arxiv.org/abs/2305.00521)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00521] StyleLipSync: Style-based Personalized Lip-sync Video Generation](http://arxiv.org/abs/2305.00521) #generative`
* Summary: <p>In this paper, we present StyleLipSync, a style-based personalized lip-sync
video generative model that can generate identity-agnostic lip-synchronizing
video from arbitrary audio. To generate a video of arbitrary identities, we
leverage expressive lip prior from the semantically rich latent space of a
pre-trained StyleGAN, where we can also design a video consistency with a
linear transformation. In contrast to the previous lip-sync methods, we
introduce pose-aware masking that dynamically locates the mask to improve the
naturalness over frames by utilizing a 3D parametric mesh predictor frame by
frame. Moreover, we propose a few-shot lip-sync adaptation method for an
arbitrary person by introducing a sync regularizer that preserves lips-sync
generalization while enhancing the person-specific visual information.
Extensive experiments demonstrate that our model can generate accurate lip-sync
videos even with the zero-shot setting and enhance characteristics of an unseen
face using a few seconds of target video through the proposed adaptation
method. Please refer to our project page.
</p>

### Title: StyleGenes: Discrete and Efficient Latent Distributions for GANs. (arXiv:2305.00599v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00599](http://arxiv.org/abs/2305.00599)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00599] StyleGenes: Discrete and Efficient Latent Distributions for GANs](http://arxiv.org/abs/2305.00599) #generative`
* Summary: <p>We propose a discrete latent distribution for Generative Adversarial Networks
(GANs). Instead of drawing latent vectors from a continuous prior, we sample
from a finite set of learnable latents. However, a direct parametrization of
such a distribution leads to an intractable linear increase in memory in order
to ensure sufficient sample diversity. We address this key issue by taking
inspiration from the encoding of information in biological organisms. Instead
of learning a separate latent vector for each sample, we split the latent space
into a set of genes. For each gene, we train a small bank of gene variants.
Thus, by independently sampling a variant for each gene and combining them into
the final latent vector, our approach can represent a vast number of unique
latent samples from a compact set of learnable parameters. Interestingly, our
gene-inspired latent encoding allows for new and intuitive approaches to
latent-space exploration, enabling conditional sampling from our
unconditionally trained model. Moreover, our approach preserves
state-of-the-art photo-realism while achieving better disentanglement than the
widely-used StyleMapping network.
</p>

### Title: Boosting Weakly-Supervised Temporal Action Localization with Text Information. (arXiv:2305.00607v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00607](http://arxiv.org/abs/2305.00607)
* Code URL: [https://github.com/lgzlilili/boosting-wtal](https://github.com/lgzlilili/boosting-wtal)
* Copy Paste: `<input type="checkbox">[[2305.00607] Boosting Weakly-Supervised Temporal Action Localization with Text Information](http://arxiv.org/abs/2305.00607) #generative`
* Summary: <p>Due to the lack of temporal annotation, current Weakly-supervised Temporal
Action Localization (WTAL) methods are generally stuck into over-complete or
incomplete localization. In this paper, we aim to leverage the text information
to boost WTAL from two aspects, i.e., (a) the discriminative objective to
enlarge the inter-class difference, thus reducing the over-complete; (b) the
generative objective to enhance the intra-class integrity, thus finding more
complete temporal boundaries. For the discriminative objective, we propose a
Text-Segment Mining (TSM) mechanism, which constructs a text description based
on the action class label, and regards the text as the query to mine all
class-related segments. Without the temporal annotation of actions, TSM
compares the text query with the entire videos across the dataset to mine the
best matching segments while ignoring irrelevant ones. Due to the shared
sub-actions in different categories of videos, merely applying TSM is too
strict to neglect the semantic-related segments, which results in incomplete
localization. We further introduce a generative objective named Video-text
Language Completion (VLC), which focuses on all semantic-related segments from
videos to complete the text sentence. We achieve the state-of-the-art
performance on THUMOS14 and ActivityNet1.3. Surprisingly, we also find our
proposed method can be seamlessly applied to existing methods, and improve
their performances with a clear margin. The code is available at
https://github.com/lgzlIlIlI/Boosting-WTAL.
</p>

### Title: ShipHullGAN: A generic parametric modeller for ship hull design using deep convolutional generative model. (arXiv:2305.00210v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.00210](http://arxiv.org/abs/2305.00210)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00210] ShipHullGAN: A generic parametric modeller for ship hull design using deep convolutional generative model](http://arxiv.org/abs/2305.00210) #generative`
* Summary: <p>In this work, we introduce ShipHullGAN, a generic parametric modeller built
using deep convolutional generative adversarial networks (GANs) for the
versatile representation and generation of ship hulls. At a high level, the new
model intends to address the current conservatism in the parametric ship design
paradigm, where parametric modellers can only handle a particular ship type. We
trained ShipHullGAN on a large dataset of 52,591 \textit{physically validated}
designs from a wide range of existing ship types, including container ships,
tankers, bulk carriers, tugboats, and crew supply vessels. We developed a new
shape extraction and representation strategy to convert all training designs
into a common geometric representation of the same resolution, as typically
GANs can only accept vectors of fixed dimension as input. A space-filling layer
is placed right after the generator component to ensure that the trained
generator can cover all design classes. During training, designs are provided
in the form of a shape-signature tensor (SST) which harnesses the compact
geometric representation using geometric moments that further enable the
inexpensive incorporation of physics-informed elements in ship design. We have
shown through extensive comparative studies and optimisation cases that
ShipHullGAN can generate designs with augmented features resulting in versatile
design spaces that produce traditional and novel designs with geometrically
valid and practically feasible shapes.
</p>

## label correction
## noise
### Title: Relaxed forced choice improves performance of visual quality assessment methods. (arXiv:2305.00220v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00220](http://arxiv.org/abs/2305.00220)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00220] Relaxed forced choice improves performance of visual quality assessment methods](http://arxiv.org/abs/2305.00220) #noise`
* Summary: <p>In image quality assessment, a collective visual quality score for an image
or video is obtained from the individual ratings of many subjects. One commonly
used format for these experiments is the two-alternative forced choice method.
Two stimuli with the same content but differing visual quality are presented
sequentially or side-by-side. Subjects are asked to select the one of better
quality, and when uncertain, they are required to guess. The relaxed
alternative forced choice format aims to reduce the cognitive load and the
noise in the responses due to the guessing by providing a third response
option, namely, ``not sure''. This work presents a large and comprehensive
crowdsourcing experiment to compare these two response formats: the one with
the ``not sure'' option and the one without it. To provide unambiguous ground
truth for quality evaluation, subjects were shown pairs of images with
differing numbers of dots and asked each time to choose the one with more dots.
Our crowdsourcing study involved 254 participants and was conducted using a
within-subject design. Each participant was asked to respond to 40 pair
comparisons with and without the ``not sure'' response option and completed a
questionnaire to evaluate their cognitive load for each testing condition. The
experimental results show that the inclusion of the ``not sure'' response
option in the forced choice method reduced mental load and led to models with
better data fit and correspondence to ground truth. We also tested for the
equivalence of the models and found that they were different. The dataset is
available at <a href="http://database.mmsp-kn.de/cogvqa-database.html.">this http URL</a>
</p>

### Title: Fusion for Visual-Infrared Person ReID in Real-World Surveillance Using Corrupted Multimodal Data. (arXiv:2305.00320v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00320](http://arxiv.org/abs/2305.00320)
* Code URL: [https://github.com/art2611/mreid-ucd-ccd](https://github.com/art2611/mreid-ucd-ccd)
* Copy Paste: `<input type="checkbox">[[2305.00320] Fusion for Visual-Infrared Person ReID in Real-World Surveillance Using Corrupted Multimodal Data](http://arxiv.org/abs/2305.00320) #noise`
* Summary: <p>Visible-infrared person re-identification (V-I ReID) seeks to match images of
individuals captured over a distributed network of RGB and IR cameras. The task
is challenging due to the significant differences between V and I modalities,
especially under real-world conditions, where images are corrupted by, e.g,
blur, noise, and weather. Indeed, state-of-art V-I ReID models cannot leverage
corrupted modality information to sustain a high level of accuracy. In this
paper, we propose an efficient model for multimodal V-I ReID -- named
Multimodal Middle Stream Fusion (MMSF) -- that preserves modality-specific
knowledge for improved robustness to corrupted multimodal images. In addition,
three state-of-art attention-based multimodal fusion models are adapted to
address corrupted multimodal data in V-I ReID, allowing to dynamically balance
each modality importance. Recently, evaluation protocols have been proposed to
assess the robustness of ReID models under challenging real-world scenarios.
However, these protocols are limited to unimodal V settings. For realistic
evaluation of multimodal (and cross-modal) V-I person ReID models, we propose
new challenging corrupted datasets for scenarios where V and I cameras are
co-located (CL) and not co-located (NCL). Finally, the benefits of our Masking
and Local Multimodal Data Augmentation (ML-MDA) strategy are explored to
improve the robustness of ReID models to multimodal corruption. Our experiments
on clean and corrupted versions of the SYSU-MM01, RegDB, and ThermalWORLD
datasets indicate the multimodal V-I ReID models that are more likely to
perform well in real-world operational conditions. In particular, our ML-MDA is
an important strategy for a V-I person ReID system to sustain high accuracy and
robustness when processing corrupted multimodal images. Also, our multimodal
ReID model MMSF outperforms every method under CL and NCL camera scenarios.
</p>

### Title: Joint tone mapping and denoising of thermal infrared images via multi-scale Retinex and multi-task learning. (arXiv:2305.00691v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00691](http://arxiv.org/abs/2305.00691)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00691] Joint tone mapping and denoising of thermal infrared images via multi-scale Retinex and multi-task learning](http://arxiv.org/abs/2305.00691) #noise`
* Summary: <p>Cameras digitize real-world scenes as pixel intensity values with a limited
value range given by the available bits per pixel (bpp). High Dynamic Range
(HDR) cameras capture those luminance values in higher resolution through an
increase in the number of bpp. Most displays, however, are limited to 8 bpp.
Naive HDR compression methods lead to a loss of the rich information contained
in those HDR images. In this paper, tone mapping algorithms for thermal
infrared images with 16 bpp are investigated that can preserve this
information. An optimized multi-scale Retinex algorithm sets the baseline. This
algorithm is then approximated with a deep learning approach based on the
popular U-Net architecture. The remaining noise in the images after tone
mapping is reduced implicitly by utilizing a self-supervised deep learning
approach that can be jointly trained with the tone mapping approach in a
multi-task learning scheme. Further discussions are provided on denoising and
deflickering for thermal infrared video enhancement in the context of tone
mapping. Extensive experiments on the public FLIR ADAS Dataset prove the
effectiveness of our proposed method in comparison with the state-of-the-art.
</p>

## diffusion
### Title: Class-Balancing Diffusion Models. (arXiv:2305.00562v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00562](http://arxiv.org/abs/2305.00562)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00562] Class-Balancing Diffusion Models](http://arxiv.org/abs/2305.00562) #diffusion`
* Summary: <p>Diffusion-based models have shown the merits of generating high-quality
visual data while preserving better diversity in recent studies. However, such
observation is only justified with curated data distribution, where the data
samples are nicely pre-processed to be uniformly distributed in terms of their
labels. In practice, a long-tailed data distribution appears more common and
how diffusion models perform on such class-imbalanced data remains unknown. In
this work, we first investigate this problem and observe significant
degradation in both diversity and fidelity when the diffusion model is trained
on datasets with class-imbalanced distributions. Especially in tail classes,
the generations largely lose diversity and we observe severe mode-collapse
issues. To tackle this problem, we set from the hypothesis that the data
distribution is not class-balanced, and propose Class-Balancing Diffusion
Models (CBDM) that are trained with a distribution adjustment regularizer as a
solution. Experiments show that images generated by CBDM exhibit higher
diversity and quality in both quantitative and qualitative ways. Our method
benchmarked the generation results on CIFAR100/CIFAR100LT dataset and shows
outstanding performance on the downstream recognition task.
</p>

### Title: Temporal Subsampling Diminishes Small Spatial Scales in Recurrent Neural Network Emulators of Geophysical Turbulence. (arXiv:2305.00100v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.00100](http://arxiv.org/abs/2305.00100)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00100] Temporal Subsampling Diminishes Small Spatial Scales in Recurrent Neural Network Emulators of Geophysical Turbulence](http://arxiv.org/abs/2305.00100) #diffusion`
* Summary: <p>The immense computational cost of traditional numerical weather and climate
models has sparked the development of machine learning (ML) based emulators.
Because ML methods benefit from long records of training data, it is common to
use datasets that are temporally subsampled relative to the time steps required
for the numerical integration of differential equations. Here, we investigate
how this often overlooked processing step affects the quality of an emulator's
predictions. We implement two ML architectures from a class of methods called
reservoir computing: (1) a form of Nonlinear Vector Autoregression (NVAR), and
(2) an Echo State Network (ESN). Despite their simplicity, it is well
documented that these architectures excel at predicting low dimensional chaotic
dynamics. We are therefore motivated to test these architectures in an
idealized setting of predicting high dimensional geophysical turbulence as
represented by Surface Quasi-Geostrophic dynamics. In all cases, subsampling
the training data consistently leads to an increased bias at small spatial
scales that resembles numerical diffusion. Interestingly, the NVAR architecture
becomes unstable when the temporal resolution is increased, indicating that the
polynomial based interactions are insufficient at capturing the detailed
nonlinearities of the turbulent flow. The ESN architecture is found to be more
robust, suggesting a benefit to the more expensive but more general structure.
Spectral errors are reduced by including a penalty on the kinetic energy
density spectrum during training, although the subsampling related errors
persist. Future work is warranted to understand how the temporal resolution of
training data affects other ML architectures.
</p>

### Title: Diffusion Models for Time Series Applications: A Survey. (arXiv:2305.00624v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.00624](http://arxiv.org/abs/2305.00624)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00624] Diffusion Models for Time Series Applications: A Survey](http://arxiv.org/abs/2305.00624) #diffusion`
* Summary: <p>Diffusion models, a family of generative models based on deep learning, have
become increasingly prominent in cutting-edge machine learning research. With a
distinguished performance in generating samples that resemble the observed
data, diffusion models are widely used in image, video, and text synthesis
nowadays. In recent years, the concept of diffusion has been extended to time
series applications, and many powerful models have been developed. Considering
the deficiency of a methodical summary and discourse on these models, we
provide this survey as an elementary resource for new researchers in this area
and also an inspiration to motivate future research. For better understanding,
we include an introduction about the basics of diffusion models. Except for
this, we primarily focus on diffusion-based methods for time series
forecasting, imputation, and generation, and present them respectively in three
individual sections. We also compare different methods for the same application
and highlight their connections if applicable. Lastly, we conclude the common
limitation of diffusion-based methods and highlight potential future research
directions.
</p>

## LLM
## segmentation
### Title: SAM on Medical Images: A Comprehensive Study on Three Prompt Modes. (arXiv:2305.00035v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00035](http://arxiv.org/abs/2305.00035)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00035] SAM on Medical Images: A Comprehensive Study on Three Prompt Modes](http://arxiv.org/abs/2305.00035) #segmentation`
* Summary: <p>The Segment Anything Model (SAM) made an eye-catching debut recently and
inspired many researchers to explore its potential and limitation in terms of
zero-shot generalization capability. As the first promptable foundation model
for segmentation tasks, it was trained on a large dataset with an unprecedented
number of images and annotations. This large-scale dataset and its promptable
nature endow the model with strong zero-shot generalization. Although the SAM
has shown competitive performance on several datasets, we still want to
investigate its zero-shot generalization on medical images. As we know, the
acquisition of medical image annotation usually requires a lot of effort from
professional practitioners. Therefore, if there exists a foundation model that
can give high-quality mask prediction simply based on a few point prompts, this
model will undoubtedly become the game changer for medical image analysis. To
evaluate whether SAM has the potential to become the foundation model for
medical image segmentation tasks, we collected more than 12 public medical
image datasets that cover various organs and modalities. We also explore what
kind of prompt can lead to the best zero-shot performance with different
modalities. Furthermore, we find that a pattern shows that the perturbation of
the box size will significantly change the prediction accuracy. Finally,
Extensive experiments show that the predicted mask quality varied a lot among
different datasets. And providing proper prompts, such as bounding boxes, to
the SAM will significantly increase its performance.
</p>

### Title: Exploring the Zero-Shot Capabilities of the Segment Anything Model (SAM) in 2D Medical Imaging: A Comprehensive Evaluation and Practical Guideline. (arXiv:2305.00109v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00109](http://arxiv.org/abs/2305.00109)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00109] Exploring the Zero-Shot Capabilities of the Segment Anything Model (SAM) in 2D Medical Imaging: A Comprehensive Evaluation and Practical Guideline](http://arxiv.org/abs/2305.00109) #segmentation`
* Summary: <p>Segmentation in medical imaging plays a crucial role in diagnosing,
monitoring, and treating various diseases and conditions. The current landscape
of segmentation in the medical domain is dominated by numerous specialized deep
learning models fine-tuned for each segmentation task and image modality.
Recently, the Segment Anything Model (SAM), a new segmentation model, was
introduced. SAM utilizes the ViT neural architecture and leverages a vast
training dataset to segment almost any object. However, its generalizability to
the medical domain remains unexplored. In this study, we assess the zero-shot
capabilities of SAM 2D in medical imaging using eight different prompt
strategies across six datasets from four imaging modalities: X-ray, ultrasound,
dermatoscopy, and colonoscopy. Our results demonstrate that SAM's zero-shot
performance is comparable and, in certain cases, superior to the current
state-of-the-art. Based on our findings, we propose a practical guideline that
requires minimal interaction and yields robust results in all evaluated
contexts.
</p>

### Title: DSEC-MOS: Segment Any Moving Object with Moving Ego Vehicle. (arXiv:2305.00126v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00126](http://arxiv.org/abs/2305.00126)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00126] DSEC-MOS: Segment Any Moving Object with Moving Ego Vehicle](http://arxiv.org/abs/2305.00126) #segmentation`
* Summary: <p>Moving Object Segmentation (MOS), a crucial task in computer vision, has
numerous applications such as surveillance, autonomous driving, and video
analytics. Existing datasets for moving object segmentation mainly focus on RGB
or Lidar videos, but lack additional event information that can enhance the
understanding of dynamic scenes. To address this limitation, we propose a novel
dataset, called DSEC-MOS. Our dataset includes frames captured by RGB cameras
embedded on moving vehicules and incorporates event data, which provide high
temporal resolution and low-latency information about changes in the scenes. To
generate accurate segmentation mask annotations for moving objects, we apply
the recently emerged large model SAM - Segment Anything Model - with moving
object bounding boxes from DSEC-MOD serving as prompts and calibrated RGB
frames, then further revise the results. Our DSEC-MOS dataset contains in total
16 sequences (13314 images). To the best of our knowledge, DSEC-MOS is also the
first moving object segmentation dataset that includes event camera in
autonomous driving. Project Page: https://github.com/ZZY-Zhou/DSEC-MOS.
</p>

### Title: Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints. (arXiv:2305.00131v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00131](http://arxiv.org/abs/2305.00131)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00131] Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints](http://arxiv.org/abs/2305.00131) #segmentation`
* Summary: <p>Self-training based on pseudo-labels has emerged as a dominant approach for
addressing conditional distribution shifts in unsupervised domain adaptation
(UDA) for semantic segmentation problems. A notable drawback, however, is that
this family of approaches is susceptible to erroneous pseudo labels that arise
from confirmation biases in the source domain and that manifest as nuisance
factors in the target domain. A possible source for this mismatch is the
reliance on only photometric cues provided by RGB image inputs, which may
ultimately lead to sub-optimal adaptation. To mitigate the effect of mismatched
pseudo-labels, we propose to incorporate structural cues from auxiliary
modalities, such as depth, to regularise conventional self-training objectives.
Specifically, we introduce a contrastive pixel-level objectness constraint that
pulls the pixel representations within a region of an object instance closer,
while pushing those from different object categories apart. To obtain object
regions consistent with the true underlying object, we extract information from
both depth maps and RGB-images in the form of multimodal clustering. Crucially,
the objectness constraint is agnostic to the ground-truth semantic labels and,
hence, appropriate for unsupervised domain adaptation. In this work, we show
that our regularizer significantly improves top performing self-training
methods (by up to $2$ points) in various UDA benchmarks for semantic
segmentation. We include all code in the supplementary.
</p>

### Title: Sensor Equivariance by LiDAR Projection Images. (arXiv:2305.00221v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00221](http://arxiv.org/abs/2305.00221)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00221] Sensor Equivariance by LiDAR Projection Images](http://arxiv.org/abs/2305.00221) #segmentation`
* Summary: <p>In this work, we propose an extension of conventional image data by an
additional channel in which the associated projection properties are encoded.
This addresses the issue of sensor-dependent object representation in
projection-based sensors, such as LiDAR, which can lead to distorted physical
and geometric properties due to variations in sensor resolution and field of
view. To that end, we propose an architecture for processing this data in an
instance segmentation framework. We focus specifically on LiDAR as a key sensor
modality for machine vision tasks and highly automated driving (HAD). Through
an experimental setup in a controlled synthetic environment, we identify a bias
on sensor resolution and field of view and demonstrate that our proposed method
can reduce said bias for the task of LiDAR instance segmentation. Furthermore,
we define our method such that it can be applied to other projection-based
sensors, such as cameras. To promote transparency, we make our code and dataset
publicly available. This method shows the potential to improve performance and
robustness in various machine vision tasks that utilize projection-based
sensors.
</p>

### Title: A Critical Analysis of the Limitation of Deep Learning based 3D Dental Mesh Segmentation Methods in Segmenting Partial Scans. (arXiv:2305.00244v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00244](http://arxiv.org/abs/2305.00244)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00244] A Critical Analysis of the Limitation of Deep Learning based 3D Dental Mesh Segmentation Methods in Segmenting Partial Scans](http://arxiv.org/abs/2305.00244) #segmentation`
* Summary: <p>Tooth segmentation from intraoral scans is a crucial part of digital
dentistry. Many Deep Learning based tooth segmentation algorithms have been
developed for this task. In most of the cases, high accuracy has been achieved,
although, most of the available tooth segmentation techniques make an implicit
restrictive assumption of full jaw model and they report accuracy based on full
jaw models. Medically, however, in certain cases, full jaw tooth scan is not
required or may not be available. Given this practical issue, it is important
to understand the robustness of currently available widely used Deep Learning
based tooth segmentation techniques. For this purpose, we applied available
segmentation techniques on partial intraoral scans and we discovered that the
available deep Learning techniques under-perform drastically. The analysis and
comparison presented in this work would help us in understanding the severity
of the problem and allow us to develop robust tooth segmentation technique
without strong assumption of full jaw model.
</p>

### Title: Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected. (arXiv:2305.00278v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00278](http://arxiv.org/abs/2305.00278)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00278] Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected](http://arxiv.org/abs/2305.00278) #segmentation`
* Summary: <p>Meta AI Research has recently released SAM (Segment Anything Model) which is
trained on a large segmentation dataset of over 1 billion masks. As a
foundation model in the field of computer vision, SAM (Segment Anything Model)
has gained attention for its impressive performance in generic object
segmentation. Despite its strong capability in a wide range of zero-shot
transfer tasks, it remains unknown whether SAM can detect things in challenging
setups like transparent objects. In this work, we perform an empirical
evaluation of two glass-related challenging scenarios: mirror and transparent
objects. We found that SAM often fails to detect the glass in both scenarios,
which raises concern for deploying the SAM in safety-critical situations that
have various forms of glass.
</p>

### Title: Optimized Machine Learning for CHD Detection using 3D CNN-based Segmentation, Transfer Learning and Adagrad Optimization. (arXiv:2305.00411v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00411](http://arxiv.org/abs/2305.00411)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00411] Optimized Machine Learning for CHD Detection using 3D CNN-based Segmentation, Transfer Learning and Adagrad Optimization](http://arxiv.org/abs/2305.00411) #segmentation`
* Summary: <p>Globally, Coronary Heart Disease (CHD) is one of the main causes of death.
Early detection of CHD can improve patient outcomes and reduce mortality rates.
We propose a novel framework for predicting the presence of CHD using a
combination of machine learning and image processing techniques. The framework
comprises various phases, including analyzing the data, feature selection using
ReliefF, 3D CNN-based segmentation, feature extraction by means of transfer
learning, feature fusion as well as classification, and Adagrad optimization.
The first step of the proposed framework involves analyzing the data to
identify patterns and correlations that may be indicative of CHD. Next, ReliefF
feature selection is applied to decide on the most relevant features from the
sample images. The 3D CNN-based segmentation technique is then used to segment
the optic disc and macula, which are important regions for CHD diagnosis.
Feature extraction using transfer learning is performed to extract features
from the segmented regions of interest. The extracted features are then fused
using a feature fusion technique, and a classifier is trained to predict the
presence of CHD. Finally, Adagrad optimization is used to optimize the
performance of the classifier. Our framework is evaluated on a dataset of
sample images collected from patients with and without CHD. The results show
that the anticipated framework accomplishes elevated accuracy in predicting the
presence of CHD. either a particular user with a reasonable degree of accuracy
compared to the previously employed classifiers like SVM, etc.
</p>

### Title: Synthetic Data-based Detection of Zebras in Drone Imagery. (arXiv:2305.00432v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00432](http://arxiv.org/abs/2305.00432)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00432] Synthetic Data-based Detection of Zebras in Drone Imagery](http://arxiv.org/abs/2305.00432) #segmentation`
* Summary: <p>Datasets that allow the training of common objects or human detectors are
widely available. These come in the form of labelled real-world images and
require either a significant amount of human effort, with a high probability of
errors such as missing labels, or very constrained scenarios, e.g. VICON
systems. Likewise, uncommon scenarios, like aerial views, animals, like wild
zebras, or difficult-to-obtain information as human shapes, are hardly
available. To overcome this, usage of synthetic data generation with realistic
rendering technologies has recently gained traction and advanced tasks like
target tracking and human pose estimation. However, subjects such as wild
animals are still usually not well represented in such datasets. In this work,
we first show that a pre-trained YOLO detector can not identify zebras in real
images recorded from aerial viewpoints. To solve this, we present an approach
for training an animal detector using only synthetic data. We start by
generating a novel synthetic zebra dataset using GRADE, a state-of-the-art
framework for data generation. The dataset includes RGB, depth, skeletal joint
locations, pose, shape and instance segmentations for each subject. We use this
to train a YOLO detector from scratch. Through extensive evaluations of our
model with real-world data from i) limited datasets available on the internet
and ii) a new one collected and manually labelled by us, we show that we can
detect zebras by using only synthetic data during training. The code, results,
trained models, and both the generated and training data are provided as
open-source at https://keeper.mpdl.mpg.de/d/12abb3bb6b12491480d5/.
</p>

### Title: PRSeg: A Lightweight Patch Rotate MLP Decoder for Semantic Segmentation. (arXiv:2305.00671v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00671](http://arxiv.org/abs/2305.00671)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00671] PRSeg: A Lightweight Patch Rotate MLP Decoder for Semantic Segmentation](http://arxiv.org/abs/2305.00671) #segmentation`
* Summary: <p>The lightweight MLP-based decoder has become increasingly promising for
semantic segmentation. However, the channel-wise MLP cannot expand the
receptive fields, lacking the context modeling capacity, which is critical to
semantic segmentation. In this paper, we propose a parametric-free patch rotate
operation to reorganize the pixels spatially. It first divides the feature map
into multiple groups and then rotates the patches within each group. Based on
the proposed patch rotate operation, we design a novel segmentation network,
named PRSeg, which includes an off-the-shelf backbone and a lightweight Patch
Rotate MLP decoder containing multiple Dynamic Patch Rotate Blocks
(DPR-Blocks). In each DPR-Block, the fully connected layer is performed
following a Patch Rotate Module (PRM) to exchange spatial information between
pixels. Specifically, in PRM, the feature map is first split into the reserved
part and rotated part along the channel dimension according to the predicted
probability of the Dynamic Channel Selection Module (DCSM), and our proposed
patch rotate operation is only performed on the rotated part. Extensive
experiments on ADE20K, Cityscapes and COCO-Stuff 10K datasets prove the
effectiveness of our approach. We expect that our PRSeg can promote the
development of MLP-based decoder in semantic segmentation.
</p>

### Title: Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation. (arXiv:2305.00673v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00673](http://arxiv.org/abs/2305.00673)
* Code URL: [https://github.com/deepmed-lab-ecnu/bcp](https://github.com/deepmed-lab-ecnu/bcp)
* Copy Paste: `<input type="checkbox">[[2305.00673] Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation](http://arxiv.org/abs/2305.00673) #segmentation`
* Summary: <p>In semi-supervised medical image segmentation, there exist empirical mismatch
problems between labeled and unlabeled data distribution. The knowledge learned
from the labeled data may be largely discarded if treating labeled and
unlabeled data separately or in an inconsistent manner. We propose a
straightforward method for alleviating the problem - copy-pasting labeled and
unlabeled data bidirectionally, in a simple Mean Teacher architecture. The
method encourages unlabeled data to learn comprehensive common semantics from
the labeled data in both inward and outward directions. More importantly, the
consistent learning procedure for labeled and unlabeled data can largely reduce
the empirical distribution gap. In detail, we copy-paste a random crop from a
labeled image (foreground) onto an unlabeled image (background) and an
unlabeled image (foreground) onto a labeled image (background), respectively.
The two mixed images are fed into a Student network and supervised by the mixed
supervisory signals of pseudo-labels and ground-truth. We reveal that the
simple mechanism of copy-pasting bidirectionally between labeled and unlabeled
data is good enough and the experiments show solid gains (e.g., over 21% Dice
improvement on ACDC dataset with 5% labeled data) compared with other
state-of-the-arts on various semi-supervised medical image segmentation
datasets. Code is available at https://github.com/DeepMed-Lab-ECNU/BCP}.
</p>

### Title: Rethinking Boundary Detection in Deep Learning Models for Medical Image Segmentation. (arXiv:2305.00678v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00678](http://arxiv.org/abs/2305.00678)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00678] Rethinking Boundary Detection in Deep Learning Models for Medical Image Segmentation](http://arxiv.org/abs/2305.00678) #segmentation`
* Summary: <p>Medical image segmentation is a fundamental task in the community of medical
image analysis. In this paper, a novel network architecture, referred to as
Convolution, Transformer, and Operator (CTO), is proposed. CTO employs a
combination of Convolutional Neural Networks (CNNs), Vision Transformer (ViT),
and an explicit boundary detection operator to achieve high recognition
accuracy while maintaining an optimal balance between accuracy and efficiency.
The proposed CTO follows the standard encoder-decoder segmentation paradigm,
where the encoder network incorporates a popular CNN backbone for capturing
local semantic information, and a lightweight ViT assistant for integrating
long-range dependencies. To enhance the learning capacity on boundary, a
boundary-guided decoder network is proposed that uses a boundary mask obtained
from a dedicated boundary detection operator as explicit supervision to guide
the decoding learning process. The performance of the proposed method is
evaluated on six challenging medical image segmentation datasets, demonstrating
that CTO achieves state-of-the-art accuracy with a competitive model
complexity.
</p>

## object detection
### Title: Wearing face mask detection using deep learning through COVID-19 pandemic. (arXiv:2305.00068v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00068](http://arxiv.org/abs/2305.00068)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00068] Wearing face mask detection using deep learning through COVID-19 pandemic](http://arxiv.org/abs/2305.00068) #object detection`
* Summary: <p>During the COVID-19 pandemic, wearing a face mask has been known to be an
effective way to prevent the spread of COVID-19. In lots of monitoring tasks,
humans have been replaced with computers thanks to the outstanding performance
of the deep learning models. Monitoring the wearing of a face mask is another
task that can be done by deep learning models with acceptable accuracy. The
main challenge of this task is the limited amount of data because of the
quarantine. In this paper, we did an investigation on the capability of three
state-of-the-art object detection neural networks on face mask detection for
real-time applications. As mentioned, here are three models used, Single Shot
Detector (SSD), two versions of You Only Look Once (YOLO) i.e., YOLOv4-tiny,
and YOLOv4-tiny-3l from which the best was selected. In the proposed method,
according to the performance of different models, the best model that can be
suitable for use in real-world and mobile device applications in comparison to
other recent studies was the YOLOv4-tiny model, with 85.31% and 50.66 for mean
Average Precision (mAP) and Frames Per Second (FPS), respectively. These
acceptable values were achieved using two datasets with only 1531 images in
three separate classes.
</p>

### Title: Exploiting the Distortion-Semantic Interaction in Fisheye Data. (arXiv:2305.00079v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00079](http://arxiv.org/abs/2305.00079)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00079] Exploiting the Distortion-Semantic Interaction in Fisheye Data](http://arxiv.org/abs/2305.00079) #object detection`
* Summary: <p>In this work, we present a methodology to shape a fisheye-specific
representation space that reflects the interaction between distortion and
semantic context present in this data modality. Fisheye data has the wider
field of view advantage over other types of cameras, but this comes at the
expense of high radial distortion. As a result, objects further from the center
exhibit deformations that make it difficult for a model to identify their
semantic context. While previous work has attempted architectural and training
augmentation changes to alleviate this effect, no work has attempted to guide
the model towards learning a representation space that reflects this
interaction between distortion and semantic context inherent to fisheye data.
We introduce an approach to exploit this relationship by first extracting
distortion class labels based on an object's distance from the center of the
image. We then shape a backbone's representation space with a weighted
contrastive loss that constrains objects of the same semantic class and
distortion class to be close to each other within a lower dimensional embedding
space. This backbone trained with both semantic and distortion information is
then fine-tuned within an object detection setting to empirically evaluate the
quality of the learnt representation. We show this method leads to performance
improvements by as much as 1.1% mean average precision over standard object
detection strategies and .6% improvement over other state of the art
representation learning approaches.
</p>

### Title: InfraDet3D: Multi-Modal 3D Object Detection based on Roadside Infrastructure Camera and LiDAR Sensors. (arXiv:2305.00314v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00314](http://arxiv.org/abs/2305.00314)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00314] InfraDet3D: Multi-Modal 3D Object Detection based on Roadside Infrastructure Camera and LiDAR Sensors](http://arxiv.org/abs/2305.00314) #object detection`
* Summary: <p>Current multi-modal object detection approaches focus on the vehicle domain
and are limited in the perception range and the processing capabilities.
Roadside sensor units (RSUs) introduce a new domain for perception systems and
leverage altitude to observe traffic. Cameras and LiDARs mounted on gantry
bridges increase the perception range and produce a full digital twin of the
traffic. In this work, we introduce InfraDet3D, a multi-modal 3D object
detector for roadside infrastructure sensors. We fuse two LiDARs using early
fusion and further incorporate detections from monocular cameras to increase
the robustness and to detect small objects. Our monocular 3D detection module
uses HD maps to ground object yaw hypotheses, improving the final perception
results. The perception framework is deployed on a real-world intersection that
is part of the A9 Test Stretch in Munich, Germany. We perform several ablation
studies and experiments and show that fusing two LiDARs with two cameras leads
to an improvement of +1.90 mAP compared to a camera-only solution. We evaluate
our results on the A9 infrastructure dataset and achieve 68.48 mAP on the test
set. The dataset and code will be available at https://a9-dataset.com to allow
the research community to further improve the perception results and make
autonomous driving safer.
</p>

### Title: A Simulation-Augmented Benchmarking Framework for Automatic RSO Streak Detection in Single-Frame Space Images. (arXiv:2305.00412v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00412](http://arxiv.org/abs/2305.00412)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00412] A Simulation-Augmented Benchmarking Framework for Automatic RSO Streak Detection in Single-Frame Space Images](http://arxiv.org/abs/2305.00412) #object detection`
* Summary: <p>Detecting Resident Space Objects (RSOs) and preventing collisions with other
satellites is crucial. Recently, deep convolutional neural networks (DCNNs)
have shown superior performance in object detection when large-scale datasets
are available. However, collecting rich data of RSOs is difficult due to very
few occurrences in the space images. Without sufficient data, it is challenging
to comprehensively train DCNN detectors and make them effective for detecting
RSOs in space images, let alone to estimate whether a detector is sufficiently
robust. The lack of meaningful evaluation of different detectors could further
affect the design and application of detection methods. To tackle this issue,
we propose that the space images containing RSOs can be simulated to complement
the shortage of raw data for better benchmarking. Accordingly, we introduce a
novel simulation-augmented benchmarking framework for RSO detection (SAB-RSOD).
In our framework, by making the best use of the hardware parameters of the
sensor that captures real-world space images, we first develop a high-fidelity
RSO simulator that can generate various realistic space images. Then, we use
this simulator to generate images that contain diversified RSOs in space and
annotate them automatically. Later, we mix the synthetic images with the
real-world images, obtaining around 500 images for training with only the
real-world images for evaluation. Under SAB-RSOD, we can train different
popular object detectors like Yolo and Faster RCNN effectively, enabling us to
evaluate their performance thoroughly. The evaluation results have shown that
the amount of available data and image resolution are two key factors for
robust RSO detection. Moreover, if using a lower resolution for higher
efficiency, we demonstrated that a simple UNet-based detection method can
already access high detection accuracy.
</p>

### Title: The MCC approaches the geometric mean of precision and recall as true negatives approach infinity. (arXiv:2305.00594v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00594](http://arxiv.org/abs/2305.00594)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00594] The MCC approaches the geometric mean of precision and recall as true negatives approach infinity](http://arxiv.org/abs/2305.00594) #object detection`
* Summary: <p>The performance of a binary classifier is described by a confusion matrix
with four entries: the number of true positives (TP), true negatives (TN),
false positives (FP), and false negatives (FN).
</p>
<p>The Matthew's Correlation Coefficient (MCC), F1, and Fowlkes--Mallows (FM)
scores are scalars that summarize a confusion matrix. Both the F1 and FM scores
are based on only three of the four entries in the confusion matrix (they
ignore TN). In contrast, the MCC takes into account all four entries of the
confusion matrix and thus can be seen as providing a more representative
picture.
</p>
<p>However, in object detection problems, measuring the number of true negatives
is so large it is often intractable. Thus we ask, what happens to the MCC as
the number of true negatives approaches infinity? This paper provides insight
into the relationship between the MCC and FM score by proving that the
FM-measure is equal to the limit of the MCC as the number of true negatives
approaches infinity.
</p>

### Title: Refined Response Distillation for Class-Incremental Player Detection. (arXiv:2305.00620v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00620](http://arxiv.org/abs/2305.00620)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00620] Refined Response Distillation for Class-Incremental Player Detection](http://arxiv.org/abs/2305.00620) #object detection`
* Summary: <p>Detecting players from sports broadcast videos is essential for intelligent
event analysis. However, existing methods assume fixed player categories,
incapably accommodating the real-world scenarios where categories continue to
evolve. Directly fine-tuning these methods on newly emerging categories also
exist the catastrophic forgetting due to the non-stationary distribution.
Inspired by recent research on incremental object detection (IOD), we propose a
Refined Response Distillation (R^2D) method to effectively mitigate
catastrophic forgetting for IOD tasks of the players. Firstly, we design a
progressive coarse-to-fine distillation region dividing scheme, separating
high-value and low-value regions from classification and regression responses
for precise and fine-grained regional knowledge distillation. Subsequently, a
tailored refined distillation strategy is developed on regions with varying
significance to address the performance limitations posed by pronounced feature
homogeneity in the IOD tasks of the players. Furthermore, we present the
NBA-IOD and Volleyball-IOD datasets as the benchmark and investigate the IOD
tasks of the players systematically. Extensive experiments conducted on
benchmarks demonstrate that our method achieves state-of-the-art results.The
code and datasets are available at https://github.com/beiyan1911/Players-IOD.
</p>

