## data-free
## transformer
### Title: PU-EdgeFormer: Edge Transformer for Dense Prediction in Point Cloud Upsampling. (arXiv:2305.01148v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01148](http://arxiv.org/abs/2305.01148)
* Code URL: [https://github.com/dohoon2045/pu-edgeformer](https://github.com/dohoon2045/pu-edgeformer)
* Copy Paste: `<input type="checkbox">[[2305.01148] PU-EdgeFormer: Edge Transformer for Dense Prediction in Point Cloud Upsampling](http://arxiv.org/abs/2305.01148) #transformer`
* Summary: <p>Despite the recent development of deep learning-based point cloud upsampling,
most MLP-based point cloud upsampling methods have limitations in that it is
difficult to train the local and global structure of the point cloud at the
same time. To solve this problem, we present a combined graph convolution and
transformer for point cloud upsampling, denoted by PU-EdgeFormer. The proposed
method constructs EdgeFormer unit that consists of graph convolution and
multi-head self-attention modules. We employ graph convolution using EdgeConv,
which learns the local geometry and global structure of point cloud better than
existing point-to-feature method. Through in-depth experiments, we confirmed
that the proposed method has better point cloud upsampling performance than the
existing state-of-the-art method in both subjective and objective aspects. The
code is available at https://github.com/dohoon2045/PU-EdgeFormer.
</p>

### Title: Exploring vision transformer layer choosing for semantic segmentation. (arXiv:2305.01279v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01279](http://arxiv.org/abs/2305.01279)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01279] Exploring vision transformer layer choosing for semantic segmentation](http://arxiv.org/abs/2305.01279) #transformer`
* Summary: <p>Extensive work has demonstrated the effectiveness of Vision Transformers. The
plain Vision Transformer tends to obtain multi-scale features by selecting
fixed layers, or the last layer of features aiming to achieve higher
performance in dense prediction tasks. However, this selection is often based
on manual operation. And different samples often exhibit different features at
different layers (e.g., edge, structure, texture, detail, etc.). This requires
us to seek a dynamic adaptive fusion method to filter different layer features.
In this paper, unlike previous encoder and decoder work, we design a neck
network for adaptive fusion and feature selection, called ViTController. We
validate the effectiveness of our method on different datasets and models and
surpass previous state-of-the-art methods. Finally, our method can also be used
as a plug-in module and inserted into different networks.
</p>

### Title: AxWin Transformer: A Context-Aware Vision Transformer Backbone with Axial Windows. (arXiv:2305.01280v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01280](http://arxiv.org/abs/2305.01280)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01280] AxWin Transformer: A Context-Aware Vision Transformer Backbone with Axial Windows](http://arxiv.org/abs/2305.01280) #transformer`
* Summary: <p>Recently Transformer has shown good performance in several vision tasks due
to its powerful modeling capabilities. To reduce the quadratic complexity
caused by the attention, some outstanding work restricts attention to local
regions or extends axial interactions. However, these methos often lack the
interaction of local and global information, balancing coarse and fine-grained
information. To address this problem, we propose AxWin Attention, which models
context information in both local windows and axial views. Based on the AxWin
Attention, we develop a context-aware vision transformer backbone, named AxWin
Transformer, which outperforming the state-of-the-art methods in both
classification and downstream segmentation and detection tasks.
</p>

### Title: Scalable Mask Annotation for Video Text Spotting. (arXiv:2305.01443v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01443](http://arxiv.org/abs/2305.01443)
* Code URL: [https://github.com/vitae-transformer/samtext](https://github.com/vitae-transformer/samtext)
* Copy Paste: `<input type="checkbox">[[2305.01443] Scalable Mask Annotation for Video Text Spotting](http://arxiv.org/abs/2305.01443) #transformer`
* Summary: <p>Video text spotting refers to localizing, recognizing, and tracking textual
elements such as captions, logos, license plates, signs, and other forms of
text within consecutive video frames. However, current datasets available for
this task rely on quadrilateral ground truth annotations, which may result in
including excessive background content and inaccurate text boundaries.
Furthermore, methods trained on these datasets often produce prediction results
in the form of quadrilateral boxes, which limits their ability to handle
complex scenarios such as dense or curved text. To address these issues, we
propose a scalable mask annotation pipeline called SAMText for video text
spotting. SAMText leverages the SAM model to generate mask annotations for
scene text images or video frames at scale. Using SAMText, we have created a
large-scale dataset, SAMText-9M, that contains over 2,400 video clips sourced
from existing datasets and over 9 million mask annotations. We have also
conducted a thorough statistical analysis of the generated masks and their
quality, identifying several research topics that could be further explored
based on this dataset. The code and dataset will be released at
\url{https://github.com/ViTAE-Transformer/SAMText}.
</p>

### Title: ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning. (arXiv:2305.01486v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01486](http://arxiv.org/abs/2305.01486)
* Code URL: [https://github.com/takihasan/arbex](https://github.com/takihasan/arbex)
* Copy Paste: `<input type="checkbox">[[2305.01486] ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning](http://arxiv.org/abs/2305.01486) #transformer`
* Summary: <p>In this paper, we introduce a framework ARBEx, a novel attentive feature
extraction framework driven by Vision Transformer with reliability balancing to
cope against poor class distributions, bias, and uncertainty in the facial
expression learning (FEL) task. We reinforce several data pre-processing and
refinement methods along with a window-based cross-attention ViT to squeeze the
best of the data. We also employ learnable anchor points in the embedding space
with label distributions and multi-head self-attention mechanism to optimize
performance against weak predictions with reliability balancing, which is a
strategy that leverages anchor points, attention scores, and confidence values
to enhance the resilience of label predictions. To ensure correct label
classification and improve the models' discriminative power, we introduce
anchor loss, which encourages large margins between anchor points.
Additionally, the multi-head self-attention mechanism, which is also trainable,
plays an integral role in identifying accurate labels. This approach provides
critical elements for improving the reliability of predictions and has a
substantial positive effect on final prediction capabilities. Our adaptive
model can be integrated with any deep neural network to forestall challenges in
various recognition tasks. Our strategy outperforms current state-of-the-art
methodologies, according to extensive experiments conducted in a variety of
contexts.
</p>

### Title: Sequence Modeling with Multiresolution Convolutional Memory. (arXiv:2305.01638v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.01638](http://arxiv.org/abs/2305.01638)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01638] Sequence Modeling with Multiresolution Convolutional Memory](http://arxiv.org/abs/2305.01638) #transformer`
* Summary: <p>Efficiently capturing the long-range patterns in sequential data sources
salient to a given task -- such as classification and generative modeling --
poses a fundamental challenge. Popular approaches in the space tradeoff between
the memory burden of brute-force enumeration and comparison, as in
transformers, the computational burden of complicated sequential dependencies,
as in recurrent neural networks, or the parameter burden of convolutional
networks with many or large filters. We instead take inspiration from
wavelet-based multiresolution analysis to define a new building block for
sequence modeling, which we call a MultiresLayer. The key component of our
model is the multiresolution convolution, capturing multiscale trends in the
input sequence. Our MultiresConv can be implemented with shared filters across
a dilated causal convolution tree. Thus it garners the computational advantages
of convolutional networks and the principled theoretical motivation of wavelet
decompositions. Our MultiresLayer is straightforward to implement, requires
significantly fewer parameters, and maintains at most a $\mathcal{O}(N\log N)$
memory footprint for a length $N$ sequence. Yet, by stacking such layers, our
model yields state-of-the-art performance on a number of sequence
classification and autoregressive density estimation tasks using CIFAR-10,
ListOps, and PTB-XL datasets.
</p>

### Title: Company classification using zero-shot learning. (arXiv:2305.01028v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.01028](http://arxiv.org/abs/2305.01028)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01028] Company classification using zero-shot learning](http://arxiv.org/abs/2305.01028) #transformer`
* Summary: <p>In recent years, natural language processing (NLP) has become increasingly
important in a variety of business applications, including sentiment analysis,
text classification, and named entity recognition. In this paper, we propose an
approach for company classification using NLP and zero-shot learning. Our
method utilizes pre-trained transformer models to extract features from company
descriptions, and then applies zero-shot learning to classify companies into
relevant categories without the need for specific training data for each
category. We evaluate our approach on publicly available datasets of textual
descriptions of companies, and demonstrate that it can streamline the process
of company classification, thereby reducing the time and resources required in
traditional approaches such as the Global Industry Classification Standard
(GICS). The results show that this method has potential for automation of
company classification, making it a promising avenue for future research in
this area.
</p>

### Title: ADVISE: AI-accelerated Design of Evidence Synthesis for Global Development. (arXiv:2305.01145v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.01145](http://arxiv.org/abs/2305.01145)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01145] ADVISE: AI-accelerated Design of Evidence Synthesis for Global Development](http://arxiv.org/abs/2305.01145) #transformer`
* Summary: <p>When designing evidence-based policies and programs, decision-makers must
distill key information from a vast and rapidly growing literature base.
Identifying relevant literature from raw search results is time and resource
intensive, and is often done by manual screening. In this study, we develop an
AI agent based on a bidirectional encoder representations from transformers
(BERT) model and incorporate it into a human team designing an evidence
synthesis product for global development. We explore the effectiveness of the
human-AI hybrid team in accelerating the evidence synthesis process. To further
improve team efficiency, we enhance the human-AI hybrid team through active
learning (AL). Specifically, we explore different sampling strategies,
including random sampling, least confidence (LC) sampling, and highest priority
(HP) sampling, to study their influence on the collaborative screening process.
Results show that incorporating the BERT-based AI agent into the human team can
reduce the human screening effort by 68.5% compared to the case of no AI
assistance and by 16.8% compared to the case of using a support vector machine
(SVM)-based AI agent for identifying 80% of all relevant documents. When we
apply the HP sampling strategy for AL, the human screening effort can be
reduced even more: by 78.3% for identifying 80% of all relevant documents
compared to no AI assistance. We apply the AL-enhanced human-AI hybrid teaming
workflow in the design process of three evidence gap maps (EGMs) for USAID and
find it to be highly effective. These findings demonstrate how AI can
accelerate the development of evidence synthesis products and promote timely
evidence-based decision making in global development in a human-AI hybrid
teaming context.
</p>

### Title: MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset. (arXiv:2305.01211v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.01211](http://arxiv.org/abs/2305.01211)
* Code URL: [https://github.com/tobiasbrugger/multilegalsbd](https://github.com/tobiasbrugger/multilegalsbd)
* Copy Paste: `<input type="checkbox">[[2305.01211] MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset](http://arxiv.org/abs/2305.01211) #transformer`
* Summary: <p>Sentence Boundary Detection (SBD) is one of the foundational building blocks
of Natural Language Processing (NLP), with incorrectly split sentences heavily
influencing the output quality of downstream tasks. It is a challenging task
for algorithms, especially in the legal domain, considering the complex and
different sentence structures used. In this work, we curated a diverse
multilingual legal dataset consisting of over 130'000 annotated sentences in 6
languages. Our experimental results indicate that the performance of existing
SBD models is subpar on multilingual legal data. We trained and tested
monolingual and multilingual models based on CRF, BiLSTM-CRF, and transformers,
demonstrating state-of-the-art performance. We also show that our multilingual
models outperform all baselines in the zero-shot setting on a Portuguese test
set. To encourage further research and development by the community, we have
made our dataset, models, and code publicly available.
</p>

### Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input. (arXiv:2305.01625v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.01625](http://arxiv.org/abs/2305.01625)
* Code URL: [https://github.com/abertsch72/unlimiformer](https://github.com/abertsch72/unlimiformer)
* Copy Paste: `<input type="checkbox">[[2305.01625] Unlimiformer: Long-Range Transformers with Unlimited Length Input](http://arxiv.org/abs/2305.01625) #transformer`
* Summary: <p>Transformer-based models typically have a predefined bound to their input
length, because of their need to potentially attend to every token in the
input. In this work, we propose Unlimiformer: a general approach that can wrap
any existing pretrained encoder-decoder transformer, and offload the attention
computation across all layers to a single $k$-nearest-neighbor index; this
index can be kept on either the GPU or CPU memory and queried in sub-linear
time. This way, we can index extremely long input sequences, while every
attention head in every decoder layer retrieves its top-$k$ keys, instead of
attending to every key. We demonstrate Unlimiformers's efficacy on several
long-document and multi-document summarization benchmarks, showing that it can
summarize even 350k token-long inputs from the BookSum dataset, without any
input truncation at test time. Unlimiformer improves pretrained models such as
BART and Longformer by extending them to unlimited inputs without additional
learned weights and without modifying their code. We make our code and models
publicly available at https://github.com/abertsch72/unlimiformer .
</p>

### Title: Unlocking the Power of Representations in Long-term Novelty-based Exploration. (arXiv:2305.01521v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.01521](http://arxiv.org/abs/2305.01521)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01521] Unlocking the Power of Representations in Long-term Novelty-based Exploration](http://arxiv.org/abs/2305.01521) #transformer`
* Summary: <p>We introduce Robust Exploration via Clustering-based Online Density
Estimation (RECODE), a non-parametric method for novelty-based exploration that
estimates visitation counts for clusters of states based on their similarity in
a chosen embedding space. By adapting classical clustering to the nonstationary
setting of Deep RL, RECODE can efficiently track state visitation counts over
thousands of episodes. We further propose a novel generalization of the inverse
dynamics loss, which leverages masked transformer architectures for multi-step
prediction; which in conjunction with RECODE achieves a new state-of-the-art in
a suite of challenging 3D-exploration tasks in DM-Hard-8. RECODE also sets new
state-of-the-art in hard exploration Atari games, and is the first agent to
reach the end screen in "Pitfall!".
</p>

## generative
### Title: Learning Structured Output Representations from Attributes using Deep Conditional Generative Models. (arXiv:2305.00980v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00980](http://arxiv.org/abs/2305.00980)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00980] Learning Structured Output Representations from Attributes using Deep Conditional Generative Models](http://arxiv.org/abs/2305.00980) #generative`
* Summary: <p>Structured output representation is a generative task explored in computer
vision that often times requires the mapping of low dimensional features to
high dimensional structured outputs. Losses in complex spatial information in
deterministic approaches such as Convolutional Neural Networks (CNN) lead to
uncertainties and ambiguous structures within a single output representation. A
probabilistic approach through deep Conditional Generative Models (CGM) is
presented by Sohn et al. in which a particular model known as the Conditional
Variational Auto-encoder (CVAE) is introduced and explored. While the original
paper focuses on the task of image segmentation, this paper adopts the CVAE
framework for the task of controlled output representation through attributes.
This approach allows us to learn a disentangled multimodal prior distribution,
resulting in more controlled and robust approach to sample generation. In this
work we recreate the CVAE architecture and train it on images conditioned on
various attributes obtained from two image datasets; the Large-scale CelebFaces
Attributes (CelebA) dataset and the Caltech-UCSD Birds (CUB-200-2011) dataset.
We attempt to generate new faces with distinct attributes such as hair color
and glasses, as well as different bird species samples with various attributes.
We further introduce strategies for improving generalized sample generation by
applying a weighted term to the variational lower bound.
</p>

### Title: Synthetic Data for Face Recognition: Current State and Future Prospects. (arXiv:2305.01021v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01021](http://arxiv.org/abs/2305.01021)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01021] Synthetic Data for Face Recognition: Current State and Future Prospects](http://arxiv.org/abs/2305.01021) #generative`
* Summary: <p>Over the past years, deep learning capabilities and the availability of
large-scale training datasets advanced rapidly, leading to breakthroughs in
face recognition accuracy. However, these technologies are foreseen to face a
major challenge in the next years due to the legal and ethical concerns about
using authentic biometric data in AI model training and evaluation along with
increasingly utilizing data-hungry state-of-the-art deep learning models. With
the recent advances in deep generative models and their success in generating
realistic and high-resolution synthetic image data, privacy-friendly synthetic
data has been recently proposed as an alternative to privacy-sensitive
authentic data to overcome the challenges of using authentic data in face
recognition development. This work aims at providing a clear and structured
picture of the use-cases taxonomy of synthetic face data in face recognition
along with the recent emerging advances of face recognition models developed on
the bases of synthetic data. We also discuss the challenges facing the use of
synthetic data in face recognition development and several future prospects of
synthetic data in the domain of face recognition.
</p>

### Title: AutoColor: Learned Light Power Control for Multi-Color Holograms. (arXiv:2305.01611v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01611](http://arxiv.org/abs/2305.01611)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01611] AutoColor: Learned Light Power Control for Multi-Color Holograms](http://arxiv.org/abs/2305.01611) #generative`
* Summary: <p>Multi-color holograms rely on simultaneous illumination from multiple light
sources. These multi-color holograms could utilize light sources better than
conventional single-color holograms and can improve the dynamic range of
holographic displays. In this letter, we introduce \projectname, the first
learned method for estimating the optimal light source powers required for
illuminating multi-color holograms. For this purpose, we establish the first
multi-color hologram dataset using synthetic images and their depth
information. We generate these synthetic images using a trending pipeline
combining generative, large language, and monocular depth estimation models.
Finally, we train our learned model using our dataset and experimentally
demonstrate that \projectname significantly decreases the number of steps
required to optimize multi-color holograms from $&gt;1000$ to $70$ iteration steps
without compromising image quality.
</p>

### Title: Generalizing Dataset Distillation via Deep Generative Prior. (arXiv:2305.01649v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01649](http://arxiv.org/abs/2305.01649)
* Code URL: [https://github.com/georgecazenavette/glad](https://github.com/georgecazenavette/glad)
* Copy Paste: `<input type="checkbox">[[2305.01649] Generalizing Dataset Distillation via Deep Generative Prior](http://arxiv.org/abs/2305.01649) #generative`
* Summary: <p>Dataset Distillation aims to distill an entire dataset's knowledge into a few
synthetic images. The idea is to synthesize a small number of synthetic data
points that, when given to a learning algorithm as training data, result in a
model approximating one trained on the original data. Despite recent progress
in the field, existing dataset distillation methods fail to generalize to new
architectures and scale to high-resolution datasets. To overcome the above
issues, we propose to use the learned prior from pre-trained deep generative
models to synthesize the distilled data. To achieve this, we present a new
optimization algorithm that distills a large number of images into a few
intermediate feature vectors in the generative model's latent space. Our method
augments existing techniques, significantly improving cross-architecture
generalization in all settings.
</p>

### Title: Humans as Light Bulbs: 3D Human Reconstruction from Thermal Reflection. (arXiv:2305.01652v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01652](http://arxiv.org/abs/2305.01652)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01652] Humans as Light Bulbs: 3D Human Reconstruction from Thermal Reflection](http://arxiv.org/abs/2305.01652) #generative`
* Summary: <p>The relatively hot temperature of the human body causes people to turn into
long-wave infrared light sources. Since this emitted light has a larger
wavelength than visible light, many surfaces in typical scenes act as infrared
mirrors with strong specular reflections. We exploit the thermal reflections of
a person onto objects in order to locate their position and reconstruct their
pose, even if they are not visible to a normal camera. We propose an
analysis-by-synthesis framework that jointly models the objects, people, and
their thermal reflections, which allows us to combine generative models with
differentiable rendering of reflections. Quantitative and qualitative
experiments show our approach works in highly challenging cases, such as with
curved mirrors or when the person is completely unseen by a normal camera.
</p>

### Title: The Role of Summarization in Generative Agents: A Preliminary Perspective. (arXiv:2305.01253v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.01253](http://arxiv.org/abs/2305.01253)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01253] The Role of Summarization in Generative Agents: A Preliminary Perspective](http://arxiv.org/abs/2305.01253) #generative`
* Summary: <p>Generative agents that simulate human society show tremendous potential for
further research and practical applications. Specifically, the generative agent
architecture comprising several meticulously designed modules constitutes the
most critical component. To facilitate progress in this research, this report
presents our integrated perspective on comprehending generative agents through
summarization, since we believe summarization is the most fundamental and
indispensable capacity of generative agents manifested across diverse
scenarios. We hope this report can provide insight into understanding the
importance of summarization capacity in generative agents and motivate future
research.
</p>

### Title: Turning Flowchart into Dialog: Plan-based Data Augmentation for Low-Resource Flowchart-grounded Troubleshooting Dialogs. (arXiv:2305.01323v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.01323](http://arxiv.org/abs/2305.01323)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01323] Turning Flowchart into Dialog: Plan-based Data Augmentation for Low-Resource Flowchart-grounded Troubleshooting Dialogs](http://arxiv.org/abs/2305.01323) #generative`
* Summary: <p>Flowchart-grounded troubleshooting dialogue (FTD) systems, which follow the
instructions of a flowchart to diagnose users' problems in specific domains
(eg., vehicle, laptop), have been gaining research interest in recent years.
However, collecting sufficient dialogues that are naturally grounded on
flowcharts is costly, thus FTD systems are impeded by scarce training data. To
mitigate the data sparsity issue, we propose a plan-based data augmentation
(PlanDA) approach that generates diverse synthetic dialog data at scale by
transforming concise flowchart into dialogues. Specifically, its generative
model employs a variational-base framework with a hierarchical planning
strategy that includes global and local latent planning variables. Experiments
on the FloDial dataset show that synthetic dialogue produced by PlanDA improves
the performance of downstream tasks, including flowchart path retrieval and
response generation, in particular on the Out-of-Flowchart settings. In
addition, further analysis demonstrate the quality of synthetic data generated
by PlanDA in paths that are covered by current sample dialogues and paths that
are not covered.
</p>

### Title: Basic syntax from speech: Spontaneous concatenation in unsupervised deep neural networks. (arXiv:2305.01626v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.01626](http://arxiv.org/abs/2305.01626)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01626] Basic syntax from speech: Spontaneous concatenation in unsupervised deep neural networks](http://arxiv.org/abs/2305.01626) #generative`
* Summary: <p>Computational models of syntax are predominantly text-based. Here we propose
that basic syntax can be modeled directly from raw speech in a fully
unsupervised way. We focus on one of the most ubiquitous and basic properties
of syntax -- concatenation. We introduce spontaneous concatenation: a
phenomenon where convolutional neural networks (CNNs) trained on acoustic
recordings of individual words start generating outputs with two or even three
words concatenated without ever accessing data with multiple words in the
input. Additionally, networks trained on two words learn to embed words into
novel unobserved word combinations. To our knowledge, this is a previously
unreported property of CNNs trained on raw speech in the Generative Adversarial
Network setting and has implications both for our understanding of how these
architectures learn as well as for modeling syntax and its evolution from raw
acoustic inputs.
</p>

### Title: The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers. (arXiv:2305.01628v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.01628](http://arxiv.org/abs/2305.01628)
* Code URL: [https://github.com/ibm/auto-contrastive-generation](https://github.com/ibm/auto-contrastive-generation)
* Copy Paste: `<input type="checkbox">[[2305.01628] The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers](http://arxiv.org/abs/2305.01628) #generative`
* Summary: <p>Applying language models to natural language processing tasks typically
relies on the representations in the final model layer, as intermediate hidden
layer representations are presumed to be less informative. In this work, we
argue that due to the gradual improvement across model layers, additional
information can be gleaned from the contrast between higher and lower layers
during inference. Specifically, in choosing between the probable next token
predictions of a generative model, the predictions of lower layers can be used
to highlight which candidates are best avoided. We propose a novel approach
that utilizes the contrast between layers to improve text generation outputs,
and show that it mitigates degenerative behaviors of the model in open-ended
generation, significantly improving the quality of generated texts.
Furthermore, our results indicate that contrasting between model layers at
inference time can yield substantial benefits to certain aspects of general
language model capabilities, more effectively extracting knowledge during
inference from a given set of model parameters.
</p>

### Title: On the use of Deep Generative Models for Perfect Prognosis Climate Downscaling. (arXiv:2305.00974v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.00974](http://arxiv.org/abs/2305.00974)
* Code URL: [https://github.com/jgonzalezab/cvae-pp-downscaling](https://github.com/jgonzalezab/cvae-pp-downscaling)
* Copy Paste: `<input type="checkbox">[[2305.00974] On the use of Deep Generative Models for Perfect Prognosis Climate Downscaling](http://arxiv.org/abs/2305.00974) #generative`
* Summary: <p>Deep Learning has recently emerged as a perfect prognosis downscaling
technique to compute high-resolution fields from large-scale coarse atmospheric
data. Despite their promising results to reproduce the observed local
variability, they are based on the estimation of independent distributions at
each location, which leads to deficient spatial structures, especially when
downscaling precipitation. This study proposes the use of generative models to
improve the spatial consistency of the high-resolution fields, very demanded by
some sectoral applications (e.g., hydrology) to tackle climate change.
</p>

### Title: Computing Expected Motif Counts for Exchangeable Graph Generative Models. (arXiv:2305.01089v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.01089](http://arxiv.org/abs/2305.01089)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01089] Computing Expected Motif Counts for Exchangeable Graph Generative Models](http://arxiv.org/abs/2305.01089) #generative`
* Summary: <p>Estimating the expected value of a graph statistic is an important inference
task for using and learning graph models. This note presents a scalable
estimation procedure for expected motif counts, a widely used type of graph
statistic. The procedure applies for generative mixture models of the type used
in neural and Bayesian approaches to graph data.
</p>

### Title: Solving Inverse Problems with Score-Based Generative Priors learned from Noisy Data. (arXiv:2305.01166v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.01166](http://arxiv.org/abs/2305.01166)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01166] Solving Inverse Problems with Score-Based Generative Priors learned from Noisy Data](http://arxiv.org/abs/2305.01166) #generative`
* Summary: <p>We present SURE-Score: an approach for learning score-based generative models
using training samples corrupted by additive Gaussian noise. When a large
training set of clean samples is available, solving inverse problems via
score-based (diffusion) generative models trained on the underlying
fully-sampled data distribution has recently been shown to outperform
end-to-end supervised deep learning. In practice, such a large collection of
training data may be prohibitively expensive to acquire in the first place. In
this work, we present an approach for approximately learning a score-based
generative model of the clean distribution, from noisy training data. We
formulate and justify a novel loss function that leverages Stein's unbiased
risk estimate to jointly denoise the data and learn the score function via
denoising score matching, while using only the noisy samples. We demonstrate
the generality of SURE-Score by learning priors and applying posterior sampling
to ill-posed inverse problems in two practical applications from different
domains: compressive wireless multiple-input multiple-output channel estimation
and accelerated 2D multi-coil magnetic resonance imaging reconstruction, where
we demonstrate competitive reconstruction performance when learning at
signal-to-noise ratio values of 0 and 10 dB, respectively.
</p>

## label correction
## noise
### Title: On the Impact of Data Quality on Image Classification Fairness. (arXiv:2305.01595v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01595](http://arxiv.org/abs/2305.01595)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01595] On the Impact of Data Quality on Image Classification Fairness](http://arxiv.org/abs/2305.01595) #noise`
* Summary: <p>With the proliferation of algorithmic decision-making, increased scrutiny has
been placed on these systems. This paper explores the relationship between the
quality of the training data and the overall fairness of the models trained
with such data in the context of supervised classification. We measure key
fairness metrics across a range of algorithms over multiple image
classification datasets that have a varying level of noise in both the labels
and the training data itself. We describe noise in the labels as inaccuracies
in the labelling of the data in the training set and noise in the data as
distortions in the data, also in the training set. By adding noise to the
original datasets, we can explore the relationship between the quality of the
training data and the fairness of the output of the models trained on that
data.
</p>

### Title: Type-enhanced Ensemble Triple Representation via Triple-aware Attention for Cross-lingual Entity Alignment. (arXiv:2305.01556v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2305.01556](http://arxiv.org/abs/2305.01556)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01556] Type-enhanced Ensemble Triple Representation via Triple-aware Attention for Cross-lingual Entity Alignment](http://arxiv.org/abs/2305.01556) #noise`
* Summary: <p>Entity alignment(EA) is a crucial task for integrating cross-lingual and
cross-domain knowledge graphs(KGs), which aims to discover entities referring
to the same real-world object from different KGs. Most existing methods
generate aligning entity representation by mining the relevance of triple
elements via embedding-based methods, paying little attention to triple
indivisibility and entity role diversity. In this paper, a novel framework
named TTEA -- Type-enhanced Ensemble Triple Representation via Triple-aware
Attention for Cross-lingual Entity Alignment is proposed to overcome the above
issues considering ensemble triple specificity and entity role features.
Specifically, the ensemble triple representation is derived by regarding
relation as information carrier between semantic space and type space, and
hence the noise influence during spatial transformation and information
propagation can be smoothly controlled via specificity-aware triple attention.
Moreover, our framework uses triple-ware entity enhancement to model the role
diversity of triple elements. Extensive experiments on three real-world
cross-lingual datasets demonstrate that our framework outperforms
state-of-the-art methods.
</p>

### Title: PGrad: Learning Principal Gradients For Domain Generalization. (arXiv:2305.01134v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.01134](http://arxiv.org/abs/2305.01134)
* Code URL: [https://github.com/qdata/pgrad](https://github.com/qdata/pgrad)
* Copy Paste: `<input type="checkbox">[[2305.01134] PGrad: Learning Principal Gradients For Domain Generalization](http://arxiv.org/abs/2305.01134) #noise`
* Summary: <p>Machine learning models fail to perform when facing out-of-distribution (OOD)
domains, a challenging task known as domain generalization (DG). In this work,
we develop a novel DG training strategy, we call PGrad, to learn a robust
gradient direction, improving models' generalization ability on unseen domains.
The proposed gradient aggregates the principal directions of a sampled roll-out
optimization trajectory that measures the training dynamics across all training
domains. PGrad's gradient design forces the DG training to ignore
domain-dependent noise signals and updates all training domains with a robust
direction covering main components of parameter dynamics. We further improve
PGrad via bijection-based computational refinement and directional plus
length-based calibrations. Our theoretical proof connects PGrad to the spectral
analysis of Hessian in training neural networks. Experiments on DomainBed and
WILDS benchmarks demonstrate that our approach effectively enables robust DG
optimization and leads to smoothly decreased loss curves. Empirically, PGrad
achieves competitive results across seven datasets, demonstrating its efficacy
across both synthetic and real-world distributional shifts. Code is available
at https://github.com/QData/PGrad.
</p>

### Title: Empowering AI drug discovery with explicit and implicit knowledge. (arXiv:2305.01523v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.01523](http://arxiv.org/abs/2305.01523)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01523] Empowering AI drug discovery with explicit and implicit knowledge](http://arxiv.org/abs/2305.01523) #noise`
* Summary: <p>Motivation: Recently, research on independently utilizing either explicit
knowledge from knowledge graphs or implicit knowledge from biomedical
literature for AI drug discovery has been growing rapidly. These approaches
have greatly improved the prediction accuracy of AI models on multiple
downstream tasks. However, integrating explicit and implicit knowledge
independently hinders their understanding of molecules. Results: We propose
DeepEIK, a unified deep learning framework that incorporates both explicit and
implicit knowledge for AI drug discovery. We adopt feature fusion to process
the multi-modal inputs, and leverage the attention mechanism to denoise the
text information. Experiments show that DeepEIK significantly outperforms
state-of-the-art methods on crucial tasks in AI drug discovery including
drug-target interaction prediction, drug property prediction and
protein-protein interaction prediction. Further studies show that benefiting
from explicit and implicit knowledge, our framework achieves a deeper
understanding of molecules and shows promising potential in facilitating drug
discovery applications.
</p>

### Title: Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees. (arXiv:2305.01588v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.01588](http://arxiv.org/abs/2305.01588)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01588] Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees](http://arxiv.org/abs/2305.01588) #noise`
* Summary: <p>Gradient clipping is a popular modification to standard (stochastic) gradient
descent, at every iteration limiting the gradient norm to a certain value $c
&gt;0$. It is widely used for example for stabilizing the training of deep
learning models (Goodfellow et al., 2016), or for enforcing differential
privacy (Abadi et al., 2016). Despite popularity and simplicity of the clipping
mechanism, its convergence guarantees often require specific values of $c$ and
strong noise assumptions.
</p>
<p>In this paper, we give convergence guarantees that show precise dependence on
arbitrary clipping thresholds $c$ and show that our guarantees are tight with
both deterministic and stochastic gradients. In particular, we show that (i)
for deterministic gradient descent, the clipping threshold only affects the
higher-order terms of convergence, (ii) in the stochastic setting convergence
to the true optimum cannot be guaranteed under the standard noise assumption,
even under arbitrary small step-sizes. We give matching upper and lower bounds
for convergence of the gradient norm when running clipped SGD, and illustrate
these results with experiments.
</p>

## diffusion
### Title: In-Context Learning Unlocked for Diffusion Models. (arXiv:2305.01115v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01115](http://arxiv.org/abs/2305.01115)
* Code URL: [https://github.com/zhendong-wang/prompt-diffusion](https://github.com/zhendong-wang/prompt-diffusion)
* Copy Paste: `<input type="checkbox">[[2305.01115] In-Context Learning Unlocked for Diffusion Models](http://arxiv.org/abs/2305.01115) #diffusion`
* Summary: <p>We present Prompt Diffusion, a framework for enabling in-context learning in
diffusion-based generative models. Given a pair of task-specific example
images, such as depth from/to image and scribble from/to image, and a text
guidance, our model automatically understands the underlying task and performs
the same task on a new query image following the text guidance. To achieve
this, we propose a vision-language prompt that can model a wide range of
vision-language tasks and a diffusion model that takes it as input. The
diffusion model is trained jointly over six different tasks using these
prompts. The resulting Prompt Diffusion model is the first diffusion-based
vision-language foundation model capable of in-context learning. It
demonstrates high-quality in-context generation on the trained tasks and
generalizes effectively to new, unseen vision tasks with their respective
prompts. Our model also shows compelling text-guided image editing results. Our
framework, with code publicly available at
https://github.com/Zhendong-Wang/Prompt-Diffusion, aims to facilitate research
into in-context learning for computer vision.
</p>

### Title: DreamPaint: Few-Shot Inpainting of E-Commerce Items for Virtual Try-On without 3D Modeling. (arXiv:2305.01257v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01257](http://arxiv.org/abs/2305.01257)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01257] DreamPaint: Few-Shot Inpainting of E-Commerce Items for Virtual Try-On without 3D Modeling](http://arxiv.org/abs/2305.01257) #diffusion`
* Summary: <p>We introduce DreamPaint, a framework to intelligently inpaint any e-commerce
product on any user-provided context image. The context image can be, for
example, the user's own image for virtual try-on of clothes from the e-commerce
catalog on themselves, the user's room image for virtual try-on of a piece of
furniture from the e-commerce catalog in their room, etc. As opposed to
previous augmented-reality (AR)-based virtual try-on methods, DreamPaint does
not use, nor does it require, 3D modeling of neither the e-commerce product nor
the user context. Instead, it directly uses 2D images of the product as
available in product catalog database, and a 2D picture of the context, for
example taken from the user's phone camera. The method relies on few-shot fine
tuning a pre-trained diffusion model with the masked latents (e.g., Masked
DreamBooth) of the catalog images per item, whose weights are then loaded on a
pre-trained inpainting module that is capable of preserving the characteristics
of the context image. DreamPaint allows to preserve both the product image and
the context (environment/user) image without requiring text guidance to
describe the missing part (product/context). DreamPaint also allows to
intelligently infer the best 3D angle of the product to place at the desired
location on the user context, even if that angle was previously unseen in the
product's reference 2D images. We compare our results against both text-guided
and image-guided inpainting modules and show that DreamPaint yields superior
performance in both subjective human study and quantitative metrics.
</p>

### Title: ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation. (arXiv:2305.01618v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01618](http://arxiv.org/abs/2305.01618)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01618] ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation](http://arxiv.org/abs/2305.01618) #diffusion`
* Summary: <p>We propose a new dataset and a novel approach to learning hand-object
interaction priors for hand and articulated object pose estimation. We first
collect a dataset using visual teleoperation, where the human operator can
directly play within a physical simulator to manipulate the articulated
objects. We record the data and obtain free and accurate annotations on object
poses and contact information from the simulator. Our system only requires an
iPhone to record human hand motion, which can be easily scaled up and largely
lower the costs of data and annotation collection. With this data, we learn 3D
interaction priors including a discriminator (in a GAN) capturing the
distribution of how object parts are arranged, and a diffusion model which
generates the contact regions on articulated objects, guiding the hand pose
estimation. Such structural and contact priors can easily transfer to
real-world data with barely any domain gap. By using our data and learned
priors, our method significantly improves the performance on joint hand and
articulated object poses estimation over the existing state-of-the-art methods.
The project is available at https://zehaozhu.github.io/ContactArt/ .
</p>

### Title: Geometric Latent Diffusion Models for 3D Molecule Generation. (arXiv:2305.01140v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.01140](http://arxiv.org/abs/2305.01140)
* Code URL: [https://github.com/minkaixu/geoldm](https://github.com/minkaixu/geoldm)
* Copy Paste: `<input type="checkbox">[[2305.01140] Geometric Latent Diffusion Models for 3D Molecule Generation](http://arxiv.org/abs/2305.01140) #diffusion`
* Summary: <p>Generative models, especially diffusion models (DMs), have achieved promising
results for generating feature-rich geometries and advancing foundational
science problems such as molecule design. Inspired by the recent huge success
of Stable (latent) Diffusion models, we propose a novel and principled method
for 3D molecule generation named Geometric Latent Diffusion Models (GeoLDM).
GeoLDM is the first latent DM model for the molecular geometry domain, composed
of autoencoders encoding structures into continuous latent codes and DMs
operating in the latent space. Our key innovation is that for modeling the 3D
molecular geometries, we capture its critical roto-translational equivariance
constraints by building a point-structured latent space with both invariant
scalars and equivariant tensors. Extensive experiments demonstrate that GeoLDM
can consistently achieve better performance on multiple molecule generation
benchmarks, with up to 7\% improvement for the valid percentage of large
biomolecules. Results also demonstrate GeoLDM's higher capacity for
controllable generation thanks to the latent modeling. Code is provided at
\url{https://github.com/MinkaiXu/GeoLDM}.
</p>

## LLM
## segmentation
### Title: Detecting Novelties with Empty Classes. (arXiv:2305.00983v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.00983](http://arxiv.org/abs/2305.00983)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.00983] Detecting Novelties with Empty Classes](http://arxiv.org/abs/2305.00983) #segmentation`
* Summary: <p>For open world applications, deep neural networks (DNNs) need to be aware of
previously unseen data and adaptable to evolving environments. Furthermore, it
is desirable to detect and learn novel classes which are not included in the
DNNs underlying set of semantic classes in an unsupervised fashion. The method
proposed in this article builds upon anomaly detection to retrieve
out-of-distribution (OoD) data as candidates for new classes. We thereafter
extend the DNN by $k$ empty classes and fine-tune it on the OoD data samples.
To this end, we introduce two loss functions, which 1) entice the DNN to assign
OoD samples to the empty classes and 2) to minimize the inner-class feature
distances between them. Thus, instead of ground truth which contains labels for
the different novel classes, the DNN obtains a single OoD label together with a
distance matrix, which is computed in advance. We perform several experiments
for image classification and semantic segmentation, which demonstrate that a
DNN can extend its own semantic space by multiple classes without having access
to ground truth.
</p>

### Title: CLIP-S$^4$: Language-Guided Self-Supervised Semantic Segmentation. (arXiv:2305.01040v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01040](http://arxiv.org/abs/2305.01040)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01040] CLIP-S$^4$: Language-Guided Self-Supervised Semantic Segmentation](http://arxiv.org/abs/2305.01040) #segmentation`
* Summary: <p>Existing semantic segmentation approaches are often limited by costly
pixel-wise annotations and predefined classes. In this work, we present
CLIP-S$^4$ that leverages self-supervised pixel representation learning and
vision-language models to enable various semantic segmentation tasks (e.g.,
unsupervised, transfer learning, language-driven segmentation) without any
human annotations and unknown class information. We first learn pixel
embeddings with pixel-segment contrastive learning from different augmented
views of images. To further improve the pixel embeddings and enable
language-driven semantic segmentation, we design two types of consistency
guided by vision-language models: 1) embedding consistency, aligning our pixel
embeddings to the joint feature space of a pre-trained vision-language model,
CLIP; and 2) semantic consistency, forcing our model to make the same
predictions as CLIP over a set of carefully designed target classes with both
known and unknown prototypes. Thus, CLIP-S$^4$ enables a new task of class-free
semantic segmentation where no unknown class information is needed during
training. As a result, our approach shows consistent and substantial
performance improvement over four popular benchmarks compared with the
state-of-the-art unsupervised and language-driven semantic segmentation
methods. More importantly, our method outperforms these methods on unknown
class recognition by a large margin.
</p>

### Title: Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels. (arXiv:2305.01160v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2305.01160](http://arxiv.org/abs/2305.01160)
* Code URL: [https://github.com/bluecdm/Long-tailed-recognition](https://github.com/bluecdm/Long-tailed-recognition)
* Copy Paste: `<input type="checkbox">[[2305.01160] Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels](http://arxiv.org/abs/2305.01160) #segmentation`
* Summary: <p>Although contrastive learning methods have shown prevailing performance on a
variety of representation learning tasks, they encounter difficulty when the
training dataset is long-tailed. Many researchers have combined contrastive
learning and a logit adjustment technique to address this problem, but the
combinations are done ad-hoc and a theoretical background has not yet been
provided. The goal of this paper is to provide the background and further
improve the performance. First, we show that the fundamental reason contrastive
learning methods struggle with long-tailed tasks is that they try to maximize
the mutual information maximization between latent features and input data. As
ground-truth labels are not considered in the maximization, they are not able
to address imbalances between class labels. Rather, we interpret the
long-tailed recognition task as a mutual information maximization between
latent features and ground-truth labels. This approach integrates contrastive
learning and logit adjustment seamlessly to derive a loss function that shows
state-of-the-art performance on long-tailed recognition benchmarks. It also
demonstrates its efficacy in image segmentation tasks, verifying its
versatility beyond image classification.
</p>

### Title: RT-K-Net: Revisiting K-Net for Real-Time Panoptic Segmentation. (arXiv:2305.01255v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01255](http://arxiv.org/abs/2305.01255)
* Code URL: [https://github.com/markusschoen/rt-k-net](https://github.com/markusschoen/rt-k-net)
* Copy Paste: `<input type="checkbox">[[2305.01255] RT-K-Net: Revisiting K-Net for Real-Time Panoptic Segmentation](http://arxiv.org/abs/2305.01255) #segmentation`
* Summary: <p>Panoptic segmentation is one of the most challenging scene parsing tasks,
combining the tasks of semantic segmentation and instance segmentation. While
much progress has been made, few works focus on the real-time application of
panoptic segmentation methods. In this paper, we revisit the recently
introduced K-Net architecture. We propose vital changes to the architecture,
training, and inference procedure, which massively decrease latency and improve
performance. Our resulting RT-K-Net sets a new state-of-the-art performance for
real-time panoptic segmentation methods on the Cityscapes dataset and shows
promising results on the challenging Mapillary Vistas dataset. On Cityscapes,
RT-K-Net reaches 60.2 % PQ with an average inference time of 32 ms for full
resolution 1024x2048 pixel images on a single Titan RTX GPU. On Mapillary
Vistas, RT-K-Net reaches 33.2 % PQ with an average inference time of 69 ms.
Source code is available at https://github.com/markusschoen/RT-K-Net.
</p>

### Title: Segment Anything is A Good Pseudo-label Generator for Weakly Supervised Semantic Segmentation. (arXiv:2305.01275v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01275](http://arxiv.org/abs/2305.01275)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01275] Segment Anything is A Good Pseudo-label Generator for Weakly Supervised Semantic Segmentation](http://arxiv.org/abs/2305.01275) #segmentation`
* Summary: <p>Weakly supervised semantic segmentation with weak labels is a long-lived
ill-posed problem. Mainstream methods mainly focus on improving the quality of
pseudo labels. In this report, we attempt to explore the potential of 'prompt
to masks' from the powerful class-agnostic large segmentation model,
segment-anything. Specifically, different weak labels are used as prompts to
the segment-anything model, generating precise class masks. The class masks are
utilized to generate pseudo labels to train the segmentation networks. We have
conducted extensive experiments on PASCAL VOC 2012 dataset. Experiments
demonstrate that segment-anything can serve as a good pseudo-label generator.
The code will be made publicly available.
</p>

### Title: Oil Spill Segmentation using Deep Encoder-Decoder models. (arXiv:2305.01386v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01386](http://arxiv.org/abs/2305.01386)
* Code URL: [https://github.com/abhishekrs4/htsm_oil_spill_segmentation](https://github.com/abhishekrs4/htsm_oil_spill_segmentation)
* Copy Paste: `<input type="checkbox">[[2305.01386] Oil Spill Segmentation using Deep Encoder-Decoder models](http://arxiv.org/abs/2305.01386) #segmentation`
* Summary: <p>Crude oil is an integral component of the modern world economy. With the
growing demand for crude oil due to its widespread applications, accidental oil
spills are unavoidable. Even though oil spills are in and themselves difficult
to clean up, the first and foremost challenge is to detect spills. In this
research, the authors test the feasibility of deep encoder-decoder models that
can be trained effectively to detect oil spills. The work compares the results
from several segmentation models on high dimensional satellite Synthetic
Aperture Radar (SAR) image data. Multiple combinations of models are used in
running the experiments. The best-performing model is the one with the
ResNet-50 encoder and DeepLabV3+ decoder. It achieves a mean Intersection over
Union (IoU) of 64.868% and a class IoU of 61.549% for the "oil spill" class
when compared with the current benchmark model, which achieved a mean IoU of
65.05% and a class IoU of 53.38% for the "oil spill" class.
</p>

### Title: An Alternative to WSSS? An Empirical Study of the Segment Anything Model (SAM) on Weakly-Supervised Semantic Segmentation Problems. (arXiv:2305.01586v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01586](http://arxiv.org/abs/2305.01586)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01586] An Alternative to WSSS? An Empirical Study of the Segment Anything Model (SAM) on Weakly-Supervised Semantic Segmentation Problems](http://arxiv.org/abs/2305.01586) #segmentation`
* Summary: <p>The Segment Anything Model (SAM) has demonstrated exceptional performance and
versatility, making it a promising tool for various related tasks. In this
report, we explore the application of SAM in Weakly-Supervised Semantic
Segmentation (WSSS). Particularly, we adapt SAM as the pseudo-label generation
pipeline given only the image-level class labels. While we observed impressive
results in most cases, we also identify certain limitations. Our study includes
performance evaluations on PASCAL VOC and MS-COCO, where we achieved remarkable
improvements over the latest state-of-the-art methods on both datasets. We
anticipate that this report encourages further explorations of adopting SAM in
WSSS, as well as wider real-world applications.
</p>

### Title: Neural LiDAR Fields for Novel View Synthesis. (arXiv:2305.01643v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01643](http://arxiv.org/abs/2305.01643)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2305.01643] Neural LiDAR Fields for Novel View Synthesis](http://arxiv.org/abs/2305.01643) #segmentation`
* Summary: <p>We present Neural Fields for LiDAR (NFL), a method to optimise a neural field
scene representation from LiDAR measurements, with the goal of synthesizing
realistic LiDAR scans from novel viewpoints. NFL combines the rendering power
of neural fields with a detailed, physically motivated model of the LiDAR
sensing process, thus enabling it to accurately reproduce key sensor behaviors
like beam divergence, secondary returns, and ray dropping. We evaluate NFL on
synthetic and real LiDAR scans and show that it outperforms explicit
reconstruct-then-simulate methods as well as other NeRF-style methods on LiDAR
novel view synthesis task. Moreover, we show that the improved realism of the
synthesized views narrows the domain gap to real scans and translates to better
registration and semantic segmentation performance.
</p>

## object detection
### Title: Faster OreFSDet : A Lightweight and Effective Few-shot Object Detector for Ore Images. (arXiv:2305.01183v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2305.01183](http://arxiv.org/abs/2305.01183)
* Code URL: [https://github.com/mvme-hbut/faster-orefsdet](https://github.com/mvme-hbut/faster-orefsdet)
* Copy Paste: `<input type="checkbox">[[2305.01183] Faster OreFSDet : A Lightweight and Effective Few-shot Object Detector for Ore Images](http://arxiv.org/abs/2305.01183) #object detection`
* Summary: <p>For the ore particle size detection, obtaining a sizable amount of
high-quality ore labeled data is time-consuming and expensive. General object
detection methods often suffer from severe over-fitting with scarce labeled
data. Despite their ability to eliminate over-fitting, existing few-shot object
detectors encounter drawbacks such as slow detection speed and high memory
requirements, making them difficult to implement in a real-world deployment
scenario. To this end, we propose a lightweight and effective few-shot detector
to achieve competitive performance with general object detection with only a
few samples for ore images. First, the proposed support feature mining block
characterizes the importance of location information in support features. Next,
the relationship guidance block makes full use of support features to guide the
generation of accurate candidate proposals. Finally, the dual-scale semantic
aggregation module retrieves detailed features at different resolutions to
contribute with the prediction process. Experimental results show that our
method consistently exceeds the few-shot detectors with an excellent
performance gap on all metrics. Moreover, our method achieves the smallest
model size of 19MB as well as being competitive at 50 FPS detection speed
compared with general object detectors. The source code is available at
https://github.com/MVME-HBUT/Faster-OreFSDet.
</p>

